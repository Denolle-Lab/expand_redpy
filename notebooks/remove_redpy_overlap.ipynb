{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda7aef9",
   "metadata": {},
   "source": [
    "##########################################################################\n",
    "\n",
    "GOAL:\n",
    "\n",
    "    make it so that this pulls in the events.csv as a df, finds the lines containing overlaps, deletes them, then puts the df back into csv just now without the overlap\n",
    "    \n",
    "    add template locations as a column in the df after checking for redpy overlap\n",
    "    \n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249ae1b",
   "metadata": {},
   "source": [
    "Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da00eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from glob import glob\n",
    "from obspy.signal.trigger import classic_sta_lta, plot_trigger, trigger_onset\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from obspy.core.utcdatetime import UTCDateTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a2447",
   "metadata": {},
   "source": [
    "Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26aa0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "rpwi = config['rpwi'] #time in seconds before and after REDpy catalog datetimes to exclude detections from, window length=2*rpwi\n",
    "homedir = config ['homedir']\n",
    "# datadir = '/data/wsd01/HOOD_data/UW/'+str(year)+'/' #directory to get data from"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b0f83",
   "metadata": {},
   "source": [
    "Read the REDpy Catalogs and Volcano Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baker = pd.read_csv('Baker_catalog.csv')\n",
    "Hood = pd.read_csv('Hood_catalog.csv')\n",
    "\n",
    "\n",
    "St_Helens = pd.read_csv('MountStHelens_catalog.csv')\n",
    "\n",
    "# Combining borehole and local catalogs with St_Helens\n",
    "\n",
    "Helens_Borehole = pd.read_csv('MSHborehole_catalog.csv')\n",
    "Helens_Borehole['Clustered'] += 2000 \n",
    "# Cluster 0 in Helens_Borehole is now Cluster 2000 in St_Helens\n",
    "Helens_Local = pd.read_csv('MSHlocal_catalog.csv')\n",
    "Helens_Local['Clustered'] += 3000\n",
    "# Cluster 0 in Helens_Local is now Cluster 3000 in St_Helens\n",
    "\n",
    "# Use St_Helens to access all three St Helens catalogs\n",
    "St_Helens = pd.concat([St_Helens,Helens_Borehole,Helens_Local])\n",
    "\n",
    "Newberry = pd.read_csv('Newberry_catalog.csv')\n",
    "Rainier = pd.read_csv('Rainier_catalog.csv')\n",
    "\n",
    "volc_md = pd.read_csv('Volcano_Metadata.csv')\n",
    "# read metadata file to create dataframe of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58bdf1",
   "metadata": {},
   "source": [
    "Use Volcano Metadata to Create Lists of Stations for Each Volcano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33449666",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baker_sta = volc_md[volc_md['Volcano_Name'] == 'Mt_Baker']['Station'].values.tolist()\n",
    "Hood_sta = volc_md[volc_md['Volcano_Name'] == 'Mt_Hood']['Station'].values.tolist() \n",
    "St_Helens_sta = volc_md[volc_md['Volcano_Name'] == 'Mt_St_Helens']['Station'].values.tolist()\n",
    "Newberry_sta = volc_md[volc_md['Volcano_Name'] == 'Newberry']['Station'].values.tolist() \n",
    "Rainier_sta = volc_md[volc_md['Volcano_Name'] == 'Mt_Rainier']['Station'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81c80f",
   "metadata": {},
   "source": [
    "Create Lists of Volcano Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enumerate [0,1,2,3,4]\n",
    "volc_list = [Baker,Hood,Newberry,Rainier,St_Helens] # list of dataframes for each volcano\n",
    "volc_list_names = ['Baker','Hood','Newberry','Rainier','St_Helens'] # list of names of each volcano\n",
    "volc_sta = [Baker_sta,Hood_sta,Newberry_sta,Rainier_sta,St_Helens_sta] # lists of stations connected to respective volcanoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69e70c",
   "metadata": {},
   "source": [
    "### Sort Detections - Jul 28, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da04f3a",
   "metadata": {},
   "source": [
    "Updated 28, not tested yet - test on Hood 2019 detections from siletzia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b0d571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make separate versions for each volcano\n",
    "# for vv,v in enumerate(volc_sta):\n",
    "v = Baker_sta\n",
    "vv = 0\n",
    "for s in range(0,len(v)): \n",
    "    try:\n",
    "        read = pd.read_csv(homedir+'detections/'+volc_list_names[vv]+'_'+v[s]+'_'+year+'_detections.csv')\n",
    "    except:\n",
    "        print('No detections for',v[s])\n",
    "        continue\n",
    "        \n",
    "    with open(homedir+'detections/'+volc_list_names[vv]+'_'+v[s]+'_'+str(year)+\n",
    "          '_clean_detections.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Template_Name\",\"Detection_Time\"]\n",
    "        file.close()\n",
    "        \n",
    "    #Make list of all clusters that have a template\n",
    "    temp_name_list = read['Template_Name'].values.tolist() #make a list of template names\n",
    "    cl_list_long = [] # make a list of the numbers in each template name, like [0,6,8,0,0,2,6,6,6,8,8...,6]\n",
    "    for i in temp_name_list: \n",
    "        num = re.findall(r'\\d+', i) #find the numbers in each template name\n",
    "        cl_list_long.append(*num)\n",
    "    cl_list = np.unique(cl_list_long) #get rid of duplicates and sort, like [0,2,6,8]\n",
    "\n",
    "\n",
    "    for cl in cl_list:\n",
    "#         times = [] #list of datetimes for this cluster\n",
    "#         for i in np.unique(temp_name_list):\n",
    "#             if i.endswith(cl):\n",
    "#                 all_times = readsta[readsta['Template_Name']==i]['Detection_Time'].values.tolist()\n",
    "#                 for at in all_times:\n",
    "#                     times.append(at)\n",
    "# #         times = read[read['Template_Name'] == v[s].lower()+chan_+'rpho'+str(cl)]['Detection_Time'].values.tolist()\n",
    "#         for ii,i in enumerate(times):\n",
    "#             et=UTCDateTime(i)+ta\n",
    "#             stt=UTCDateTime(i)-tb\n",
    "#             utct=UTCDateTime(i)\n",
    "#             st = obspy.read(*glob(datadir+str(i.julday).zfill(3)+'/'+v[s]+'.*.'+str(year)+'.*'))\n",
    "#             st.select(component=\"Z\") #Use only the Z Component\n",
    "#             st.filter(type='bandpass',freqmin=fqmin,freqmax=fqmax)\n",
    "#             st.detrend(type='demean')\n",
    "#             st.resample(fs)\n",
    "#             st.trim(starttime=stt,endtime=et)\n",
    "#             st.merge(fill_value = 0)\n",
    "#             if len(st)==0: continue\n",
    "#     # classic\n",
    "#             try:\n",
    "#                 cft = classic_sta_lta(st[0].data, int(nsta * fs), int(nlta * fs))\n",
    "#                 print('-------------')\n",
    "#                 print('detection: '+str(ii),'cluster id: '+str(cl))\n",
    "#                 plot_trigger(st[0], cft, thr_on, thr_off)\n",
    "#                 on_off = np.array(trigger_onset(cft, thr_on, thr_off))\n",
    "#                 # show trigger on and off times, rounded to 4 decimal places\n",
    "#                 trig_on = round(float(on_off[:, 0] / fs)-tb,4)\n",
    "#                 trig_off = round(float(on_off[:, 1] / fs)-tb,4)\n",
    "#                 print('Trigger on',trig_on,'seconds after detect time')\n",
    "#                 print('Trigger off',trig_off,'seconds after detect time')\n",
    "#             except:\n",
    "#                 print('NOT FOUND') #if no signal can be found, print 'NOT FOUND' and skip the rest of the loop\n",
    "#                 continue\n",
    "#             signal_window = st[0].copy()\n",
    "#             noise_window = st[0].copy()\n",
    "\n",
    "#             signal_window.trim(starttime=UTCDateTime(i)+trig_on-0.5,endtime=UTCDateTime(i)+trig_off) \n",
    "#             #i+trig_on-0.5 to include lead up to the signal\n",
    "#             noise_window.trim(starttime=UTCDateTime(i)-10,endtime=UTCDateTime(i))\n",
    "\n",
    "#             snr = 20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "#                               / np.percentile(np.abs(noise_window.data),pr))/np.log(10)\n",
    "#             if snr<7.: continue #if SNR is too low, skip saving it\n",
    "    \n",
    "        #put skipping REDpy detections here\n",
    "        #read REDpy catalog to have a reference\n",
    "        catalog = pd.read_csv('Hood_catalog.csv')\n",
    "        rpdatetimes = catalog[catalog['Clustered'] == cl]['datetime'].values.tolist() #get every datetime for this cluster\n",
    "        #make a list of datetimes for the current cluster\n",
    "        skip=1 #set variable to arbitrary number\n",
    "        for rr,r in enumerate(rpdatetimes): #run through each redpy time for this cl\n",
    "            rs = UTCDateTime(r)-wi #redpy time\n",
    "            rend = UTCDateTime(r)+wi #changed from re to rend because of import re for cl_list\n",
    "            if UTCDateTime(i)>rs and UTCDateTime(i)<rend:\n",
    "                skip=2 #if there is an overlap, reset the variable and break out of the loop\n",
    "                print('Overlap with REDpy detections')\n",
    "                break\n",
    "        if skip!=2: #if skip has NOT been redefined, save this detection\n",
    "            for u in np.unique(temp_name_list):\n",
    "                if u.endswith(cl):\n",
    "                    t = u\n",
    "            row = [t,i,trig_on,trig_off,snr]\n",
    "            print(row)\n",
    "#             automatically filters out detections that it can't find a signal for\n",
    "            with open(homedir+'detections/'+volc_list_names[vv]+'_'+v[s]+'_'+str(year)+\n",
    "                      '_clean_detections.csv', 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(row)\n",
    "                file.close()\n",
    "\n",
    "\n",
    "            break\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
