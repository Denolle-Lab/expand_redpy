{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baecd3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import yaml\n",
    "import csv\n",
    "from time import time\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "#read config file for parameters\n",
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "smooth_length = config['smooth_length']\n",
    "fs = config['fs']\n",
    "tb = config['tb']\n",
    "ta = config['ta']\n",
    "fqmin = config['fqmin']\n",
    "fqmax = config['fqmax']\n",
    "chan = config['chan']\n",
    "homedir = config['homedir']\n",
    "readdir = config['readdir']\n",
    "minsta = config['minsta']\n",
    "grid_length = float(config['grid_length'])\n",
    "grid_height = float(config['grid_height'])\n",
    "step = config['step']\n",
    "t_step = config['t_step']\n",
    "vs_min = config['vs_min']\n",
    "vs_max = config['vs_max']\n",
    "vs_step = config['vs_step']\n",
    "volc_lat_lon = config['volc_lat_lon']\n",
    "volc_list_names = config['volc_list_names']\n",
    "nlta = config['nlta']\n",
    "\n",
    "vv = config['vv']\n",
    "vv=3\n",
    "\n",
    "iend = 3520 #end of possible pick times\n",
    "istart = 176 #beginning of possible pick times (currently excluding taper)\n",
    "\n",
    "\n",
    "print(volc_list_names[vv])\n",
    "\n",
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv')\n",
    "#associate network and station\n",
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str)\n",
    "\n",
    "\n",
    "#get clusterid from template name\n",
    "def getcl_id(t_name_str): #for normalized\n",
    "    t_cl = int(t_name_str.split('_')[-1])\n",
    "    return t_cl\n",
    "\n",
    "#get station name from template name\n",
    "def getnet_sta(template_str): #for normalized\n",
    "    t_net = template_str.split('_')[0]\n",
    "    t_sta = template_str.split('_')[1]\n",
    "    netsta = t_net+'_'+t_sta\n",
    "    return t_net, t_sta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7700989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some info\n",
    "v = volc_md[volc_md['Volcano_Name'] == volc_list_names[vv]]['netsta'].values.tolist() #list of network and station per volc\n",
    "zz = chan[-2:].lower() #the last two letters of channel names (essentially the letters in chan)\n",
    "csv_name = f'{homedir}locations/{volc_list_names[vv]}_ELEP_normalized_picktimes.csv' #name of the csv for picktimes at this volcano\n",
    "h5_name = f'{homedir}h5/{volc_list_names[vv]}_ELEP_smb_pred.h5' #name of h5 file for smb_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7421aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "smbs = []\n",
    "temps = []\n",
    "\n",
    "with h5py.File(h5_name, \"r\") as f: #pull in fingerprints\n",
    "    for ff in f.keys():\n",
    "#         print(ff)\n",
    "        if ff.startswith('smb_pred'):\n",
    "            smbs.append(f[ff][()])\n",
    "        if ff.startswith('template'):\n",
    "            temps.append(f[ff][()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28f9651",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(csv_name, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Network','Station','Cluster_ID','Template_Name','SMB_peak']) #,'SMB_peak_MBF'\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a1af2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for smb, template in zip(smbs,temps): #go through each cluster\n",
    "    \n",
    "    time0 = time()\n",
    "    \n",
    "    print('---')\n",
    "    cl = getcl_id(str(template[0])[2:-1])\n",
    "    print(f'for cluster {cl}')\n",
    "    \n",
    "    dup_list = [] #a list to check if any smb has been saved twice\n",
    "    \n",
    "    temps_p = [] #list of template picks for the cluster\n",
    "    rows = [] #list of csv rows for this cluster\n",
    "\n",
    "    for ss,s in enumerate(smb):\n",
    "        smb_pred = s[0][0]\n",
    "        t_name = str(template[ss])[2:-1]\n",
    "        \n",
    "        if t_name in dup_list: #if we have already seen this template/smb\n",
    "            continue #skip to next one\n",
    "            \n",
    "        else:\n",
    "        \n",
    "            dup_list.append(t_name) #record seeing this template/smb\n",
    "\n",
    "#             #plot\n",
    "            plt.plot(smb_pred);\n",
    "            print(f'before mbf: {smb_pred}')\n",
    "            plt.title(t_name)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            sfs = fs\n",
    "            istart = int(istart) #t_before*sfs - t_around*sfs\n",
    "            iend = int(iend) #np.min((t_before*sfs + t_around*sfs,smb_pred.shape[1]))\n",
    "            print(f'istart {istart} or {istart/sfs} seconds; iend {iend} or {iend/sfs} seconds')\n",
    "\n",
    "                \n",
    "            print(f'after mbf: {smb_pred}')\n",
    "\n",
    "            if np.isnan(smb_pred[istart:iend]).any(): #if smb_pred is just nans, skip\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                ### from mbf_elep_fun.py ###\n",
    "\n",
    "                peaks = signal.find_peaks(smb_pred[istart:iend],distance=5*sfs, height=0.03)\n",
    "\n",
    "                if len(peaks[0]) == 0:\n",
    "                    peaks = signal.find_peaks(smb_pred[istart:iend],distance=5*sfs)\n",
    "                    \n",
    "                print('peaks:')\n",
    "                print(peaks[0]) ## PEAKS, NOT PICKS. they tell you how long after istart ###\n",
    "                \n",
    "                \n",
    "                ### END from mbf_elep_fun.py ###\n",
    "\n",
    "\n",
    "                picks = [i+istart for i in peaks[0]] # ADDED ISTART\n",
    "                temps_p.append(picks)\n",
    "                csv_picks=' '.join([str(i/fs)for i in picks]) #formatted for saving in csv\n",
    "                \n",
    "                net, sta = getnet_sta(t_name)\n",
    "\n",
    "                #append picks to csv\n",
    "                row = [net, sta, cl, t_name, csv_picks] ### APPEND PICKS NOT PEAKS\n",
    "                rows.append(row)\n",
    "\n",
    "    ### FILTERING PEAKS ###\n",
    "\n",
    "    one_peak = [] #list of n and y if the template only has one peak/picktime\n",
    "    for p in temps_p: #for saved picks\n",
    "        if len(p)==1:\n",
    "            one_peak.append('y')\n",
    "        else:\n",
    "            one_peak.append('n')\n",
    "            \n",
    "            \n",
    "    if one_peak.count('y')/len(one_peak) ==1: #if there is only one peak per template\n",
    "        #write info to csv as is\n",
    "        for row in rows:\n",
    "            with open(csv_name, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(row)\n",
    "                file.close()\n",
    "\n",
    "    if one_peak.count('y')/len(one_peak) >=0.5 and one_peak.count('y')/len(one_peak) <1: #if 75% of templates have one peak\n",
    "\n",
    "        one_p_value = [] #list of peak values when there is only one\n",
    "        for peaks in temps_p:\n",
    "            if len(peaks)==1:\n",
    "                one_p_value.append(peaks[0])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        for en,peaks in enumerate(temps_p):\n",
    "            if len(peaks)>1:\n",
    "                sums = []#list of sum of differences between possible peaks and the \"confirmed\" peaks\n",
    "                for peak in peaks:\n",
    "                    sums.append(sum([abs(peak-v) for v in one_p_value]))\n",
    "                closest_peak = peaks[sums.index(min(sums))] #find which peak has the smallest summed distance\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            rows[en][-1] = str(closest_peak/fs) #update the corresponding row's pick for csv\n",
    "\n",
    "        #write updated info to csv\n",
    "        for row in rows:\n",
    "            with open(csv_name, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(row)\n",
    "                file.close()\n",
    "\n",
    "\n",
    "    if one_peak.count('y')/len(one_peak) <0.5:\n",
    "\n",
    "        for en,peaks in enumerate(temps_p):\n",
    "            if len(peaks)>1:\n",
    "                rows[en][-1] = 'UNCERTAIN'\n",
    "\n",
    "        #write updated info to csv\n",
    "        for row in rows:\n",
    "            with open(csv_name, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(row)\n",
    "                file.close()\n",
    "    time1 = time()\n",
    "    \n",
    "    print(f'{time1-time0} seconds for this cluster')\n",
    "        \n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7036cc6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo 3.8 (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
