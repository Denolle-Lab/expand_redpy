{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348ac66f",
   "metadata": {},
   "source": [
    "Created August 17, 2023\n",
    "\n",
    "Purpose is to update location grid search in the following ways:\n",
    "1. use templates whose constituent waveforms were normalized before stacking (fixes most false features)\n",
    "2. use elep to find picktimes instead of envelope cross correlation\n",
    "3. base velocity model of grid search on p and s picktimes when possible\n",
    "4. compare locations to ComCat potential matches from the <a href=\"https://assets.pnsn.org/red/\">redpy website</a> when possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5425951e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched-filter CPU is not compiled! Should be here: /home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/fast_matched_filter/lib/matched_filter_CPU.so\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import yaml\n",
    "import csv\n",
    "import eqcorrscan\n",
    "from eqcorrscan import Tribe\n",
    "from time import time\n",
    "import obspy\n",
    "from obspy import UTCDateTime, Trace\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from obspy.signal.cross_correlation import *\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a076193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config file for parameters\n",
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "smooth_length = config['smooth_length']\n",
    "fs = config['fs']\n",
    "tb = config['tb']\n",
    "ta = config['ta']\n",
    "fqmin = config['fqmin']\n",
    "fqmax = config['fqmax']\n",
    "chan = config['chan']\n",
    "homedir = config['homedir']\n",
    "readdir = config['readdir']\n",
    "minsta = config['minsta']\n",
    "grid_length = float(config['grid_length'])\n",
    "grid_height = float(config['grid_height'])\n",
    "step = config['step']\n",
    "t_step = config['t_step']\n",
    "vs_min = config['vs_min']\n",
    "vs_max = config['vs_max']\n",
    "vs_step = config['vs_step']\n",
    "volc_lat_lon = config['volc_lat_lon']\n",
    "\n",
    "vv = config['vv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f3fda39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3523\n"
     ]
    }
   ],
   "source": [
    "#read catalogues and metadata\n",
    "\n",
    "Baker = pd.read_csv(readdir+'Baker_catalog.csv')\n",
    "Hood = pd.read_csv(readdir+'Hood_catalog.csv')\n",
    "\n",
    "\n",
    "St_Helens = pd.read_csv(readdir+'MountStHelens_catalog.csv')\n",
    "\n",
    "# Combining borehole and local catalogs with St_Helens\n",
    "\n",
    "Helens_Borehole = pd.read_csv(readdir+'MSHborehole_catalog.csv')\n",
    "Helens_Borehole['Clustered'] += 2000 \n",
    "# Cluster 0 in Helens_Borehole is now Cluster 2000 in St_Helens\n",
    "Helens_Local = pd.read_csv(readdir+'MSHlocal_catalog.csv')\n",
    "Helens_Local['Clustered'] += 3000\n",
    "# Cluster 0 in Helens_Local is now Cluster 3000 in St_Helens\n",
    "\n",
    "# Use St_Helens to access all three St Helens catalogs\n",
    "St_Helens = pd.concat([St_Helens,Helens_Borehole,Helens_Local])\n",
    "clid = np.unique(St_Helens['Clustered'].values.tolist()) #find the largest cluster ID for a volcano to set range\n",
    "print(clid[-1])\n",
    "\n",
    "Newberry = pd.read_csv(readdir+'Newberry_catalog.csv')\n",
    "Rainier = pd.read_csv(readdir+'Rainier_catalog.csv')\n",
    "\n",
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv')\n",
    "# read metadata file to create dataframe of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6533674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#associate network and station\n",
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str)\n",
    "\n",
    "# Create Lists of Stations for Each Volcano Using volc_md\n",
    "Baker_sta = volc_md[volc_md['Volcano_Name'] == 'Baker']['netsta'].values.tolist()\n",
    "Hood_sta = volc_md[volc_md['Volcano_Name'] == 'Hood']['netsta'].values.tolist() \n",
    "St_Helens_sta = volc_md[volc_md['Volcano_Name'] == 'St_Helens']['netsta'].values.tolist()\n",
    "Newberry_sta = volc_md[volc_md['Volcano_Name'] == 'Newberry']['netsta'].values.tolist() \n",
    "Rainier_sta = volc_md[volc_md['Volcano_Name'] == 'Rainier']['netsta'].values.tolist()\n",
    "\n",
    "#Create Lists of Volcano Information\n",
    "volc_list = [Baker,Hood,Newberry,Rainier,St_Helens] # list of dataframes for each volcano\n",
    "volc_list_names = ['Baker','Hood','Newberry','Rainier','St_Helens'] # list of names of each volcano\n",
    "volc_sta = [Baker_sta,Hood_sta,Newberry_sta,Rainier_sta,St_Helens_sta] # lists of stations connected to respective volcanoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#envelope cross correlation picktime\n",
    "def pick_time(ref_env, data_env_dict, st): \n",
    "    est_picktimes=str(st[0].stats.starttime)\n",
    "    xcor = correlate(data_env_dict,ref_env,int(50*fs))\n",
    "    index = np.argmax(xcor)\n",
    "    cc = round(xcor[index],9) #correlation coefficient\n",
    "    shift = 50*fs-index #how much it is shifted from the reference envelope\n",
    "    #print(shift, cc, key)\n",
    "    relative_p = shift/fs\n",
    "    p = UTCDateTime(est_picktimes) + shift/fs  # p is the new phase pick for each station\n",
    "    return p, shift, relative_p\n",
    "\n",
    "#color map for plotting\n",
    "def get_cmap(n, name='viridis'): #hsv\n",
    "#     Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "#     RGB color; the keyword argument name must be a standard mpl colormap name.\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "# define function to predict synthetic arrival times\n",
    "def travel_time(t0, x, y, vs, sta_x, sta_y):\n",
    "    dist = np.sqrt((sta_x - x)**2 + (sta_y - y)**2)\n",
    "    tt = t0 + dist/vs\n",
    "    return tt\n",
    "\n",
    "# define function to compute residual sum of squares\n",
    "def error(synth_arrivals,arrivals):\n",
    "    res = arrivals - synth_arrivals   #make sure arrivals are in the right order, maybe iterate through keys\n",
    "    res_sqr = res**2\n",
    "    rss = np.sum(res_sqr)\n",
    "    return rss\n",
    "\n",
    "# define function to iterate through grid and calculate travel time residuals\n",
    "def gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals):\n",
    "    rss_mat = np.zeros((len(t0),len(x_vect),len(y_vect)))\n",
    "    rss_mat[:,:,:] = np.nan\n",
    "    for i in range(len(t0)): \n",
    "        for j in range(len(x_vect)):\n",
    "            for k in range(len(y_vect)):\n",
    "                for m in range(len(vs)): #parameterize velocity\n",
    "                    synth_arrivals = []\n",
    "                    for h in range(len(sta_x)):\n",
    "                        tt = travel_time(t0[i],x_vect[j],y_vect[k],vs[m],sta_x[h],sta_y[h]) \n",
    "                    #add vs in nested loop, vector 1000-5000, per cluster to account for p and s waves\n",
    "                        synth_arrivals.append(tt)\n",
    "                    rss = error(np.array(synth_arrivals),np.array(arrivals))\n",
    "                    rss_mat[i,j,k] = rss\n",
    "    return rss_mat\n",
    "\n",
    "# define function to convert the location index into latitude and longitude\n",
    "def location(x_dist, y_dist, start_lat, start_lon):\n",
    "    bearing = 90-np.rad2deg(np.arctan(y_dist/x_dist))\n",
    "    dist = np.sqrt((x_dist)**2 + (y_dist)**2)\n",
    "    d = distance.geodesic(meters = dist)\n",
    "    loc_lat = d.destination(point=[start_lat,start_lon], bearing=bearing)[0]\n",
    "    loc_lon = d.destination(point=[start_lat,start_lon], bearing=bearing)[1]\n",
    "    return loc_lat, loc_lon, d\n",
    "\n",
    "# define function to find diameter in meters of the error on the location\n",
    "def error_diameter(new_array):\n",
    "    min_idx = np.min(new_array[:,1])\n",
    "    max_idx = np.max(new_array[:,1])\n",
    "    difference = max_idx-min_idx\n",
    "    diameter_m = difference*1000\n",
    "    return diameter_m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774096b3",
   "metadata": {},
   "source": [
    "Find and Save locations (currently copy and pasted from template_locations.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vv,v in enumerate(volc_sta): #vv is the number in the list, v is the station list for current volcano\n",
    "vv=0\n",
    "v = volc_sta[vv] #not doing a volcano loop\n",
    "clid = np.unique(volc_list[vv]['Clustered'].values.tolist()) #find the cluster IDs for a volcano to set range\n",
    "cllen = len(str(clid[-1])) #length of the largest cluster ID, used for zfill\n",
    "zz = chan[-2:].lower() #the last two letters of channel names (essentially the letters in chan)\n",
    "#make csv?\n",
    "\n",
    "with open(homedir+f'/locations/{volc_list_names[vv]}_Template_Locations.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Volcano_Name','netsta','Picktimes','Cluster_ID','Latitude','Longitude',])\n",
    "    file.close()\n",
    "\n",
    "for cl in range(13,14):#options: cl in range(0,clid[-1]+1) for specific ranges; cl in clid for all Cluster IDs\n",
    "    temps_s = {} #empty dictionary that will be filled with the templates for this cluster\n",
    "    print('------') #print a divider\n",
    "    print(\"cluster:\",str(cl).zfill(cllen)) #print the cluster ID\n",
    "    stopwatch0=time() #note the time\n",
    "    for s in range(0,len(v)): #loop through stations\n",
    "        net, sta =  v[s].split('.') #add specific network per station\n",
    "\n",
    "########################################################################    \n",
    "#                       FINDING PICK TIMES                             #\n",
    "########################################################################\n",
    "\n",
    "        # try to read the .tgz/Tribe file\n",
    "        try:\n",
    "            T = Tribe().read(*glob(f'{homedir}templates/Volcano_{volc_list_names[vv]}_Network_{net}_Station_{sta}_Channel_*.tgz'))\n",
    "        except:\n",
    "            print(f'{net}.{sta} tgz does not exist')\n",
    "            continue\n",
    "        for t in T: #for each template in the tgz\n",
    "            if t.name.endswith(str(cl).zfill(cllen)): #if the template name endswith this cluster\n",
    "                temps_s[f'{net.lower()}.{t.name}']=t #save to dictionary and include network name\n",
    "                break\n",
    "\n",
    "    if len(temps_s) < minsta: #if the number of templates for this cluster is less than minsta (see config)\n",
    "        print('not enough stations with data for this cluster') #print a reminder\n",
    "        stopwatch2=time() #note the time\n",
    "        print(f'{stopwatch2-stopwatch0} s for this cluster') #say how many s to go through this cluster\n",
    "        continue #move onto the next cluster\n",
    "        \n",
    "    data_env_dict = {} #a dictionary of the envelopes for each template stream\n",
    "    for t in temps_s: #for each saved template (aka each template for this cluster and volc that exists)\n",
    "        data_envelope = obspy.signal.filter.envelope(temps_s[t].st[0].data) #make an envelope\n",
    "        data_envelope /= np.max(data_envelope) #average envelope (?)\n",
    "        data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length) #smooth the envelope\n",
    "        data_env_dict[t] = data_envelope #save the envelope to the dictionary\n",
    "\n",
    "    pick_times = {} #dictionary of picktimes for each template\n",
    "    for key in data_env_dict: #for each envelope\n",
    "        p, shift, relative_p = pick_time(ref_env=data_env_dict[list(data_env_dict.keys())[0]], \n",
    "                data_env_dict=data_env_dict[key],st=temps_s[key].st) #calculate picktimes\n",
    "        pick_times[key] = relative_p #save to dictionary\n",
    "    print(f'{cl} offsets are {pick_times}') #print pick times relative to first template stream (can be negative)\n",
    "    \n",
    "    #arranging picktimes from 0 (earliest station) to later (positive) times\n",
    "    #will NOT be used for plotting picktimes, but will be used for location\n",
    "    dif_dict = {} #dictionary of picktimes in reference to earliest picktime (in positive seconds after the earliest picktime)\n",
    "    max_value = max(pick_times, key=pick_times.get) #get key for max value of pick_times aka the earliest picktime\n",
    "    for key in pick_times: #for each picktime\n",
    "        dif = round(abs(pick_times[max_value] - pick_times[key]),4) #max value minus current value\n",
    "        dif_dict[key] = dif #save to dictionary with the same key as pick_times\n",
    "\n",
    "    \n",
    "########################################################################    \n",
    "#                       PLOTTING PICK TIMES                            #\n",
    "########################################################################\n",
    "\n",
    "#     cmap = get_cmap(len(temps_s)) #get cmap aka colors for the plot, see def(get_cmap) for color palette\n",
    "#     plt.figure(figsize=(10,10)) #set plot size\n",
    "#     plt.title('aligned templates, vlines are template starts') #plot title\n",
    "#     for tt,t in enumerate(temps_s): #for every template\n",
    "#         shift = round(pick_times[t]*fs) #find shift based on picktimes\n",
    "#         st0 = temps_s[t].st.copy() #make a copy for a reference\n",
    "#         maxdata = len(temps_s[t].st[0].data[:]) #find maximum length of template stream\n",
    "\n",
    "#         empty = Trace(np.zeros(shift)) #an empty trace/a trace filled with zeros\n",
    "#         if shift<0: #if shift is negative\n",
    "#             temps_s[t].st[0].data[:shift]=st0[0].data[-shift:] #shift to the left\n",
    "#         if shift>0: #if shift is positive\n",
    "#             temps_s[t].st[0].data[shift:]=st0[0].data[:-shift] #shift to the right\n",
    "#             temps_s[t].st[0].data[:shift]=empty.data[:shift] #get rid of leftover data from shift\n",
    "#         #note: if shift == 0, will be plotting with no shifting\n",
    "#         plt.plot(temps_s[t].st[0].data[:]/np.max(np.abs(temps_s[t].st[0].data[:]))+2*tt,color=cmap(tt), label=t) #plot stream\n",
    "#         plt.vlines(shift,ymin=-1,ymax=2*len(temps_s),color=cmap(tt)) #plot line in same color representing the start of the template\n",
    "#         plt.legend() #show the legend\n",
    "    \n",
    "#     first_sta = max(pick_times, key=pick_times.get) #gives you the template name for the first\n",
    "    #station to get a signal (largest/most positive pick time)\n",
    "    \n",
    "########################################################################    \n",
    "#                         FINDING LOCATION                             #\n",
    "########################################################################\n",
    "\n",
    "    # define input parameters\n",
    "    arrivals = [] #relative picktimes, dif_dict as a list\n",
    "    sta_lats = [] #station latitudes, from metadata\n",
    "    sta_lons = [] #station longitudes, from metadata\n",
    "    netsta_names = [] #list of station names with networks\n",
    "    for key in dif_dict: #for each station\n",
    "        arrivals.append(dif_dict[key]) #append pick time to arrivals\n",
    "        \n",
    "        #finding station name\n",
    "        if not key.endswith(str(cl).zfill(cllen)): #if the wrong cluster id\n",
    "            print('template name does not match cluster ID') #print an error\n",
    "            continue #and skip the rest\n",
    "        if key.endswith(f'{zz}rp{volc_list_names[vv][:2].lower()}{str(cl).zfill(cllen)}'): #if the key has a channel name in it (the 'hz')\n",
    "            md_netsta = key[:-(7+cllen)] #remember the stuff before channel name rpvo and cluster ID\n",
    "        if not key.endswith(f'{zz}rp{volc_list_names[vv][:2].lower()}{str(cl).zfill(cllen)}'): #if the key has NO channel name in it\n",
    "            md_netsta = key[:-(4+cllen)] #remember the stuff before rpvo and cluster ID\n",
    "#         print(md_netsta)\n",
    "        lat = volc_md[volc_md['netsta']==md_netsta.upper()]['Latitude'].values.tolist() #get latitude from metadata\n",
    "        sta_lats.append(lat[0]) #append\n",
    "        \n",
    "        lon = volc_md[volc_md['netsta']==md_netsta.upper()]['Longitude'].values.tolist() #get longitude form metadata\n",
    "        sta_lons.append(lon[0]) #append\n",
    "        \n",
    "        netsta_names.append(md_netsta.upper()) # make list of station names\n",
    "\n",
    "\n",
    "    # define grid origin in lat,lon\n",
    "    \n",
    "    #finding bottom left corner of grid map\n",
    "    lat_start = volc_lat_lon[volc_list_names[vv]][0] - (grid_length/222000) #volcano lat minus half of grid length in decimal lat long\n",
    "    lon_start = volc_lat_lon[volc_list_names[vv]][1] - (grid_height/222000) #volcano long minus half of grid height in decimal lat long\n",
    "        \n",
    "    #station lat lons to x y\n",
    "    sta_x = []\n",
    "    sta_y = []\n",
    "    for i in range(len(sta_lats)):\n",
    "        x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "        y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "        sta_x.append(x_dist)\n",
    "        sta_y.append(y_dist)\n",
    "\n",
    "    # set grid points\n",
    "    x_vect = np.arange(0, grid_length, step)\n",
    "    y_vect = np.arange(0, grid_height, step)\n",
    "    t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "    vs = np.arange(vs_min,vs_max,vs_step)\n",
    "\n",
    "    print('yo')\n",
    "    # carry out the gridsearch\n",
    "    rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals)\n",
    "    print('here')\n",
    "    # find lowest error lat, lon, and origin time\n",
    "    loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "    \n",
    "    # find the lat and lon of the location index\n",
    "    loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "    err_thr = np.min(np.log10(rss_mat))+.05\n",
    "    thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "    diameter = error_diameter(thr_array)\n",
    "    \n",
    "    print('location lat lon',loc_lat,loc_lon)\n",
    "\n",
    "    # plot a spatial map of error for lowest-error origin time\n",
    "#     fig,ax = plt.subplots()\n",
    "#     ax.scatter(x_vect[loc_idx[1]],y_vect[loc_idx[2]],s=100,marker='*',c='r')\n",
    "#     im = ax.imshow(np.log10(rss_mat[loc_idx[0],:,:].T),origin=\"lower\",extent=[0,grid_length,0,grid_height])\n",
    "#     fig.colorbar(im)\n",
    "#     plt.show()\n",
    "    \n",
    "    row = [volc_list_names[vv],' '.join(netsta_names),' '.join([str(i) for i in arrivals]),cl,loc_lat,loc_lon]\n",
    "    with open(homedir+f'/locations/{volc_list_names[vv]}_Template_Locations.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "        file.close()\n",
    "    \n",
    "    stopwatch1=time() #note the time\n",
    "    print(f'{stopwatch1-stopwatch0} s for this cluster') #print time it took to go through this cluster\n",
    "    \n",
    "    #write into csv relative picktimes in seconds after first_sta, probably list separated by \n",
    "    #spaces, like how stations are saved in events, make sure index is same for the template \n",
    "    #name and picktime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae80e1f",
   "metadata": {},
   "source": [
    "Compare to ComCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a69b55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 11 #51 is a cluster on hood with no matches\n",
    "url = f'https://assets.pnsn.org/red/hood/clusters/{cl}.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53695837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME 2009-12-01T10:09:16.150 LAT 45.331 LON -121.721\n",
      "TIME 2009-12-01T10:19:51.000 LAT 45.331 LON -121.721\n"
     ]
    }
   ],
   "source": [
    "r = requests.get(url)\n",
    "\n",
    "matches = str(r.content.splitlines()[20]).split('Potential local match:')[1:]\n",
    "\n",
    "\n",
    "if len(matches)>0:\n",
    "    for m in matches:\n",
    "        m_feats = m.split(' ')\n",
    "        m_time = m_feats[1]\n",
    "        m_lat = float(m_feats[2][1:-1])\n",
    "        m_lon = float(m_feats[3][:-1])\n",
    "        print(f'TIME {m_time} LAT {m_lat} LON {m_lon}')\n",
    "\n",
    "else:\n",
    "    print('no ComCat matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ec052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
