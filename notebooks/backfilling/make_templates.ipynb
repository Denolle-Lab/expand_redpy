{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba47834",
   "metadata": {},
   "source": [
    "# Make Templates from REDpy Clusters using EQCorrScan\n",
    "Created: Jul 14, 2022\n",
    "\n",
    "Updated: Aug 8, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c47527",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bcc00",
   "metadata": {},
   "source": [
    "Import everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fb7363a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched-filter CPU is not compiled! Should be here: /home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/fast_matched_filter/lib/matched_filter_CPU.so\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy import Trace\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.signal.trigger import classic_sta_lta, plot_trigger, trigger_onset\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "# %matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"/data/wsd01/pnwstore/\")\n",
    "import eqcorrscan\n",
    "from eqcorrscan.core.match_filter import match_filter\n",
    "from eqcorrscan.core.match_filter.tribe import Tribe\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "\n",
    "# from pnwstore.mseed import WaveformClient\n",
    "# from obspy.core.utcdatetime import UTCDateTime\n",
    "\n",
    "# client = WaveformClient()\n",
    "client2 = Client('IRIS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda6a44",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd81674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "fs = config['fs']\n",
    "tb = config['tb']\n",
    "ta = config['ta']\n",
    "fqmin = config['fqmin']\n",
    "fqmax = config['fqmax']\n",
    "nbucket = config['nbucket']\n",
    "chan = config['chan']\n",
    "homedir = config['homedir']\n",
    "readdir = config['readdir']\n",
    "f_o = config['f_o']\n",
    "p_len = config['p_len']\n",
    "\n",
    "vv = config['vv']\n",
    "\n",
    "snr_t = config['snr_t']\n",
    "thr_on = config['thr_on']\n",
    "thr_off = config['thr_off']\n",
    "nsta = config['nsta']\n",
    "nlta = config['nlta']\n",
    "pr = config['pr']\n",
    "\n",
    "print(vv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e985db6",
   "metadata": {},
   "source": [
    "Read REDpy Catalogs and Volcano Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc13b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baker = pd.read_csv(readdir+'Baker_catalog.csv')\n",
    "Hood = pd.read_csv(readdir+'Hood_catalog.csv')\n",
    "\n",
    "\n",
    "St_Helens = pd.read_csv(readdir+'MountStHelens_catalog.csv')\n",
    "\n",
    "# Combining borehole and local catalogs with St_Helens\n",
    "\n",
    "Helens_Borehole = pd.read_csv(readdir+'MSHborehole_catalog.csv')\n",
    "Helens_Borehole['Clustered'] += 2000 \n",
    "# Cluster 0 in Helens_Borehole is now Cluster 2000 in St_Helens\n",
    "Helens_Local = pd.read_csv(readdir+'MSHlocal_catalog.csv')\n",
    "Helens_Local['Clustered'] += 3000\n",
    "# Cluster 0 in Helens_Local is now Cluster 3000 in St_Helens\n",
    "\n",
    "# Use St_Helens to access all three St Helens catalogs\n",
    "St_Helens = pd.concat([St_Helens,Helens_Borehole,Helens_Local])\n",
    "\n",
    "Newberry = pd.read_csv(readdir+'Newberry_catalog.csv')\n",
    "Rainier = pd.read_csv(readdir+'Rainier_catalog.csv')\n",
    "\n",
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv')\n",
    "# read metadata file to create dataframe of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f787bb",
   "metadata": {},
   "source": [
    "Associate networks and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c95d0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6a446",
   "metadata": {},
   "source": [
    "Create Lists of Stations for Each Volcano Using volc_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ba8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baker_sta = volc_md[volc_md['Volcano_Name'] == 'Baker']['netsta'].values.tolist()\n",
    "Hood_sta = volc_md[volc_md['Volcano_Name'] == 'Hood']['netsta'].values.tolist() \n",
    "St_Helens_sta = volc_md[volc_md['Volcano_Name'] == 'St_Helens']['netsta'].values.tolist()\n",
    "Newberry_sta = volc_md[volc_md['Volcano_Name'] == 'Newberry']['netsta'].values.tolist() \n",
    "Rainier_sta = volc_md[volc_md['Volcano_Name'] == 'Rainier']['netsta'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c6e2f",
   "metadata": {},
   "source": [
    "Create Lists of Volcano Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f5e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_list = [Baker,Hood,Newberry,Rainier,St_Helens] # list of dataframes for each volcano\n",
    "volc_list_names = ['Baker','Hood','Newberry','Rainier','St_Helens'] # list of names of each volcano\n",
    "volc_sta = [Baker_sta,Hood_sta,Newberry_sta,Rainier_sta,St_Helens_sta] # lists of stations connected to respective volcanoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24863e2",
   "metadata": {},
   "source": [
    "### Making the Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e36ab",
   "metadata": {},
   "source": [
    "Define update_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b47490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(data, streamdata, ibucket):\n",
    "    streamdata = np.expand_dims(streamdata, axis = 0)\n",
    "\n",
    "    if ibucket in data:\n",
    "        data[ibucket] = np.concatenate((data[ibucket], streamdata), axis = 0)\n",
    "    else:\n",
    "        data[ibucket] = streamdata\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e220cf8",
   "metadata": {},
   "source": [
    "Make and Save Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39a3c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# changed most (all?) .stats.station to sta\n",
    "\n",
    "# for vv,v in enumerate(volc_sta): #vv is the number in the list, v is the station list for current volcano\n",
    "v = volc_sta[vv]\n",
    "clid = volc_list[vv]['Clustered'].values.tolist() #find the largest cluster ID for a volcano to set range\n",
    "for s in range(0,len(v)): #loop through stations\n",
    "    net, sta =  v[s].split('.') #add specific network per station\n",
    "    t0 = time() #record time\n",
    "    #Create Dictionary data and DataFrame meta\n",
    "    data = {}\n",
    "    meta = pd.DataFrame(columns = [\n",
    "        \"source_id\", \"source_origin_time\", \"source_latitude_deg\", \"source_longitude_deg\", \"source_type\",\n",
    "        \"source_depth_km\", \"split\", \"source_magnitude\", \"station_network_code\", \"trace_channel\", \n",
    "        \"station_code\", \"station_location_code\", \"station_latitude_deg\",  \"station_longitude_deg\",\n",
    "        \"station_elevation_m\", \"trace_name\", \"trace_sampling_rate_hz\", \"trace_start_time\",\n",
    "        \"trace_S_arrival_sample\", \"trace_P_arrival_sample\", \"CODE\"])\n",
    "\n",
    "    cid = [] #cid will become a list of cluster IDs that have templates\n",
    "    st3=obspy.Stream() #st3 will become a stream of traces, each trace being the template/stack for a cluster\n",
    "    for cl in range(0,(clid[-1]+1)): #cl is cluster ID\n",
    "    # clid[-1] is the highest cluster ID, and add 1 so that the last cluster id is not skipped\n",
    "        t2=time()\n",
    "        sst=obspy.Stream() #sst will have every trace for each cluster\n",
    "        print('------------') #divider for clarity\n",
    "        print('Cluster ID: '+str(cl)+' Volcano: '+volc_list_names[vv]+' Station: '+sta+' Network: '+net) #keeping track of what is currently running\n",
    "        for ii,i in enumerate(volc_list[vv][volc_list[vv]['Clustered'] == cl]['datetime']): #i is each datetime from cl at this volcano\n",
    "            print(i)\n",
    "            stt=UTCDateTime(i)-tb-nlta #starttime\n",
    "            et=UTCDateTime(i)+ta #endtime\n",
    "            utct=UTCDateTime(i) #datetime from REDpy catalog\n",
    "            try:\n",
    "                st = client.get_waveforms(network=net,station=sta,location='*',\n",
    "                                          channel=chan, year=utct.year, doy=utct.julday)\n",
    "                st = st.detrend(type = 'demean')\n",
    "                st.filter(type='bandpass',freqmin=fqmin,freqmax=fqmax)\n",
    "                st.resample(fs) #get same sampling rate for all events\n",
    "                st.trim(starttime=stt,endtime=et)\n",
    "                st.merge(fill_value = 0)\n",
    "            except:\n",
    "                print('Could not find waveform')\n",
    "                continue #if no waveform can be found, move onto next time\n",
    "            if len(st)==0:\n",
    "                print('no data for this time')\n",
    "                continue\n",
    "            print(len(st[0].data))\n",
    "            if len(st[0].data) == round((ta+tb+nlta)*fs)+1: #if there is enough data to contribute to making a stack\n",
    "                sst.append(st[0])\n",
    "    #         break\n",
    "        print(sst) #see how many traces were found\n",
    "        st2=sst.copy() #copy of sst after appending is finished/all waveforms for a cluster are gathered, reference for shifting\n",
    "        st4=st2.copy() #st4 will become the aligned version of st2\n",
    "        for i in range(0, len(st2)):\n",
    "            print('Working on shifting') # Will not print if len(st2)==0\n",
    "            # Also serves as a divider between shift and cc values for different waveforms\n",
    "            xcor = obspy.signal.cross_correlation.correlate(st2[i].data[:],sst[0].data[:],200)\n",
    "            index = np.argmax(xcor)\n",
    "            cc = round(xcor[index],9)\n",
    "            shift = 200-index\n",
    "            print(shift,cc) #the shift and cross correlation values before shifting/alignment. Perfect shift is 0, perfect cross correlation is 1\n",
    "            if shift<0: st4[i].data[:shift]=st2[i].data[-shift:]\n",
    "            if shift>0: st4[i].data[shift:]=st2[i].data[:-shift]\n",
    "            xcor = obspy.signal.cross_correlation.correlate(st4[i].data[:],sst[0].data[:],200)\n",
    "            index = np.argmax(xcor)\n",
    "            cc = round(xcor[index],9)\n",
    "            shift = 200-index\n",
    "            print(shift,cc) #shift and cross correlation values after alignment. shift should be 0\n",
    "        print('shift complete')\n",
    "        print( )\n",
    "        sst2=st4.copy() #sst2 is a copy of st4 once alignment is finished\n",
    "        sst2.stack() #stack aligned waveforms\n",
    "        if len(sst)<2: #some clusters will have 1 or 0 traces due to unavailable data\n",
    "            print('sst not long enough')\n",
    "            t4=time()\n",
    "            print(str(t4-t2)+' seconds to attempt to find waveforms')\n",
    "            continue\n",
    "\n",
    "        print('length of sst: '+str(len(sst))) #should be 2 or higher\n",
    "        st3.append(sst2[0])\n",
    "\n",
    "        cid.append('rp'+volc_list_names[vv][:2].lower()+str(cl).zfill(len(str(clid[-1])))) #append the clusterID to cid. rp for REDpy, volc_list_names[vv][:2].lower() for the volcano name\n",
    "        # writing seisbench h5py\n",
    "\n",
    "        lat = volc_md[volc_md['Station'] == sta]['Latitude'].values[0]\n",
    "        lon = volc_md[volc_md['Station'] == sta]['Longitude'].values[0]\n",
    "       # fill in metadata \n",
    "        meta = meta.append({\"source_id\": cid[-1], \"source_origin_time\": '', \n",
    "            \"source_latitude_deg\": \"%.3f\" % 0, \"source_longitude_deg\": \"%.3f\" % 0, \n",
    "            \"source_type\": 'unknown',\n",
    "            \"source_depth_km\": \"%.3f\" % 0, \"source_magnitude\": 0,\n",
    "            \"station_network_code\": net, \"trace_channel\": sst[0].stats.channel, \n",
    "            \"station_code\": sta, \"station_location_code\": sst[0].stats.location,\n",
    "            \"station_latitude_deg\": lat,  \"station_longitude_deg\": lon,\n",
    "            \"station_elevation_m\": 0,\n",
    "            \"trace_p_arrival_sample\": 0, \"CODE\": sta.lower()+'bhz'+cid[-1]}, ignore_index = True)\n",
    "\n",
    "        # fill in data\n",
    "        ibucket = np.random.choice(list(np.arange(nbucket) + 1))\n",
    "        data = update_data(data, st3[-1], ibucket)\n",
    "        print('ibucket: ',ibucket,data[ibucket])\n",
    "        t3=time()\n",
    "        print(str(t3-t2)+' seconds to make the stack for this cluster')\n",
    "# BREAK FOR THE CLUSTER LOOP\n",
    "#         break\n",
    "    t1=time()\n",
    "    print(str(t1-t0)+'seconds to make stacks for one station for all clusters with enough data')\n",
    "    \n",
    "    # integrate tribe\n",
    "    stack_list = [] #will be a list of streams,each stream has one stack as a trace\n",
    "    for i in range(0,len(st3)):\n",
    "        temp = obspy.Stream(st3[i]) #give each trace in st3 its own stream\n",
    "        stack_list.append(temp) #append each stream to stack_list\n",
    "        print('appending stack list')\n",
    "    print(stack_list)\n",
    "\n",
    "    print('got stack list!')\n",
    "\n",
    "    #filter for snr with temp list, remember to keep cid in order\n",
    "    cidu = [] #updated cid\n",
    "    temp_list = []\n",
    "    for ll,l in enumerate(stack_list):\n",
    "        cft = classic_sta_lta(l[0].data, int(nsta * fs), int(nlta * fs)) #basis for stalta\n",
    "        print('-------------')\n",
    "        print(l[0].stats.starttime+nlta)\n",
    "        plot_trigger(l[0], cft, thr_on, thr_off) #print stalta plots\n",
    "        print('C ID:', cid[ll])\n",
    "        on_off = np.array(trigger_onset(cft, thr_on, thr_off)) #x value of vlines in the plot (plot_trigger is NOT needed for this to work)\n",
    "        if not np.any(on_off):\n",
    "            print('NO SIGNAL FOUND')\n",
    "            continue\n",
    "        index = []\n",
    "        for ii,i in enumerate(on_off): #for each signal detection in on_off\n",
    "        #     print(i)\n",
    "            if i[1]-i[0] == 0: #if the start and end times are same, mark\n",
    "                index.append(ii)\n",
    "        on_off = np.delete(on_off,index, axis=0) #remove marked signal windows by index\n",
    "        print('on_off')\n",
    "        print(on_off)\n",
    "        amps = []\n",
    "        for ii,i in enumerate(on_off): #go through each possible signal window\n",
    "            start = i[0]\n",
    "            end = i[1]\n",
    "            amps.append(Trace(data=l[0].data[start:end]).max()) #find max amplitude in that signal window\n",
    "        # print(amps)\n",
    "        amp = max(amps, key=abs) #find max amplitude of stream within sta/lta windows\n",
    "        # print(amp)\n",
    "        for ii,i in enumerate(on_off): #go through each possible signal window\n",
    "            start = i[0]\n",
    "            end = i[1]\n",
    "            ooamp = Trace(data=l[0].data[start:end]).max() #find max amplitude in that signal window\n",
    "            print('amp',amp,'ooamp',ooamp,'start',start,'end',end)\n",
    "            if amp == ooamp: #if max amplitude of this window is the same as the overall max amplitude\n",
    "                print('wow')\n",
    "                trig_on = round(float(start / fs),4) #took out -nlta\n",
    "                trig_off = round(float(end / fs),4) #get start and end times\n",
    "                break #move on\n",
    "        # show trigger on and off times, rounded to 4 decimal places\n",
    "        print('Trigger on',trig_on-nlta,'seconds after tb time') #trig_on-nlta is picktime...?\n",
    "        print('Trigger off',trig_off-nlta,'seconds after tb time')\n",
    "\n",
    "        signal_window = l[0].copy()\n",
    "        noise_window = l[0].copy()\n",
    "\n",
    "        signal_window.trim(starttime=l[0].stats.starttime+trig_on,endtime=l[0].stats.starttime+trig_off)\n",
    "        #i+trig_on-0.5 to include lead up to the signal\n",
    "        noise_window.trim(starttime=l[0].stats.starttime,endtime=l[0].stats.starttime+nlta)\n",
    "\n",
    "        snr = 20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                          / np.percentile(np.abs(noise_window.data),pr))/np.log(10)\n",
    "        print('snr:',snr)\n",
    "        if snr>=snr_t: #if SNR is greater than or equal to minimum, save it\n",
    "            l.trim(starttime=l[0].stats.starttime+nlta,endtime=l[0].stats.endtime)\n",
    "            cidu.append(cid[ll])\n",
    "            temp_list.append(l)\n",
    "    \n",
    "\n",
    "    template_stream = [] #will be a list of streams that have been filtered to become templates\n",
    "    for i in range(0,len(temp_list)):\n",
    "        template_stream.append(eqcorrscan.core.match_filter.template.Template(\n",
    "            name=sta.lower()+temp_list[i][0].stats.channel.lower()+cidu[i],st=temp_list[i], \\\n",
    "                lowcut=fqmin, highcut=fqmax, samp_rate=fs, filt_order=f_o, process_length=p_len))\n",
    "        #filter each stream from temp_list and append to template_stream\n",
    "        print('appending template stream')\n",
    "    print('made template stream!')\n",
    "\n",
    "    tribe = eqcorrscan.core.match_filter.tribe.Tribe(templates = template_stream)\n",
    "    #make a tribe from template_stream, each template will be made from a stream in template_stream\n",
    "#     print('making tribe...')\n",
    "\n",
    "    if len(st3) > 0:\n",
    "        print('Writing tribe in progress...')\n",
    "        tribe.write(homedir+'/templates/Volcano_' + volc_list_names[vv] + '_Network_' + \\\n",
    "                    net + '_Station_' + sta + '_Channel_' + st3[0].stats.channel)\n",
    "#         print('Wrote tribe sucessfully!')\n",
    "        # add to file\n",
    "        meta.to_csv(\"/data/whd02/Data_rp/metadata_\"+volc_list_names[vv]+\"_\"+net+\"_\"+sta+\".csv\",sep = ',', index=False)\n",
    "        f = h5py.File(\"/data/whd02/Data_rp/waveforms_\"+volc_list_names[vv]+\"_\"+net+\"_\"+sta+\".hdf5\",'a') #appending mode\n",
    "            #If the file does not exist, it creates a new file for writing.\n",
    "        # need to define f in order to close it in order to open it in mode w\n",
    "        if f: f.close()\n",
    "        f = h5py.File(\"/data/whd02/Data_rp/waveforms_\"+volc_list_names[vv]+\"_\"+net+\"_\"+sta+\".hdf5\",'w') #writing mode\n",
    "#         f['/data_format/component_order'] ='ZNE'\n",
    "        print(range(nbucket))\n",
    "        for b in range(nbucket):\n",
    "            f['/data/bucket%d' % (b + 1)] = data[b + 1]\n",
    "        f.close()\n",
    "\n",
    "        print('Saved!')\n",
    "# BREAK FOR THE STATION LOOP\n",
    "#     break\n",
    "# BREAK FOR THE VOLCANO LOOP\n",
    "#     break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo 3.8 (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
