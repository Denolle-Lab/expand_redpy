{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6db518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched-filter CPU is not compiled! Should be here: /home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/fast_matched_filter/lib/matched_filter_CPU.so\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import eqcorrscan\n",
    "from eqcorrscan import Tribe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from glob import glob\n",
    "from obspy import UTCDateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8defc55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    \n",
    "readdir = config['readdir']\n",
    "homedir = config['homedir']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc563ab",
   "metadata": {},
   "source": [
    "# find X\n",
    "29,404 number of templates from X number of waveforms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c61384b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3523\n"
     ]
    }
   ],
   "source": [
    "#read in REDPy catalogs and metadata\n",
    "Baker = pd.read_csv(readdir+'Baker_catalog.csv')\n",
    "Hood = pd.read_csv(readdir+'Hood_catalog.csv')\n",
    "\n",
    "St_Helens = pd.read_csv(readdir+'MountStHelens_catalog.csv')\n",
    "\n",
    "# Combining borehole and local catalogs with St_Helens\n",
    "\n",
    "Helens_Borehole = pd.read_csv(readdir+'MSHborehole_catalog.csv')\n",
    "Helens_Borehole['Clustered'] += 2000 \n",
    "# Cluster 0 in Helens_Borehole is now Cluster 2000 in St_Helens\n",
    "Helens_Local = pd.read_csv(readdir+'MSHlocal_catalog.csv')\n",
    "Helens_Local['Clustered'] += 3000\n",
    "# Cluster 0 in Helens_Local is now Cluster 3000 in St_Helens\n",
    "\n",
    "# Use St_Helens to access all three St Helens catalogs\n",
    "St_Helens = pd.concat([St_Helens,Helens_Borehole,Helens_Local])\n",
    "clid = np.unique(St_Helens['Clustered'].values.tolist()) #find the largest cluster ID for a volcano to set range\n",
    "print(clid[-1])\n",
    "\n",
    "Newberry = pd.read_csv(readdir+'Newberry_catalog.csv')\n",
    "Rainier = pd.read_csv(readdir+'Rainier_catalog.csv')\n",
    "\n",
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv')\n",
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str)\n",
    "# read metadata file to create dataframe of labels\n",
    "\n",
    "#list of catalogs\n",
    "volc_dict = {'Baker': Baker,'Hood': Hood,'Newberry': Newberry,'Rainier': Rainier,'St_Helens': St_Helens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf765c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#number of REDPy events for each cluster on each station for the amount of time the station was active\n",
    "\n",
    "cl_event_dict = {} #dictionary, keys = clusterIDs, value = number of events on that cluster\n",
    "\n",
    "events_per_station = []\n",
    "for index, row in volc_md.iterrows(): #through each station (row of volc_md)\n",
    "    volc_name = row['Volcano_Name']\n",
    "    net = row['Network']\n",
    "    sta = row['Station']\n",
    "    #call tgz file\n",
    "    try:\n",
    "        T = Tribe().read(*glob(f'{homedir}templates/Volcano_{volc_name}_Network_{net}_Station_{sta}_Channel_*.tgz'))\n",
    "    except:\n",
    "        print(f'{net}.{sta} tgz does not exist')\n",
    "        print(*glob(f'{homedir}templates/Volcano_{volc_name}_Network_{net}_Station_{sta}_Channel_*.tgz'))\n",
    "        continue\n",
    "    t_amt = len(T) #number of templates, inherently accounts for time station was active\n",
    "    #duration of station:\n",
    "    starttime = UTCDateTime(row['Starttime']) #starttime for the station\n",
    "    endtime = UTCDateTime(row['Endtime']) #endtime for the station\n",
    "\n",
    "    events_per_cluster = []\n",
    "    for t in T:\n",
    "        #get clusterID\n",
    "        if volc_name=='Baker' or volc_name=='Hood' or volc_name=='Newberry' or volc_name=='Rainier':\n",
    "            cl = t.name[-3:] #account for zfill, to parameterize, use what make_templates uses to get zfill amount (cid[-1] from volcmd)\n",
    "        if volc_name=='St_Helens':\n",
    "            cl = t.name[-4:] #account for zfill\n",
    "        \n",
    "#         print('---',cl)\n",
    "        \n",
    "        catalog = volc_dict[volc_name]\n",
    "        catalog_events = catalog[catalog['Clustered']==int(cl)]['datetime'].values.tolist()#list of event times for this clusterID #from REDPy catalog\n",
    "        cl_events = [UTCDateTime(i) for i in catalog_events] #to datetime\n",
    "#         print(f'original | len: {len(cl_events)} |')\n",
    "        for event in cl_events:\n",
    "            if event < starttime:\n",
    "                cl_events.remove(event)#remove it\n",
    "#                 print('removed')\n",
    "                continue #skip because it is already removed\n",
    "            if event > endtime:\n",
    "                cl_events.remove(event)#remove it\n",
    "#                 print('removed')\n",
    "#         print(f'updated | len: {len(cl_events)} |')\n",
    "        amt_events = len(cl_events) #length of cl_events after removals\n",
    "        events_per_cluster.append(amt_events)\n",
    "        print(amt_events,end=' ')\n",
    "    print(' ')\n",
    "    events_per_station.append(sum(events_per_cluster))\n",
    "    print(f'events per station {net}.{sta}',events_per_station)\n",
    "#     break\n",
    "    \n",
    "total_events = sum(events_per_station)\n",
    "print(total_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a33ea93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "369655\n"
     ]
    }
   ],
   "source": [
    "print(total_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8dd5b",
   "metadata": {},
   "source": [
    "#------#\n",
    "\n",
    "total events = 369655 = X\n",
    "\n",
    "#------#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eab9e2",
   "metadata": {},
   "source": [
    " # find Y\n",
    " To do Y cross-correlations (minimum estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948aeee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# of cross correlations per day:\n",
    "cc_per_chunk = round(200/ 78) #if overlap is minimal, how many cross correlation per chunk (of 200s)\n",
    "print(cc_per_chunk)\n",
    "chunks_per_day = 86400 / 200 #how many chunks per day\n",
    "\n",
    "min_cc_per_day = chunks_per_day*cc_per_chunk\n",
    "print(min_cc_per_day)\n",
    "\n",
    "\n",
    "list_of_cc_per_station = []\n",
    "for index, row in volc_md.iterrows(): #through each station (row of volc_md)\n",
    "    volc_name = row['Volcano_Name']\n",
    "    net = row['Network']\n",
    "    sta = row['Station']\n",
    "    print('----',volc_name,net,sta)\n",
    "    #call tgz file\n",
    "    try:\n",
    "        T = Tribe().read(*glob(f'{homedir}templates/Volcano_{volc_name}_Network_{net}_Station_{sta}_Channel_*.tgz'))\n",
    "    except:\n",
    "        print(f'{net}.{sta} tgz does not exist')\n",
    "        print(*glob(f'{homedir}templates/Volcano_{volc_name}_Network_{net}_Station_{sta}_Channel_*.tgz'))\n",
    "        continue\n",
    "    t_amt = len(T)#number of templates for that station, from .tgz file\n",
    "    \n",
    "    starttime = UTCDateTime(row['Starttime']) #starttime for the station\n",
    "    if starttime < UTCDateTime('2002-01-01T00:00:00'): #only account for time I'm running over, not the entire duration\n",
    "        starttime = UTCDateTime('2002-01-01T00:00:00')\n",
    "    endtime = UTCDateTime(row['Endtime']) #endtime for the station\n",
    "    if endtime > UTCDateTime('2022-01-01T00:00:00'): #only account for time I'm running over, not the entire duration\n",
    "        endtime = UTCDateTime('2022-01-01T00:00:00')\n",
    "        \n",
    "    sta_len = (endtime-starttime)/86400 #station duration in s / (s/day) = station duration in days\n",
    "    cc_per_sta = t_amt*sta_len*min_cc_per_day #templates to run * number of days to run over * cc per day\n",
    "    list_of_cc_per_station.append(cc_per_sta)\n",
    "    \n",
    "    \n",
    "total_cc = sum(list_of_cc_per_station)\n",
    "print(total_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28902a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184156214487.41998\n"
     ]
    }
   ],
   "source": [
    "print(total_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807f9335",
   "metadata": {},
   "source": [
    "#------#\n",
    "\n",
    "minimum total cc calculations = 184156214487 = Y\n",
    "\n",
    "#------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7179fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo 3.8 (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
