{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab72beec",
   "metadata": {},
   "source": [
    "Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55331a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from time import time\n",
    "import sys\n",
    "sys.path.append(\"/data/wsd01/pnwstore/\")\n",
    "import csv\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972b26fa",
   "metadata": {},
   "source": [
    "Print commands as strings so I can copy and paste them into main Find_Events cell (maybe make it's own script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e590e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOVE BELOW PARAMS\n",
    "#NOT YET TESTED\n",
    "#make station_list (can pull from volcano metadata)\n",
    "for ii,i in enumerate(station_list): #set variables\n",
    "    print('sta'+str(ii)+' = '+i)\n",
    "for ii,i in enumerate(station_list): #set reads\n",
    "    print('readsta'+str(ii)+' = pd.read_csv('+homedir+'detections/'+volc+'_sta'+str(ii)+'_'+year+'_clean_detections.csv')\n",
    "sta_list = []\n",
    "for ii,i in enumerate(station_list):\n",
    "    sta_list.append('sta'+str(ii))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03db1b6",
   "metadata": {},
   "source": [
    "Parameters and dataframes for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386de310",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc = 'Hood' #integrate these as parameters in the actual code\n",
    "year = '2019'\n",
    "\n",
    "homedir = '/home/smocz/redpy_expand_new_files/' #home directory or directory to save new files to\n",
    "datadir = '/data/wsd01/HOOD_data/UW/'+str(year)+'/' #directory to get data from\n",
    "\n",
    "#station names\n",
    "sta0 = 'HOOD'\n",
    "sta1 = 'PALM'\n",
    "sta2 = 'TIMB'\n",
    "sta3 = 'TDH'\n",
    "sta4 = 'SHRK'\n",
    "#panda dataframe for detections at a station\n",
    "readsta0 = pd.read_csv(homedir+'detections/'+volc+'_'+sta0+'_'+year+'_clean_detections.csv') \n",
    "readsta1 = pd.read_csv(homedir+'detections/'+volc+'_'+sta1+'_'+year+'_clean_detections.csv')\n",
    "readsta2 = pd.read_csv(homedir+'detections/'+volc+'_'+sta2+'_'+year+'_clean_detections.csv')\n",
    "readsta3 = pd.read_csv(homedir+'detections/'+volc+'_'+sta3+'_'+year+'_clean_detections.csv')\n",
    "readsta4 = pd.read_csv(homedir+'detections/'+volc+'_'+sta4+'_'+year+'_clean_detections.csv')\n",
    "#list of stations for saving (must be in order)\n",
    "sta_list = [sta0,sta1,sta2,sta3,sta4]\n",
    "#time in seconds before and after a detection to check for similar detections\n",
    "wi = 13 #was 1\n",
    "#concat of station dataframes for running through all detections\n",
    "readsta = pd.concat([readsta0,readsta1,readsta2,readsta3,readsta4])\n",
    "\n",
    "#NOTE FOR EDITING: be sure to also update below cell when adding stations\n",
    "#new loop per station, update sta#times and readsta#, update match#, and add match# to match_list\n",
    "\n",
    "#maybe concatenate the detections from every station, then see if they match across all \n",
    "#tations, there will be min one match (the station it's from), but that's ok because we \n",
    "#can just do if it has at least 4 detections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a18d50",
   "metadata": {},
   "source": [
    "# NOTE FOR EDITING: be sure to also update below cell when adding stations\n",
    "### new loop per station, update sta#times and readsta#, update match#, and add match# to match_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "with open(homedir+'events/'+volc+'_'+year+'_events.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Earliest_Detection_Time\",\"Cluster_ID\",\"Stations_Found\",\"Stations\"])\n",
    "    #detection time, cluster id (no longer needs to be separated by station), and number of stations with a \n",
    "    #detection for this event\n",
    "    file.close()\n",
    "\n",
    "#for all detections we want to run (concatenated)\n",
    "temp_name_list = readsta['Template_Name'].values.tolist() #make a list of template names\n",
    "cl_list_long = [] # make a list of the numbers in each template name\n",
    "for i in temp_name_list: \n",
    "    num = re.findall(r'\\d+', i)\n",
    "    cl_list_long.append(*num)\n",
    "cl_list = np.unique(cl_list_long) #get rid of duplicates\n",
    "\n",
    "#for cluster x\n",
    "for cl in cl_list:\n",
    "    t1 = time()\n",
    "#for detection x\n",
    "    times = []\n",
    "    for i in np.unique(temp_name_list):\n",
    "        if i.endswith(cl):\n",
    "            all_times = readsta[readsta['Template_Name']==i]['Detection_Time'].values.tolist()\n",
    "            for at in all_times:\n",
    "                times.append(at)\n",
    "    for ii,i in enumerate(times):\n",
    "        t3 = time()\n",
    "#run through each detection time on all stations\n",
    "        print( )\n",
    "        print('------------')\n",
    "        print('trying detection',ii,'cluster',cl,i)\n",
    "        \n",
    "        sta0times = []\n",
    "        for a in np.unique(readsta0['Template_Name'].values.tolist()):\n",
    "            if a.endswith(cl):\n",
    "                all_times = readsta0[readsta0['Template_Name']==a]['Detection_Time'].values.tolist()\n",
    "                for at in all_times:\n",
    "                    sta0times.append(at)\n",
    "        match=0 #set variable to arbitrary number\n",
    "        for tt,t in enumerate(sta0times):\n",
    "            ts = UTCDateTime(t)-wi\n",
    "            te = UTCDateTime(t)+wi\n",
    "            if UTCDateTime(i)>ts and UTCDateTime(i)<te:\n",
    "                match=2 #if there is an overlap, reset the variable and break out of the loop\n",
    "                print('Overlap with station '+sta0+' detections')\n",
    "        \n",
    "        sta1times = []\n",
    "        for a in np.unique(readsta1['Template_Name'].values.tolist()):\n",
    "            if a.endswith(cl):\n",
    "                all_times = readsta1[readsta1['Template_Name']==a]['Detection_Time'].values.tolist()\n",
    "                for at in all_times:\n",
    "                    sta1times.append(at)\n",
    "        match0=0 #set variable to arbitrary number\n",
    "        for tt,t in enumerate(sta1times):\n",
    "            ts = UTCDateTime(t)-wi\n",
    "            te = UTCDateTime(t)+wi\n",
    "            if UTCDateTime(i)>ts and UTCDateTime(i)<te:\n",
    "                match0=2 #if there is an overlap, reset the variable and break out of the loop\n",
    "                print('Overlap with station '+sta1+' detections')\n",
    "\n",
    "        sta2times = []\n",
    "        for a in np.unique(readsta2['Template_Name'].values.tolist()):\n",
    "            if a.endswith(cl):\n",
    "                all_times = readsta2[readsta2['Template_Name']==a]['Detection_Time'].values.tolist()\n",
    "                for at in all_times:\n",
    "                    sta2times.append(at)\n",
    "        match1=0 #set variable to arbitrary number\n",
    "        for tt,t in enumerate(sta2times):\n",
    "            ts = UTCDateTime(t)-wi\n",
    "            te = UTCDateTime(t)+wi\n",
    "            if UTCDateTime(i)>ts and UTCDateTime(i)<te:\n",
    "                match1=2 #if there is an overlap, reset the variable and break out of the loop\n",
    "                print('Overlap with station '+sta2+' detections')\n",
    "                sta0times = readsta0['Detection_Time'].values.tolist()\n",
    "        \n",
    "        sta3times = []\n",
    "        for a in np.unique(readsta3['Template_Name'].values.tolist()):\n",
    "            if a.endswith(cl):\n",
    "                all_times = readsta3[readsta3['Template_Name']==a]['Detection_Time'].values.tolist()\n",
    "                for at in all_times:\n",
    "                    sta3times.append(at)\n",
    "        match2=0 #set variable to arbitrary number\n",
    "        for tt,t in enumerate(sta3times):\n",
    "            ts = UTCDateTime(t)-wi\n",
    "            te = UTCDateTime(t)+wi\n",
    "            if UTCDateTime(i)>ts and UTCDateTime(i)<te:\n",
    "                match2=2 #if there is an overlap, reset the variable and break out of the loop\n",
    "                print('Overlap with station '+sta3+' detections')\n",
    "            \n",
    "        sta4times = []\n",
    "        for a in np.unique(readsta4['Template_Name'].values.tolist()):\n",
    "            if a.endswith(cl):\n",
    "                all_times = readsta4[readsta4['Template_Name']==a]['Detection_Time'].values.tolist()\n",
    "                for at in all_times:\n",
    "                    sta4times.append(at)\n",
    "        match3=0 #set variable to arbitrary number\n",
    "        for tt,t in enumerate(sta4times):\n",
    "            ts = UTCDateTime(t)-wi\n",
    "            te = UTCDateTime(t)+wi\n",
    "            if UTCDateTime(i)>ts and UTCDateTime(i)<te:\n",
    "                match3=2 #if there is an overlap, reset the variable and break out of the loop\n",
    "                print('Overlap with station '+sta4+' detections')\n",
    "        \n",
    "        t2 = time()\n",
    "        print(t2-t3,'seconds to test overlap for detection',ii)\n",
    "        \n",
    "        match_list = [match, match0, match1, match2, match3]\n",
    "    \n",
    "        save_list = [] #for saving a list of stations\n",
    "        for mm,m in enumerate(match_list):\n",
    "            if m==2:\n",
    "                save_list.append(sta_list[mm])\n",
    "\n",
    "        if match_list.count(2) >= 4: #if at least 4 matches equal 2:\n",
    "#         if match==2 and match0==2 and match1==2 and match2==2:\n",
    "            print('saving...')\n",
    "            #ADD A CATCH: if an event of this cluster and this time give or take 1 second \n",
    "            #already exists, do NOT save\n",
    "            check = pd.read_csv(homedir+'events/'+volc+'_'+year+'_events.csv')\n",
    "            checktimes = check[check['Cluster_ID']==int(cl)]['Detection_Time'].values.tolist()\n",
    "            checking = 0\n",
    "            for tt,t in enumerate(checktimes):\n",
    "                ts = UTCDateTime(t)-wi\n",
    "                te = UTCDateTime(t)+wi\n",
    "                if UTCDateTime(i)>ts and UTCDateTime(i)<te: \n",
    "                    print('already recorded event')\n",
    "                    checking=1 #checking still =1 because we don't want the normal save process\n",
    "                    #if i is earlier (less than) than the recorded event, overwrite the event (we want the earliest detected time)\n",
    "                    if i<t:\n",
    "                        print('replacing...')\n",
    "                        print('old time:',t,'new time:',i)\n",
    "                        check.replace(t,i,inplace=True) #ONLY WORKS FOR THE DATAFRAME NOT THE CSV\n",
    "                        check.to_csv(homedir+'events/'+volc+'_'+year+'_events.csv',index=False)\n",
    "                        #read panda dataframe - change dataframe, overwrite the csv with this dataframe\n",
    "                        print('changed csv')\n",
    "                        #2+ clusters at the same time shouldn't happen because templates are 13s long, with \n",
    "                        #trig int of 6. any '2' events in the same window would just be one event and waves \n",
    "                        #shouldn't be so slow that a different one can be at the exact time (within 1 second \n",
    "                        #of the detection)\n",
    "                    break\n",
    "            if checking==1: continue\n",
    "            row = [i,int(cl),match_list.count(2),' '.join(save_list)]\n",
    "            print(row)\n",
    "            \n",
    "            with open(homedir+'events/'+volc+'_'+year+'_events.csv', 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(row)\n",
    "                file.close()\n",
    "    t4 = time()\n",
    "    print(t4-t3,'seconds for cluster',cl)\n",
    "t5 = time()\n",
    "print(t5-t0,'seconds to find all new events on',volc,'in',year)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
