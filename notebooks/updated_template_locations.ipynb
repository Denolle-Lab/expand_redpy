{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348ac66f",
   "metadata": {},
   "source": [
    "Created August 17, 2023\n",
    "\n",
    "Purpose is to update location grid search in the following ways:\n",
    "1. use templates whose constituent waveforms were normalized before stacking (fixes most false features)\n",
    "2. use elep to find picktimes instead of envelope cross correlation\n",
    "3. improved location search\n",
    "4. compare locations to ComCat potential matches from the <a href=\"https://assets.pnsn.org/red/\">redpy website</a> when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ebc586",
   "metadata": {},
   "source": [
    "last updated November 20, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import h5py\n",
    "import yaml\n",
    "import csv\n",
    "import math\n",
    "import eqcorrscan\n",
    "from eqcorrscan import Tribe\n",
    "from time import time\n",
    "import obspy\n",
    "from obspy import UTCDateTime, Trace\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from obspy.signal.cross_correlation import *\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy import distance\n",
    "from tqdm import trange\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "import torch\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "import seisbench.models as sbm\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "from ELEP.elep.ensemble_learners import ensemble_regressor_cnn\n",
    "from mbf_elep_func import apply_mbf\n",
    "from ELEP.elep import mbf, mbf_utils\n",
    "from ELEP.elep import trigger_func\n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "\n",
    "from ELEP.elep.mbf_utils import make_LogFq, make_LinFq, rec_filter_coeff, create_obspy_trace\n",
    "from ELEP.elep.mbf import MB_filter as MBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a076193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config file for parameters\n",
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "smooth_length = config['smooth_length']\n",
    "fs = config['fs']\n",
    "tb = config['tb']\n",
    "ta = config['ta']\n",
    "fqmin = config['fqmin']\n",
    "fqmax = config['fqmax']\n",
    "chan = config['chan']\n",
    "homedir = config['homedir']\n",
    "readdir = config['readdir']\n",
    "minsta = config['minsta']\n",
    "grid_length = float(config['grid_length'])\n",
    "grid_height = float(config['grid_height'])\n",
    "step = config['step']\n",
    "t_step = config['t_step']\n",
    "vs_min = config['vs_min']\n",
    "vs_max = config['vs_max']\n",
    "vs_step = config['vs_step']\n",
    "volc_lat_lon = config['volc_lat_lon']\n",
    "volc_list_names = config['volc_list_names']\n",
    "nlta = config['nlta']\n",
    "\n",
    "vv = config['vv']\n",
    "\n",
    "tape = 0.05 #percent of waveform to taper on either side\n",
    "\n",
    "# istart = config['nlta']*fs\n",
    "# print(istart)\n",
    "\n",
    "\n",
    "print(volc_list_names[vv])\n",
    "\n",
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv')\n",
    "#associate network and station\n",
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get clusterid from template name\n",
    "def getcl_id(t_name_str): #for normalized\n",
    "    t_cl = int(t_name_str.split('_')[-1])\n",
    "    return t_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81685e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML picker parameters\n",
    "paras_semblance = {'dt':0.025, 'semblance_order':4, 'window_flag':True, \n",
    "                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "p_thrd, s_thrd = 0.01, 0.05\n",
    "\n",
    "# download models\n",
    "pretrain_list = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\",\"neic\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "\n",
    "#list of models to run through\n",
    "list_models = [pn_pnw_model,pn_ethz_model,pn_scedc_model,pn_neic_model,pn_geofon_model,pn_stead_model,pn_instance_model]\n",
    "\n",
    "pn_pnw_model.to(device); #imodel 0\n",
    "pn_ethz_model.to(device); #imodel 1\n",
    "pn_scedc_model.to(device); #imodel 2\n",
    "pn_neic_model.to(device); #imodel 3\n",
    "pn_geofon_model.to(device); #imodel 4\n",
    "pn_stead_model.to(device); #imodel 5\n",
    "pn_instance_model.to(device); #imodel 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in templates for volcano\n",
    "\n",
    "all_temps = []\n",
    "all_waves = []\n",
    "\n",
    "for filepath in glob(f'/home/smocz/expand_redpy_new_files/h5/normalized_{volc_list_names[vv].lower()}_templates_*.h5'):\n",
    "    net = filepath.split('_')[-2]\n",
    "    with h5py.File(filepath, \"r\") as f: #pull in fingerprints\n",
    "        template_name = f[\"template_name\"][()]\n",
    "        waveforms = f[\"waveforms\"][()]\n",
    "#         print(f.keys()) #print what data is in this file\n",
    "    [all_temps.append(i) for i in template_name]\n",
    "    [all_waves.append(i) for i in waveforms]\n",
    "    \n",
    "all_waves = np.array(all_waves)\n",
    "all_temps = [str(i)[2:-1] for i in all_temps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some info\n",
    "v = volc_md[volc_md['Volcano_Name'] == volc_list_names[vv]]['netsta'].values.tolist() #list of network and station per volc\n",
    "clid = np.unique([getcl_id(i) for i in all_temps]) #list of cluster ids\n",
    "cllen = len(str(max(clid))) #length of the largest cluster ID, used for zfill\n",
    "zz = chan[-2:].lower() #the last two letters of channel names (essentially the letters in chan)\n",
    "csv_name = f'{homedir}locations/{volc_list_names[vv]}_ELEP_normalized_picktimes.csv' #name of the csv for picktimes at this volcano\n",
    "h5_name = f'{homedir}h5/{volc_list_names[vv]}_ELEP_smb_pred.h5' #name of h5 file for smb_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774096b3",
   "metadata": {},
   "source": [
    "Find picktimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24567ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv for volcano\n",
    "# with open(csv_name, 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['Network','Station','Cluster_ID','Template_Name','SMB_peak']) #,'SMB_peak_MBF'\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d1b3f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### PULL IN TEMPLATES ###\n",
    "# cl_trange = trange(max(clid), desc=\"Finding picktimes for each cluster\", leave=True)\n",
    "cl_trange=range(101,max(clid))\n",
    "for cl in cl_trange:\n",
    "#     print('------') #print a divider\n",
    "#     print(\"cluster:\",str(cl).zfill(cllen)) #print the cluster ID\n",
    "    \n",
    "    temps_w = [] #list of templates waveforms for this cluster\n",
    "    temps_n = [] #list of template names\n",
    "    temps_p = [] #list of template picks (list of arrays)\n",
    "    preds = [] #pred data for looking\n",
    "    \n",
    "    stopwatch0=time() #note the time\n",
    "    \n",
    "    s_trange = trange(len(v), desc=f\"Finding picktimes for each station at cluster {cl}\", leave=True) #has a progress bar\n",
    "#     s_trange = range(0,len(v)) #no progress bar\n",
    "\n",
    "    rows = [] #list of rows to append to csv\n",
    "    for s in s_trange: #loop through stations that have a template for this cluster\n",
    "        net, sta =  v[s].split('.') #add specific network per station\n",
    "        \n",
    "        for tt,t in enumerate(all_temps): #go through each template\n",
    "            if t.split('_')[0]==net and t.split('_')[1]==sta and t.split('_')[-1]==str(cl): #if the template is for this station and cluster\n",
    "                \n",
    "                ### PREPARE DATA ###\n",
    "#                 Trace(all_waves[tt]).plot(); #show before preparation\n",
    "                wave = all_waves[tt].copy() #create copy\n",
    "                \n",
    "                t_tapered = Trace(wave).taper(tape) #make waveform a trace to taper it\n",
    "                padded_wave = np.hstack((t_tapered.data[:],np.zeros(2480))) #take data from tapered trace and pad end \n",
    "                #with zeros so that len(t_trace)=6000 and will fit in the nueral network\n",
    "                t_trace = Trace(padded_wave,{'sampling_rate':fs}) #make back into a Trace, and set sampling rate\n",
    "#                 print(len(t_trace))\n",
    "                \n",
    "                temps_w.append(t_trace) #append trace\n",
    "                temps_n.append(t) #append name\n",
    "                \n",
    "#                 t_trace.plot(); #plot trace after preparation\n",
    "\n",
    "    \n",
    "                ### FIND PICKS! ###\n",
    "\n",
    "                #picking params\n",
    "                evt_data = Stream(traces=[t_trace])\n",
    "                sta_available = [sta]\n",
    "                list_models = list_models\n",
    "                twin = len(t_trace)-1\n",
    "\n",
    "\n",
    "                dt = 1/fs; fs = fs\n",
    "                nfqs = 5\n",
    "                nt = 6000; nc = 3\n",
    "                iend = len(wave)-1 #end of possible pick times\n",
    "                istart = math.ceil(tape*(len(wave)-1)) #beginning of possible pick times (currently excluding taper)\n",
    "                \n",
    "                fq_list = make_LogFq(fqmin, fqmax, dt, nfqs)\n",
    "                coeff_HP, coeff_LP = rec_filter_coeff(fq_list, dt)\n",
    "                MBF_paras = {'f_min':fqmin, 'f_max':fqmax, 'nfqs':nfqs, 'frequencies':fq_list, 'CN_HP':coeff_HP, 'CN_LP':coeff_LP,                     'dt':dt, 'fs':fs, 'nt':nt, 'nc':nc, 'npoles': 2}\n",
    "\n",
    "                paras_semblance = {'dt':dt, 'semblance_order':2, 'window_flag':True, \n",
    "                                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "                \n",
    "                #find picktimes!\n",
    "                peaks,smb_pred = apply_mbf(evt_data, sta_available,                 list_models, MBF_paras, paras_semblance, istart, iend) #smb_peak,smb_peak_mbf,\n",
    "                \n",
    "                \n",
    "                print(peaks[0]) # THESE PICKS ARE OFFSET BY ISTART\n",
    "                \n",
    "                picks = [i+istart for i in peaks[0]] # ADDED ISTART\n",
    "                temps_p.append(picks)\n",
    "                csv_picks=' '.join([str(i/fs)for i in picks]) #formatted for saving in csv\n",
    "                \n",
    "                preds.append([smb_pred])\n",
    "        \n",
    "                #append picks to csv\n",
    "                row = [net, sta, cl, t, csv_picks] ### APPEND PICKS NOT PEAKS\n",
    "                rows.append(row)\n",
    "                \n",
    "    if len(temps_n)==0:\n",
    "        print(f'no templates for cluster {cl}')\n",
    "        continue\n",
    "\n",
    "    ### FILTERING PEAKS ###\n",
    "\n",
    "    one_peak = [] #list of n and y if the template only has one peak/picktime\n",
    "    for p in temps_p: #for saved picks\n",
    "        if len(p)==1:\n",
    "            one_peak.append('y')\n",
    "        else:\n",
    "            one_peak.append('n')\n",
    "            \n",
    "            \n",
    "#     if one_peak.count('y')/len(one_peak) ==1: #if there is only one peak per template\n",
    "#         #write info to csv as is\n",
    "#         for row in rows:\n",
    "#             with open(csv_name, 'a', newline='') as file:\n",
    "#                 writer = csv.writer(file)\n",
    "#                 writer.writerow(row)\n",
    "#                 file.close()\n",
    "\n",
    "    if one_peak.count('y')/len(one_peak) >=0.5 and one_peak.count('y')/len(one_peak) <1: #if 75% of templates have one peak\n",
    "\n",
    "        one_p_value = [] #list of peak values when there is only one\n",
    "        for peaks in temps_p:\n",
    "            if len(peaks)==1:\n",
    "                one_p_value.append(peaks[0])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        for en,peaks in enumerate(temps_p):\n",
    "            if len(peaks)>1:\n",
    "                sums = []#list of sum of differences between possible peaks and the \"confirmed\" peaks\n",
    "                for peak in peaks:\n",
    "                    sums.append(sum([abs(peak-v) for v in one_p_value]))\n",
    "                closest_peak = peaks[sums.index(min(sums))] #find which peak has the smallest summed distance\n",
    "\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            rows[en][-1] = str(closest_peak/fs) #update the corresponding row's pick for csv\n",
    "\n",
    "        #write updated info to csv\n",
    "#         for row in rows:\n",
    "#             with open(csv_name, 'a', newline='') as file:\n",
    "#                 writer = csv.writer(file)\n",
    "#                 writer.writerow(row)\n",
    "#                 file.close()\n",
    "\n",
    "\n",
    "    if one_peak.count('y')/len(one_peak) <0.5:\n",
    "\n",
    "        for en,peaks in enumerate(temps_p):\n",
    "            if len(peaks)>1:\n",
    "                rows[en][-1] = 'UNCERTAIN'\n",
    "\n",
    "        #write updated info to csv\n",
    "#         for row in rows:\n",
    "#             with open(csv_name, 'a', newline='') as file:\n",
    "#                 writer = csv.writer(file)\n",
    "#                 writer.writerow(row)\n",
    "#                 file.close()\n",
    "\n",
    "\n",
    "    #save smb_pred to h5\n",
    "#     try:\n",
    "#         open(h5_name)\n",
    "#         with h5py.File(h5_name, \"a\") as f: #append info\n",
    "#             f.create_dataset(f\"smb_pred_cl_{cl}\", data=np.array(preds))\n",
    "#             f.create_dataset(f\"template_names_cl_{cl}\", data=temps_n)\n",
    "#     except:\n",
    "#         with h5py.File(h5_name, \"w\") as f: #create file\n",
    "#             f.create_dataset(f\"smb_pred_cl_{cl}\", data=np.array(preds))\n",
    "#             f.create_dataset(f\"template_names_cl_{cl}\", data=temps_n)\n",
    "\n",
    "    for row in rows: #print info for testing\n",
    "        print(row)\n",
    "                    \n",
    "    ### PLOT ###\n",
    "    height = len(temps_w)\n",
    "    fig, ax0 = plt.subplots(figsize=(6,height))\n",
    "\n",
    "    yscale = 2 #how far to space waveforms from eachother\n",
    "    wavecolor = 'black'\n",
    "    for ww, wave in enumerate(temps_w):\n",
    "        ax0.plot(wave.data[:]/np.max(np.abs(wave.data))+yscale+(yscale*ww),color=wavecolor,linewidth=.5)\n",
    "        for i in temps_p[ww]: \n",
    "            plt.vlines(x=i,ymin=(yscale+(yscale*ww))-(0.5*yscale),ymax=(yscale+(yscale*ww))+(0.5*yscale),color='red')\n",
    "    plt.vlines(x=nlta*fs,ymin=0,ymax=(yscale+(yscale*ww))+(0.5*yscale),color='gray',linestyle='dashed')\n",
    "    #RED is possible pick times\n",
    "    #GRAY is the 10 second mark/end noise window for SNR\n",
    "\n",
    "    ax0.tick_params(axis='y', which='both',left=False,labelleft=False)\n",
    "\n",
    "\n",
    "    fig.suptitle(f'Cluster {cl} on {volc_list_names[vv]}')\n",
    "    fig.set_tight_layout(True)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a49c06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(one_peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read h5 file\n",
    "cl = 24\n",
    "\n",
    "with h5py.File(h5_name, \"r\") as f:\n",
    "    h5_names_test = f[f\"template_names_cl_{cl}\"][()]\n",
    "    smb = f[f\"smb_pred_cl_{cl}\"][()]\n",
    "\n",
    "print(h5_names_test)\n",
    "\n",
    "for s in smb:\n",
    "    plt.plot(s[0][0]);\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f685428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find peaks from h5 smbpred\n",
    "#same filters as in mbf_elep_func\n",
    "for s in smb:\n",
    "    peaks = signal.find_peaks(s[0][0][istart:iend],distance=5*fs, height=0.03)\n",
    "\n",
    "    if len(peaks[0]) == 0:\n",
    "        peaks = signal.find_peaks(s[0][0][istart:iend],distance=5*fs)\n",
    "        \n",
    "    print(peaks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bfb2eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae80e1f",
   "metadata": {},
   "source": [
    "Compare to ComCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 11 #51 is a cluster on hood with no matches\n",
    "url = f'https://assets.pnsn.org/red/hood/clusters/{cl}.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53695837",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "\n",
    "matches = str(r.content.splitlines()[20]).split('Potential local match:')[1:]\n",
    "\n",
    "c_times = []\n",
    "c_lats = []\n",
    "c_lons = []\n",
    "if len(matches)>0:\n",
    "    for m in matches:\n",
    "        m_feats = m.split(' ')\n",
    "        m_time = m_feats[1]\n",
    "        c_times.append(m_time)\n",
    "        m_lat = float(m_feats[2][1:-1])\n",
    "        c_lats.append(m_lat)\n",
    "        m_lon = float(m_feats[3][:-1])\n",
    "        c_lons.append(m_lon)\n",
    "        print(f'TIME {m_time} LAT {m_lat} LON {m_lon}')\n",
    "\n",
    "else:\n",
    "    print('no ComCat matches')\n",
    "    \n",
    "avg_c_lat = np.average(c_lats)\n",
    "avg_c_lon = np.average(c_lons)\n",
    "print(f'Average ComCat Location lat {avg_c_lat} lon {avg_c_lon}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ec052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ComCat and my locations to compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
