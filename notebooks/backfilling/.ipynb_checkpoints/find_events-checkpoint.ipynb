{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab72beec",
   "metadata": {},
   "source": [
    "Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55331a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from time import time\n",
    "import csv\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a75a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021]\n"
     ]
    }
   ],
   "source": [
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "vv = config['vv']\n",
    "volc_list_names = config['volc_list_names']\n",
    "volc = volc_list_names[vv]\n",
    "year = config['year']\n",
    "years = config['years']\n",
    "print(years)\n",
    "\n",
    "homedir = config['homedir']\n",
    "readdir = config['readdir']\n",
    "\n",
    "minsta = config['minsta']\n",
    "\n",
    "#time in seconds before and after a detection to check for similar detections\n",
    "wi = config['wi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e345e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv') #read volcano metadata\n",
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str) \n",
    "#combine network and station to keep them associated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739282f",
   "metadata": {},
   "source": [
    "Main loop - finding overlaps - updated june 26, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b4d342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for vv,v in enumerate(volc_list_names): #for each volcano\n",
    "#     volc = v\n",
    "    \n",
    "t0 = time() #record time\n",
    "\n",
    "for year in years: #for each year\n",
    "\n",
    "    #get detections at this volcano in this year\n",
    "    sta_list = volc_md[volc_md['Volcano_Name']==volc]['netsta'].values.tolist() #get a list of stations at this volcano\n",
    "    readstadict = {} #will become a dictionary of dataframes of station detections\n",
    "    for i in sta_list: #for each station at the volcano\n",
    "        try: \n",
    "            readstadict[i] = pd.read_csv(homedir+f'detections/{volc}_{i}_{year}_detections.csv') \n",
    "            #get a dictionary of detections at each station\n",
    "        except: #if the detection file does NOT exist\n",
    "            print(f'no detections for {i} in {year}') #say so\n",
    "            readstadict[i] = pd.DataFrame() #make an empty data frame maintain index\n",
    "\n",
    "    print('---')\n",
    "\n",
    "    rsta_list = [readstadict[i] for i in readstadict] #list version of readstadict\n",
    "\n",
    "    readsta = pd.concat(rsta_list) #make a single dataframe for the whole volcano from the list of station dataframs\n",
    "    print(type(rsta_list[0]))\n",
    "\n",
    "    print('----')\n",
    "\n",
    "    print(readsta)\n",
    "\n",
    "    #for all detections we want to run (concatenated), make cl_list for the volcano\n",
    "    temp_name_list = readsta['Template_Name'].values.tolist() #make a list of template names\n",
    "    cl_list_long = [] # make a list of the numbers in each template name\n",
    "    for i in temp_name_list: \n",
    "        if volc=='Baker' or volc=='Hood' or volc=='Newberry' or volc=='Rainier':\n",
    "            num = i[-3:] #account for zfill, to parameterize, use what make_templates uses to get zfill amount (cid[-1] from volcmd)\n",
    "        if volc=='St_Helens':\n",
    "            num = i[-4:] #account for zfill\n",
    "    #     num = re.findall(r'\\d+', i)\n",
    "        cl_list_long.append(num) #*num for findall but some stations have numbers in their name, so using zfill instead of \n",
    "        #just numbers\n",
    "    cl_list = np.unique(cl_list_long) #get rid of duplicates\n",
    "\n",
    "    csv_name = homedir+f'events/{volc}_{year}_events.csv'\n",
    "    with open(csv_name, 'w', newline='') as file: #make a csv to save to\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"Earliest_Detection_Time\",\"Cluster_ID\",\"Stations_Found\",\"Stations\"]) #,\"Stations_Diff\"\n",
    "        #detection time, cluster id, and number of stations with a detection for this event, what stations that is, ?\n",
    "        file.close()\n",
    "    for cl in cl_list:\n",
    "        t1 = time() #record time\n",
    "        times = [] #get list of datetimes for all templates for this cluster\n",
    "        for i in np.unique(temp_name_list): #for each template\n",
    "            if i.endswith(cl): #if the template ends with the cluster ID\n",
    "                all_times = readsta[readsta['Template_Name']==i]['Detection_Time'].values.tolist() #find the times for that template\n",
    "                for at in all_times:\n",
    "                    times.append(at) #append each template time to the list of datetimes\n",
    "        for ii,i in enumerate(times): #for all detections\n",
    "            t3 = time()\n",
    "        #run through each detection time on all stations\n",
    "            print( )\n",
    "            print('------------')\n",
    "            print('trying detection',ii,'cluster',cl,i)\n",
    "\n",
    "            times_diff = []\n",
    "            match_list = []\n",
    "            for ss,s in enumerate(sta_list): #for each station\n",
    "                statimes = [] #get the times for this station\n",
    "                try:\n",
    "                    readstadict[s]['Template_Name']\n",
    "                except:\n",
    "                    print(f'no data available for {s} in {year}')\n",
    "                    continue\n",
    "                for a in np.unique(readstadict[s]['Template_Name'].values.tolist()):\n",
    "                    # for every template in this station's df\n",
    "                    if a.endswith(cl): #if it matches this cluster\n",
    "                        all_times = readstadict[s][readstadict[s]['Template_Name']==a]['Detection_Time'].values.tolist() \n",
    "                        #get a list of detection times at this station for this cluster\n",
    "                        for at in all_times:\n",
    "                            statimes.append(at) #append to statimes\n",
    "                match=0 #set variable to arbitrary number\n",
    "                for tt,t in enumerate(statimes): #for every datetime in the station's detections for this cluster\n",
    "                    ts = UTCDateTime(t)-wi #find the time wi s before\n",
    "                    te = UTCDateTime(t)+wi #find the time wi s after\n",
    "                    if UTCDateTime(i)>ts and UTCDateTime(i)<te:\n",
    "                        diff = UTCDateTime(i)-UTCDateTime(t)\n",
    "                        match=2 #if there is an overlap, reset the variable and break out of the loop\n",
    "                        print('Overlap with station '+s+' detections')\n",
    "                        break\n",
    "                match_list.append(match)\n",
    "            print('match_list:',match_list)\n",
    "\n",
    "            t2 = time()\n",
    "            print(t2-t3,'seconds to test overlap for detection',ii)\n",
    "\n",
    "            save_list = [] #for saving a list of stations\n",
    "            for mm,m in enumerate(match_list):\n",
    "                if m==2:\n",
    "                    save_list.append(sta_list[mm])\n",
    "\n",
    "            if match_list.count(2) >= minsta: #if at least 4 matches equal 2:\n",
    "        #         if match==2 and match0==2 and match1==2 and match2==2:\n",
    "                print('saving...')\n",
    "                check = pd.read_csv(csv_name) #read the csv we made\n",
    "                print('check:', check)\n",
    "                checktimes = check[check['Cluster_ID']==int(cl)]['Earliest_Detection_Time'].values.tolist() \n",
    "                #find the datetimes that have already been saved\n",
    "                checking = 0\n",
    "                for tt,t in enumerate(checktimes):\n",
    "                    ts = UTCDateTime(t)-wi\n",
    "                    te = UTCDateTime(t)+wi\n",
    "                    if UTCDateTime(i)>ts and UTCDateTime(i)<te: \n",
    "                        print('already recorded event')\n",
    "                        checking=1 #checking still =1 because we don't want the normal save process\n",
    "                        #if i is earlier (less than) than the recorded event, overwrite the event (we want the earliest detected time)\n",
    "                        if i<t: #if this time is earlier than the saved time\n",
    "                            print('replacing...')\n",
    "                            print('old time:',t,'new time:',i) \n",
    "                            check.replace(t,i,inplace=True) #replace t with i in the df\n",
    "                            check.to_csv(csv_name,index=False)\n",
    "                            #read panda dataframe - change dataframe, overwrite the csv with this dataframe\n",
    "                            print('changed csv')\n",
    "                        break\n",
    "                if checking==1: continue # because it has already been saved\n",
    "                row = [i,int(cl),match_list.count(2),' '.join(save_list),] #put together the row, #, ' '.join(times_diff)\n",
    "                print(row)\n",
    "\n",
    "                with open(csv_name, 'a', newline='') as file:\n",
    "                    writer = csv.writer(file)\n",
    "                    writer.writerow(row) #save the row to csv\n",
    "                    file.close()\n",
    "\n",
    "            #####################################################################\n",
    "\n",
    "        t4 = time()\n",
    "        print(t4-t1,'seconds for cluster',cl)\n",
    "    t5 = time()\n",
    "    print(t5-t0,'seconds to find all new events on',volc,'in',year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea975c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
