{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda7aef9",
   "metadata": {},
   "source": [
    "##########################################################################\n",
    "\n",
    "GOAL:\n",
    "\n",
    "    make it so that this pulls in the events.csv as a df, finds the lines containing overlaps, deletes them, then puts the df back into csv just now without the overlap\n",
    "    \n",
    "    add template locations as a column in the df after checking for redpy overlap\n",
    "    \n",
    "##########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0249ae1b",
   "metadata": {},
   "source": [
    "Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da00eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from glob import glob\n",
    "from obspy.signal.trigger import classic_sta_lta, plot_trigger, trigger_onset\n",
    "import csv\n",
    "import re\n",
    "\n",
    "from obspy.core.utcdatetime import UTCDateTime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a2447",
   "metadata": {},
   "source": [
    "Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26aa0d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "rpwi = config['rpwi'] #time in seconds before and after REDpy catalog datetimes to exclude detections from, window length=2*rpwi\n",
    "homedir = config ['homedir']\n",
    "readdir = config['readdir']\n",
    "vv = config['vv']\n",
    "years = config['years']\n",
    "volc_list_names = config['volc_list_names']\n",
    "# datadir = '/data/wsd01/HOOD_data/UW/'+str(year)+'/' #directory to get data from\n",
    "\n",
    "print(volc_list_names[vv])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b0f83",
   "metadata": {},
   "source": [
    "Read the REDpy Catalogs and Volcano Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae1cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baker = pd.read_csv(readdir+'Baker_catalog.csv')\n",
    "Hood = pd.read_csv(readdir+'Hood_catalog.csv')\n",
    "\n",
    "\n",
    "St_Helens = pd.read_csv(readdir+'MountStHelens_catalog.csv')\n",
    "\n",
    "# Combining borehole and local catalogs with St_Helens\n",
    "\n",
    "Helens_Borehole = pd.read_csv(readdir+'MSHborehole_catalog.csv')\n",
    "Helens_Borehole['Clustered'] += 2000 \n",
    "# Cluster 0 in Helens_Borehole is now Cluster 2000 in St_Helens\n",
    "Helens_Local = pd.read_csv(readdir+'MSHlocal_catalog.csv')\n",
    "Helens_Local['Clustered'] += 3000\n",
    "# Cluster 0 in Helens_Local is now Cluster 3000 in St_Helens\n",
    "\n",
    "# Use St_Helens to access all three St Helens catalogs\n",
    "St_Helens = pd.concat([St_Helens,Helens_Borehole,Helens_Local])\n",
    "clid = np.unique(St_Helens['Clustered'].values.tolist()) #find the largest cluster ID for a volcano to set range\n",
    "print(clid[-1])\n",
    "\n",
    "Newberry = pd.read_csv(readdir+'Newberry_catalog.csv')\n",
    "Rainier = pd.read_csv(readdir+'Rainier_catalog.csv')\n",
    "\n",
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv')\n",
    "# read metadata file to create dataframe of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58bdf1",
   "metadata": {},
   "source": [
    "Use Volcano Metadata to Create Lists of Stations for Each Volcano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33449666",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str)\n",
    "\n",
    "Baker_sta = volc_md[volc_md['Volcano_Name'] == 'Baker']['netsta'].values.tolist()\n",
    "Hood_sta = volc_md[volc_md['Volcano_Name'] == 'Hood']['netsta'].values.tolist() \n",
    "St_Helens_sta = volc_md[volc_md['Volcano_Name'] == 'St_Helens']['netsta'].values.tolist()\n",
    "Newberry_sta = volc_md[volc_md['Volcano_Name'] == 'Newberry']['netsta'].values.tolist() \n",
    "Rainier_sta = volc_md[volc_md['Volcano_Name'] == 'Rainier']['netsta'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81c80f",
   "metadata": {},
   "source": [
    "Create Lists of Volcano Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c309e055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#enumerate [0,1,2,3,4]\n",
    "volc_list = [Baker,Hood,Newberry,Rainier,St_Helens] # list of dataframes for each volcano\n",
    "volc_list_names = ['Baker','Hood','Newberry','Rainier','St_Helens'] # list of names of each volcano\n",
    "volc_sta = [Baker_sta,Hood_sta,Newberry_sta,Rainier_sta,St_Helens_sta] # lists of stations connected to respective volcanoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf43eb",
   "metadata": {},
   "source": [
    "Updated Sorting - December 4, 2022\n",
    "\n",
    "- removes redpy overlap\n",
    "- adds location (same as the location for each cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = volc_sta[vv]\n",
    "# print(v)\n",
    "# print(volc_list_names[vv])\n",
    "catalog = volc_list[vv]\n",
    "pd_sta = {} #will become a dictionary of event dataframes\n",
    "for year in years: #for each year\n",
    "#     print(year)\n",
    "    try: #try to read the events csv\n",
    "        read = pd.read_csv(homedir+f'events/{volc_list_names[vv]}_{year}_events.csv')\n",
    "    except:\n",
    "        print(f'No detections on {volc_list_names[vv]} for {year}') #if no events csv, say so\n",
    "        continue\n",
    "    pd_sta[f'{volc_list_names[vv]}_{year}'] = pd.read_csv(homedir+f'events/{volc_list_names[vv]}_{year}_events.csv')\n",
    "    #record the events csv as a dataframe in pd_dict\n",
    "\n",
    "read = pd.concat(pd_sta, axis=0,ignore_index=True) #combine the different year dataframes into one dataframe for the whole volcano\n",
    "\n",
    "display(read)\n",
    "\n",
    "\n",
    "#go by cluster to speed things up:\n",
    "cl_list = np.unique(read['Cluster_ID'].values.tolist())\n",
    "\n",
    "for cl in cl_list:\n",
    "    times = read[read['Cluster_ID']==cl]['Earliest_Detection_Time'].values.tolist() #get every time for this cluster from new detections\n",
    "#     print(cl,'-',times)\n",
    "    rpdatetimes = catalog[catalog['Clustered'] == cl]['datetime'].values.tolist() #get every time for this cluster from REDPy\n",
    "    for ii,i in enumerate(times): #for each detection time\n",
    "        #make a list of datetimes for the current cluster\n",
    "        skip=1 #set variable to arbitrary number\n",
    "        for rr,r in enumerate(rpdatetimes): #run through each redpy time for this cl\n",
    "            rs = UTCDateTime(r)-rpwi #redpy time\n",
    "            rend = UTCDateTime(r)+rpwi #changed from re to rend because of import re for cl_list\n",
    "            if UTCDateTime(i)>rs and UTCDateTime(i)<rend:#if there is an overlap\n",
    "                skip=2 #reset the variable \n",
    "                print(f'Overlap with REDpy detections, {i}') #say so\n",
    "                #drop the row from the csv\n",
    "                useless = read[read['Earliest_Detection_Time']==i].index.tolist()\n",
    "                read = read.drop(useless)\n",
    "                break #break out of the loop\n",
    "        if skip != 2: #if NO overlap has occured\n",
    "            print(f'no overlap for {i} cluster {cl}')\n",
    "read = read.reset_index(drop=True) #reset index\n",
    "\n",
    "display(read)\n",
    "# add cluster locations\n",
    "loc_csv = pd.read_csv(homedir+f'locations/{volc_list_names[vv]}_Template_Locations.csv') #read locations csv\n",
    "\n",
    "read['Latitude'] = '' #make new columns in the dataframe\n",
    "read['Longitude'] = ''\n",
    "# display(read)\n",
    "cl_list_updated = np.unique(read['Cluster_ID'].values.tolist()) #get a new cl_list \n",
    "#(without any clusters that might have been dropped entirely from overlap)\n",
    "for cl in cl_list_updated:\n",
    "    print('---')\n",
    "    cl_indx = read[read['Cluster_ID']==cl].index.tolist() #get index numbers for the rows with this cl\n",
    "    print('cl_indx',cl_indx)\n",
    "#find the indexes for each cluster\n",
    "    lat = loc_csv[loc_csv['Cluster_ID']==cl]['Latitude'].values.tolist() #find lat for this cl\n",
    "    lon = loc_csv[loc_csv['Cluster_ID']==cl]['Longitude'].values.tolist() #find lon for this cl\n",
    "    print(lat[0],lon[0])\n",
    "#input the correct lat and lon for those indexes\n",
    "    for ci in cl_indx: #for each index of this cl\n",
    "        print('ci',ci)\n",
    "        read.at[ci, 'Latitude'] = lat[0] #make the column 'Latitude' at this index (ci) equal latitude for this cl\n",
    "        read.at[ci, 'Longitude'] = lon[0] #make the column 'Longitude' at this index (ci) equal longitude for this cl\n",
    "            \n",
    "read.to_csv(homedir+f'final_catalogs/{volc_list_names[vv]}_updated_catalog.csv',index=False) #save as csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4821e831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#display whole dataframe for better viewing than from the csv itself\n",
    "\n",
    "df = pd.read_csv(homedir+f'final_catalogs/{volc_list_names[vv]}_updated_catalog.csv')\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae34b533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bc1da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo 3.8 (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
