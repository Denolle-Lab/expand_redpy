{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02acad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import math\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy import Trace\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy.signal.trigger import classic_sta_lta, plot_trigger, trigger_onset\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec #for organizing subplots\n",
    "from tqdm import trange\n",
    "                        # t = trange(len(waveforms), desc=\"Trimming Waveforms \", leave=True)\n",
    "                        #    for i in t:\n",
    "import h5py\n",
    "# %matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"/data/wsd01/pnwstore/\")\n",
    "import eqcorrscan\n",
    "from eqcorrscan.core.match_filter import match_filter\n",
    "from eqcorrscan.core.match_filter.tribe import Tribe\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "\n",
    "client = WaveformClient()\n",
    "client2 = Client('IRIS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "fs = config['fs']\n",
    "tb = config['tb']\n",
    "ta = config['ta']\n",
    "fqmin = config['fqmin']\n",
    "fqmax = config['fqmax']\n",
    "nbucket = config['nbucket']\n",
    "chan = config['chan']\n",
    "homedir = config['homedir']\n",
    "readdir = config['readdir']\n",
    "f_o = config['f_o']\n",
    "p_len = config['p_len']\n",
    "\n",
    "vv = config['vv']\n",
    "\n",
    "snr_t = config['snr_t']\n",
    "thr_on = config['thr_on']\n",
    "thr_off = config['thr_off']\n",
    "nsta = config['nsta']\n",
    "nlta = config['nlta']\n",
    "pr = config['pr']\n",
    "\n",
    "print(vv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b02f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baker = pd.read_csv(readdir+'Baker_catalog.csv')\n",
    "Hood = pd.read_csv(readdir+'Hood_catalog.csv')\n",
    "\n",
    "\n",
    "St_Helens = pd.read_csv(readdir+'MountStHelens_catalog.csv')\n",
    "\n",
    "# Combining borehole and local catalogs with St_Helens\n",
    "\n",
    "Helens_Borehole = pd.read_csv(readdir+'MSHborehole_catalog.csv')\n",
    "Helens_Borehole['Clustered'] += 2000 \n",
    "# Cluster 0 in Helens_Borehole is now Cluster 2000 in St_Helens\n",
    "Helens_Local = pd.read_csv(readdir+'MSHlocal_catalog.csv')\n",
    "Helens_Local['Clustered'] += 3000\n",
    "# Cluster 0 in Helens_Local is now Cluster 3000 in St_Helens\n",
    "\n",
    "# Use St_Helens to access all three St Helens catalogs\n",
    "St_Helens = pd.concat([St_Helens,Helens_Borehole,Helens_Local])\n",
    "\n",
    "Newberry = pd.read_csv(readdir+'Newberry_catalog.csv')\n",
    "Rainier = pd.read_csv(readdir+'Rainier_catalog.csv')\n",
    "\n",
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv')\n",
    "# read metadata file to create dataframe of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e820ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str)\n",
    "Baker_sta = volc_md[volc_md['Volcano_Name'] == 'Baker']['netsta'].values.tolist()\n",
    "Hood_sta = volc_md[volc_md['Volcano_Name'] == 'Hood']['netsta'].values.tolist() \n",
    "St_Helens_sta = volc_md[volc_md['Volcano_Name'] == 'St_Helens']['netsta'].values.tolist()\n",
    "Newberry_sta = volc_md[volc_md['Volcano_Name'] == 'Newberry']['netsta'].values.tolist() \n",
    "Rainier_sta = volc_md[volc_md['Volcano_Name'] == 'Rainier']['netsta'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eead927",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_list = [Baker,Hood,Newberry,Rainier,St_Helens] # list of dataframes for each volcano\n",
    "volc_list_names = ['Baker','Hood','Newberry','Rainier','St_Helens'] # list of names of each volcano\n",
    "volc_sta = [Baker_sta,Hood_sta,Newberry_sta,Rainier_sta,St_Helens_sta] # lists of stations connected to respective volcanoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31e3720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for vv,v in enumerate(volc_sta): #vv is the number in the list, v is the station list for current volcano\n",
    "v = volc_sta[vv]\n",
    "clid = volc_list[vv]['Clustered'].values.tolist() #find the largest cluster ID for a volcano to set range\n",
    "for s in range(0,len(v)): #loop through stations\n",
    "    norm_stack_list = [] #list of normalized stacks for every cluster\n",
    "    norm_stack_names = [] #list of names for the normalized stacks\n",
    "    \n",
    "    net, sta =  v[s].split('.') #add specific network per station\n",
    "    t0 = time() #record time\n",
    "    cid = [] #cid will become a list of cluster IDs that have templates\n",
    "    st3=obspy.Stream() #st3 will become a stream of traces, each trace being the template/stack for a cluster\n",
    "\n",
    "    for cl in range(0,(clid[-1]+1)): #cl is cluster ID\n",
    "    # clid[-1] is the highest cluster ID, and add 1 so that the last cluster id is not skipped\n",
    "        t2=time()\n",
    "        sst=obspy.Stream() #sst will have every trace for each cluster\n",
    "        print('------------') #divider for clarity\n",
    "        print('Cluster ID: '+str(cl)+' Volcano: '+volc_list_names[vv]+' Station: '+sta+' Network: '+net) #keeping track of what is currently running\n",
    "        date_list = volc_list[vv][volc_list[vv]['Clustered'] == cl]['datetime'].values.tolist()\n",
    "        for ii,i in enumerate(date_list): #i is each datetime from cl at this volcano\n",
    "            stt=UTCDateTime(i)-tb-nlta #starttime\n",
    "            et=UTCDateTime(i)+ta #endtime\n",
    "            utct=UTCDateTime(i) #datetime from REDpy catalog\n",
    "            \n",
    "            sta_st = UTCDateTime(volc_md[volc_md['netsta']==v[s]]['Starttime'].values[0])\n",
    "            sta_nd = UTCDateTime(volc_md[volc_md['netsta']==v[s]]['Endtime'].values[0])\n",
    "            \n",
    "            if UTCDateTime(i)<sta_st or UTCDateTime(i)>sta_nd:\n",
    "#                 print(f'{i} before sta start {sta_st} or after sta end {sta_nd}')\n",
    "                continue\n",
    "            \n",
    "#             print(i)\n",
    "            \n",
    "            ### PULL WAVEFORM ###\n",
    "            \n",
    "            st = client.get_waveforms(network=net,station=sta,location='*',\n",
    "                                      channel=chan, year=utct.year, doy=utct.julday)\n",
    "            st = st.detrend(type = 'demean')\n",
    "            st.filter(type='bandpass',freqmin=fqmin,freqmax=fqmax)\n",
    "            st.resample(fs) #get same sampling rate for all events\n",
    "            st.trim(starttime=stt,endtime=et)\n",
    "            st.merge(fill_value = 0)\n",
    "\n",
    "            ### CHECK DATA ###\n",
    "            \n",
    "            if len(st)==0:\n",
    "                print('no data for this time')\n",
    "                continue\n",
    "#             print(len(st[0].data))\n",
    "            if len(st[0].data) == round((ta+tb+nlta)*fs)+1: #if there is enough data to contribute to making a stack\n",
    "                sst.append(st[0])\n",
    "    #         break\n",
    "    \n",
    "        ### SHIFT TRACES IN CL ###\n",
    "    \n",
    "        print(sst) #see how many traces were found\n",
    "        st2=sst.copy() #copy of sst after appending is finished/all waveforms for a cluster are gathered, reference for shifting\n",
    "        st4=st2.copy() #st4 will become the aligned version of st2\n",
    "        for i in range(0, len(st2)):\n",
    "            print('Working on shifting') # Will not print if len(st2)==0\n",
    "            # Also serves as a divider between shift and cc values for different waveforms\n",
    "            xcor = obspy.signal.cross_correlation.correlate(st2[i].data[:],sst[0].data[:],200)\n",
    "            index = np.argmax(xcor)\n",
    "            cc = round(xcor[index],9)\n",
    "            shift = 200-index\n",
    "            print('before alignment:',shift,cc) #the shift and cross correlation values before shifting/alignment. Perfect shift is 0, perfect cross correlation is 1\n",
    "            if shift<0: st4[i].data[:shift]=st2[i].data[-shift:]\n",
    "            if shift>0: st4[i].data[shift:]=st2[i].data[:-shift]\n",
    "            xcor = obspy.signal.cross_correlation.correlate(st4[i].data[:],sst[0].data[:],200)\n",
    "            index = np.argmax(xcor)\n",
    "            cc = round(xcor[index],9)\n",
    "            shift = 200-index\n",
    "            print('after alignment',shift,cc) #shift and cross correlation values after alignment. shift should be 0\n",
    "        print('shift complete')\n",
    "        print( )\n",
    "        \n",
    "        \n",
    "        sst2=st4.copy() #sst2 is a copy of st4 once alignment is finished\n",
    "        sst3=st4.copy() #another copy for normalization\n",
    "        sst2.stack() #stack aligned waveforms\n",
    "        \n",
    "        sst3 = sst3.normalize()\n",
    "        sst3.stack()\n",
    "        \n",
    "        if len(sst)<2: #some clusters will have 1 or 0 traces due to unavailable data\n",
    "            print('sst not long enough')\n",
    "            t4=time()\n",
    "            print(f'{str(t4-t2)} seconds to attempt to find {len(date_list)} waveforms')\n",
    "            continue\n",
    "\n",
    "        print('length of sst: '+str(len(sst))) #should be 2 or higher\n",
    "        st3.append(sst2[0])\n",
    "\n",
    "        cid.append('rp'+volc_list_names[vv][:2].lower()+str(cl).zfill(len(str(clid[-1])))) #append the clusterID to cid. rp for REDpy, volc_list_names[vv][:2].lower() for the volcano name\n",
    "        # writing seisbench h5py\n",
    "\n",
    "        lat = volc_md[volc_md['netsta']==v[s]]['Latitude'].values[0]\n",
    "        lon = volc_md[volc_md['netsta']==v[s]]['Longitude'].values[0]\n",
    "        \n",
    "        ### PLOTTING ###\n",
    "        \n",
    "        if len(sst) <=4:\n",
    "            height = len(sst)\n",
    "        if len(sst)>4 and len(sst)<100:\n",
    "            height = math.ceil(len(sst)/2)\n",
    "        if len(sst) >=100:\n",
    "            height = math.ceil(len(sst)/5)\n",
    "        \n",
    "        fig, ax0 = plt.subplots(figsize=(6,height))\n",
    "        gs = GridSpec(height, 2, figure=fig) #make GridSpec for formatting subplots, based on number of waveforms\n",
    "        ax1 = fig.add_subplot(gs[0:height,0:1])\n",
    "        ax2 = fig.add_subplot(gs[0:1,1:2])\n",
    "        ax3 = fig.add_subplot(gs[1:2,1:2])\n",
    "        \n",
    "        yscale = 2 #how far to space waveforms from eachother\n",
    "        wavecolor = 'black'\n",
    "        for ww, wave in enumerate(sst):\n",
    "            ax1.plot(wave.data[:]/np.max(np.abs(wave.data))+yscale+(yscale*ww),color=wavecolor,linewidth=.5)\n",
    "        ax1.tick_params(axis='both', which='both', bottom=False,left=False,labelbottom=False,labelleft=False)\n",
    "\n",
    "            \n",
    "        #plot un-normalized stack\n",
    "        ax2.plot(sst2[0].data[:],color='red',linewidth=.5)\n",
    "        ax2.set_title('Stack (no normalization)')\n",
    "        ax2.tick_params(axis='x', which='both', bottom=False,labelbottom=False)\n",
    "\n",
    "        \n",
    "        #plot normalized stack\n",
    "        ax3.plot(sst3[0].data[:],color='orange',linewidth=.5)\n",
    "        ax3.set_title('Normalized Stack')\n",
    "        ax3.tick_params(axis='x', which='both', bottom=False,labelbottom=False)\n",
    "        \n",
    "        fig.suptitle(f'Cluster {cl} at {v[s]} on {volc_list_names[vv]}')\n",
    "        fig.set_tight_layout(True)\n",
    "        fig.delaxes(ax0) #remove unused ax\n",
    "        fig.savefig(f'{homedir}stack_plots/Volcano_{volc_list_names[vv]}_netsta_{v[s]}_cl_{str(cl).zfill(len(str(clid[-1])))}.svg')\n",
    "        \n",
    "        ### SNR on the normalized stack (sst3) ###\n",
    "\n",
    "        cft = classic_sta_lta(sst3[0].data, int(nsta * fs), int(nlta * fs)) #basis for stalta\n",
    "#         print('-------------')\n",
    "#         print(sst3[0].stats.starttime+nlta)\n",
    "        plot_trigger(sst3[0], cft, thr_on, thr_off) #print stalta plots\n",
    "#         print('C ID:', cid[ll])\n",
    "        on_off = np.array(trigger_onset(cft, thr_on, thr_off)) #x value of vlines in the plot (plot_trigger is NOT needed for this to work)\n",
    "        if not np.any(on_off):\n",
    "            print('NO SIGNAL FOUND')\n",
    "            continue\n",
    "        index = []\n",
    "        for ii,i in enumerate(on_off): #for each signal detection in on_off\n",
    "        #     print(i)\n",
    "            if i[1]-i[0] == 0: #if the start and end times are same, mark\n",
    "                index.append(ii)\n",
    "        on_off = np.delete(on_off,index, axis=0) #remove marked signal windows by index\n",
    "        print('on_off')\n",
    "        print(on_off)\n",
    "        amps = []\n",
    "        for ii,i in enumerate(on_off): #go through each possible signal window\n",
    "            start = i[0]\n",
    "            end = i[1]\n",
    "            amps.append(Trace(data=sst3[0].data[start:end]).max()) #find max amplitude in that signal window\n",
    "        # print(amps)\n",
    "        amp = max(amps, key=abs) #find max amplitude of stream within sta/lta windows\n",
    "        # print(amp)\n",
    "        for ii,i in enumerate(on_off): #go through each possible signal window\n",
    "            start = i[0]\n",
    "            end = i[1]\n",
    "            ooamp = Trace(data=sst3[0].data[start:end]).max() #find max amplitude in that signal window\n",
    "            print('amp',amp,'ooamp',ooamp,'start',start,'end',end)\n",
    "            if amp == ooamp: #if max amplitude of this window is the same as the overall max amplitude\n",
    "#                 print('wow')\n",
    "                trig_on = round(float(start / fs),4) #took out -nlta\n",
    "                trig_off = round(float(end / fs),4) #get start and end times\n",
    "                break #move on\n",
    "        # show trigger on and off times, rounded to 4 decimal places\n",
    "        print('Trigger on',trig_on-nlta,'seconds after tb time') #trig_on-nlta is picktime...?\n",
    "        print('Trigger off',trig_off-nlta,'seconds after tb time')\n",
    "\n",
    "        signal_window = sst3[0].copy()\n",
    "        noise_window = sst3[0].copy()\n",
    "        \n",
    "\n",
    "        signal_window.trim(starttime=sst3[0].stats.starttime+trig_on,endtime=sst3[0].stats.starttime+trig_off)\n",
    "        #i+trig_on-0.5 to include lead up to the signal?\n",
    "        noise_window.trim(starttime=sst3[0].stats.starttime,endtime=sst3[0].stats.starttime+nlta) \n",
    "        #the noise window is the part that can't be considered in the sta/lta anyway\n",
    "\n",
    "        snr = 20 * np.log(np.percentile(np.abs(signal_window.data),pr) \n",
    "                          / np.percentile(np.abs(noise_window.data),pr))/np.log(10)\n",
    "        print('snr:',snr)\n",
    "        if snr<snr_t:\n",
    "            print('SNR too low')\n",
    "            continue\n",
    "            \n",
    "        ### APPENDING ###\n",
    "        \n",
    "        #append normalized waveform data to a list\n",
    "        norm_stack_list.append(sst3[0].data[:])\n",
    "        norm_stack_names.append(f'{net}_{sta}_rp{volc_list_names[vv][:2].lower()}')\n",
    "                \n",
    "#         break\n",
    "    ### Save Station info to h5 ###\n",
    "    with h5py.File(f\"{homedir}h5/normalized_{volc_list_names[vv].lower()}_templates_{net}_{sta}.h5\", \"w\") as f:\n",
    "        f.create_dataset(\"waveforms\", data=np.array(norm_stack_list))\n",
    "        f.create_dataset(\"template_name\", data=norm_stack_names)\n",
    "        \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afcf865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
