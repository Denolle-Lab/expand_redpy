{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ba47834",
   "metadata": {},
   "source": [
    "# Make Templates from REDpy Clusters using EQCorrScan\n",
    "Created: Jul 14, 2022\n",
    "\n",
    "Updated: Jul 14, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c47527",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07bcc00",
   "metadata": {},
   "source": [
    "Import everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from obspy.clients.fdsn import Client\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"/data/wsd01/pnwstore/\")\n",
    "import eqcorrscan\n",
    "from eqcorrscan.core.match_filter import match_filter\n",
    "from eqcorrscan.core.match_filter.tribe import Tribe\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "\n",
    "from pnwstore.mseed import WaveformClient\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "\n",
    "client = WaveformClient()\n",
    "client2 = Client('IRIS')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eda6a44",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd81674",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 40 #sampling rate in hertz\n",
    "tb = 3 #time in seconds before origin time\n",
    "ta = 10 #time in seconds after origin time\n",
    "fqmin = 1 #minimum frequency for bandpass filter\n",
    "fqmax = 10 #maximum frequency for bandpass filter\n",
    "nbucket = 1 # breaking up matrix, if having keyerror for nbucket, just set it to 1\n",
    "chan = '*HZ' #channel to get waveforms from\n",
    "homedir = '/home/smocz/redpy_expand_new_files/' #path to home directory or directory to store outputs (NOT in repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e985db6",
   "metadata": {},
   "source": [
    "Read REDpy Catalogs and Volcano Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc13b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baker = pd.read_csv('Baker_catalog.csv')\n",
    "Hood = pd.read_csv('Hood_catalog.csv')\n",
    "\n",
    "\n",
    "St_Helens = pd.read_csv('MountStHelens_catalog.csv')\n",
    "\n",
    "# Combining borehole and local catalogs with St_Helens\n",
    "\n",
    "Helens_Borehole = pd.read_csv('MSHborehole_catalog.csv')\n",
    "Helens_Borehole['Clustered'] += 2000 \n",
    "# Cluster 0 in Helens_Borehole is now Cluster 2000 in St_Helens\n",
    "Helens_Local = pd.read_csv('MSHlocal_catalog.csv')\n",
    "Helens_Local['Clustered'] += 3000\n",
    "# Cluster 0 in Helens_Local is now Cluster 3000 in St_Helens\n",
    "\n",
    "# Use St_Helens to access all three St Helens catalogs\n",
    "St_Helens = pd.concat([St_Helens,Helens_Borehole,Helens_Local])\n",
    "\n",
    "Newberry = pd.read_csv('Newberry_catalog.csv')\n",
    "Rainier = pd.read_csv('Rainier_catalog.csv')\n",
    "\n",
    "volc_md = pd.read_csv('Volcano_Metadata.csv')\n",
    "# read metadata file to create dataframe of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6a446",
   "metadata": {},
   "source": [
    "Create Lists of Stations for Each Volcano Using volc_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ba8851",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baker_sta = volc_md[volc_md['Volcano_Name'] == 'Mt_Baker']['Station'].values.tolist()\n",
    "Hood_sta = volc_md[volc_md['Volcano_Name'] == 'Mt_Hood']['Station'].values.tolist() \n",
    "St_Helens_sta = volc_md[volc_md['Volcano_Name'] == 'Mt_St_Helens']['Station'].values.tolist()\n",
    "Newberry_sta = volc_md[volc_md['Volcano_Name'] == 'Newberry']['Station'].values.tolist() \n",
    "Rainier_sta = volc_md[volc_md['Volcano_Name'] == 'Mt_Rainier']['Station'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535c6e2f",
   "metadata": {},
   "source": [
    "Create Lists of Volcano Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f5e852",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_list = [Baker,Hood,Newberry,Rainier,St_Helens] # list of dataframes for each volcano\n",
    "volc_list_names = ['Baker','Hood','Newberry','Rainier','St_Helens'] # list of names of each volcano\n",
    "volc_sta = [Baker_sta,Hood_sta,Newberry_sta,Rainier_sta,St_Helens_sta] # lists of stations connected to respective volcanoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24863e2",
   "metadata": {},
   "source": [
    "### Making the Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e36ab",
   "metadata": {},
   "source": [
    "Define update_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b47490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_data(data, streamdata, ibucket):\n",
    "    streamdata = np.expand_dims(streamdata, axis = 0)\n",
    "\n",
    "    if ibucket in data:\n",
    "        data[ibucket] = np.concatenate((data[ibucket], streamdata), axis = 0)\n",
    "    else:\n",
    "        data[ibucket] = streamdata\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e220cf8",
   "metadata": {},
   "source": [
    "Make and Save Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION: how often should data and meta reset? Every volcano? \n",
    "# currently the seisbench is named by volcano and station - so I think we could put it between t0 and cid\n",
    "# changed most (all?) .stats.station to v[s]\n",
    "\n",
    "# for vv,v in enumerate(volc_sta): #vv is the number in the list, v is the station list for current volcano\n",
    "v = St_Helens_sta\n",
    "vv = 4\n",
    "clid = volc_list[vv]['Clustered'].values.tolist() #find the largest cluster ID for a volcano to set range\n",
    "for s in range(35,len(v)): #loop through stations\n",
    "    t0 = time() #record time\n",
    "    #Create Dictionary data and DataFrame meta\n",
    "    data = {}\n",
    "    meta = pd.DataFrame(columns = [\n",
    "        \"source_id\", \"source_origin_time\", \"source_latitude_deg\", \"source_longitude_deg\", \"source_type\",\n",
    "        \"source_depth_km\", \"split\", \"source_magnitude\", \"station_network_code\", \"trace_channel\", \n",
    "        \"station_code\", \"station_location_code\", \"station_latitude_deg\",  \"station_longitude_deg\",\n",
    "        \"station_elevation_m\", \"trace_name\", \"trace_sampling_rate_hz\", \"trace_start_time\",\n",
    "        \"trace_S_arrival_sample\", \"trace_P_arrival_sample\", \"CODE\"])\n",
    "\n",
    "    cid = [] #cid will become a list of cluster IDs that have templates\n",
    "    st3=obspy.Stream() #st3 will become a stream of traces, each trace being the template/stack for a cluster\n",
    "    for cl in range(0,(clid[-1]+1)): #cl is cluster ID\n",
    "    # clid[-1] is the highest cluster ID, and add 1 so that the last cluster id is not skipped\n",
    "        t2=time()\n",
    "        sst=obspy.Stream() #sst will have every trace for each cluster\n",
    "        print('------------') #divider for clarity\n",
    "        print('Cluster ID: '+str(cl)+' Volcano: '+volc_list_names[vv]+' Station: '+v[s]) #keeping track of what is currently running\n",
    "        for ii,i in enumerate(volc_list[vv][volc_list[vv]['Clustered'] == cl]['datetime']): #i is each datetime from cl at this volcano\n",
    "            stt=UTCDateTime(i)-tb #starttime\n",
    "            et=UTCDateTime(i)+ta #endtime\n",
    "            utct=UTCDateTime(i) #datetime from REDpy catalog\n",
    "            try:\n",
    "                st = client.get_waveforms(network='*',station=v[s],location='*',channel=chan, year=utct.year, doy=utct.julday)\n",
    "                st = st.detrend(type = 'demean')\n",
    "                st.filter(type='bandpass',freqmin=fqmin,freqmax=fqmax)\n",
    "                st.resample(fs) #get same sampling rate for all events\n",
    "                st.trim(starttime=stt,endtime=et)\n",
    "                st.merge(fill_value = 0)\n",
    "                if len(st[0].data) == round((ta+tb)*fs)+1: #if there is enough data to contribute to making a stack\n",
    "                    sst.append(st[0])\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "    #         break\n",
    "        print(sst) #see how many traces were found\n",
    "        st2=sst.copy() #copy of sst after appending is finished/all waveforms for a cluster are gathered, reference for shifting\n",
    "        st4=st2.copy() #st4 will become the aligned version of st2\n",
    "        for i in range(0, len(st2)):\n",
    "            print('Working on shifting') # Will not print if len(st2)==0\n",
    "            # Also serves as a divider between shift and cc values for different waveforms\n",
    "            xcor = obspy.signal.cross_correlation.correlate(st2[i].data[:],sst[0].data[:],200)\n",
    "            index = np.argmax(xcor)\n",
    "            cc = round(xcor[index],9)\n",
    "            shift = 200-index\n",
    "            print(shift,cc) #the shift and cross correlation values before shifting/alignment. Perfect shift is 0, perfect cross correlation is 1\n",
    "            if shift<0: st4[i].data[:shift]=st2[i].data[-shift:]\n",
    "            if shift>0: st4[i].data[shift:]=st2[i].data[:-shift]\n",
    "            xcor = obspy.signal.cross_correlation.correlate(st4[i].data[:],sst[0].data[:],200)\n",
    "            index = np.argmax(xcor)\n",
    "            cc = round(xcor[index],9)\n",
    "            shift = 200-index\n",
    "            print(shift,cc) #shift and cross correlation values after alignment. shift should be 0\n",
    "        print('shift complete')\n",
    "        print( )\n",
    "        sst2=st4.copy() #sst2 is a copy of st4 once alignment is finished\n",
    "        sst2.stack() #stack aligned waveforms\n",
    "        if len(sst)<2: #some clusters will have 1 or 0 traces due to unavailable data\n",
    "            print('sst not long enough')\n",
    "            t4=time()\n",
    "            print(str(t4-t2)+' seconds to attempt to find waveforms')\n",
    "            continue\n",
    "\n",
    "        print('length of sst: '+str(len(sst))) #should be 2 or higher\n",
    "        st3.append(sst2[0])\n",
    "\n",
    "        cid.append('rp'+volc_list_names[vv][:2].lower()+str(cl).zfill(len(str(clid[-1])))) #append the clusterID to cid. rp for REDpy, volc_list_names[vv][:2].lower() for the volcano name\n",
    "        # writing seisbench h5py\n",
    "\n",
    "        lat = volc_md[volc_md['Station'] == v[s]]['Latitude'].values[0]\n",
    "        lon = volc_md[volc_md['Station'] == v[s]]['Longitude'].values[0]\n",
    "       # fill in metadata \n",
    "        meta = meta.append({\"source_id\": cid[-1], \"source_origin_time\": '', \n",
    "            \"source_latitude_deg\": \"%.3f\" % 0, \"source_longitude_deg\": \"%.3f\" % 0, \n",
    "            \"source_type\": 'unknown',\n",
    "            \"source_depth_km\": \"%.3f\" % 0, \"source_magnitude\": 0,\n",
    "            \"station_network_code\": sst[0].stats.network, \"trace_channel\": sst[0].stats.channel, \n",
    "            \"station_code\": v[s], \"station_location_code\": sst[0].stats.location,\n",
    "            \"station_latitude_deg\": lat,  \"station_longitude_deg\": lon,\n",
    "            \"station_elevation_m\": 0,\n",
    "            \"trace_p_arrival_sample\": 0, \"CODE\": v[s].lower()+'bhz'+cid[-1]}, ignore_index = True)\n",
    "\n",
    "        # fill in data\n",
    "        ibucket = np.random.choice(list(np.arange(nbucket) + 1))\n",
    "        data = update_data(data, st3[-1], ibucket)\n",
    "        print('ibucket: ',ibucket,data[ibucket])\n",
    "        t3=time()\n",
    "        print(str(t3-t2)+' seconds to make the stack for this cluster')\n",
    "# BREAK FOR THE CLUSTER LOOP\n",
    "#         break\n",
    "    t1=time()\n",
    "    print(str(t1-t0)+'seconds to make stacks for one station for all clusters with enough data')\n",
    "    # integrate tribe\n",
    "    temp_list = [] #will be a list of streams,each stream has one stack as a trace\n",
    "    for i in range(0,len(st3)):\n",
    "        temp = obspy.Stream(st3[i]) #give each trace in st3 its own stream\n",
    "        temp_list.append(temp) #append each stream to temp_list\n",
    "        print('appending temp list')\n",
    "    print(temp_list)\n",
    "\n",
    "    print('got temp list!')\n",
    "\n",
    "\n",
    "    template_stream = [] #will be a list of streams that have been filtered to become templates\n",
    "    for i in range(0,len(temp_list)):\n",
    "        template_stream.append(eqcorrscan.core.match_filter.template.Template(\n",
    "            name=v[s].lower()+'bhz'+cid[i],st=temp_list[i], \\\n",
    "                lowcut=fqmin, highcut=fqmax, samp_rate=fs, filt_order=4, process_length=200))\n",
    "        #filter each stream from temp_list and append to template_stream\n",
    "        print('appending template stream')\n",
    "    print('made template stream!')\n",
    "\n",
    "    tribe = eqcorrscan.core.match_filter.tribe.Tribe(templates = template_stream)\n",
    "    #make a tribe from template_stream, each template will be made from a stream in template_stream\n",
    "    print('making tribe...')\n",
    "\n",
    "    if len(st3) > 0:\n",
    "        print('Writing tribe in progress...')\n",
    "        tribe.write(homedir+'/templates/Volcano_' + volc_list_names[vv] + '_Station_' + v[s] + '_Channel_' + st3[0].stats.channel)\n",
    "        print('Wrote tribe sucessfully!')\n",
    "        # add to file\n",
    "        meta.to_csv(\"/data/whd02/Data_rp/metadata_\"+volc_list_names[vv]+\"_\"+v[s]+\".csv\",sep = ',', index=False)\n",
    "        f = h5py.File(\"/data/whd02/Data_rp/waveforms_\"+volc_list_names[vv]+\"_\"+v[s]+\".hdf5\",'a') #appending mode\n",
    "            #If the file does not exist, it creates a new file for writing.\n",
    "        # need to define f in order to close it in order to open it in mode w\n",
    "        if f: f.close()\n",
    "        f = h5py.File(\"/data/whd02/Data_rp/waveforms_\"+volc_list_names[vv]+\"_\"+st3[0].stats.station+\".hdf5\",'w') #writing mode\n",
    "        f['/data_format/component_order'] ='ZNE'\n",
    "        print(range(nbucket))\n",
    "        for b in range(nbucket):\n",
    "            f['/data/bucket%d' % (b + 1)] = data[b + 1]\n",
    "        f.close()\n",
    "\n",
    "        print('Saved!')\n",
    "# BREAK FOR THE STATION LOOP\n",
    "#     break\n",
    "# BREAK FOR THE VOLCANO LOOP\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85907bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
