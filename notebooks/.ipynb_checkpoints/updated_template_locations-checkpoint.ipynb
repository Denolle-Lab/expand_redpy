{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348ac66f",
   "metadata": {},
   "source": [
    "Created August 17, 2023\n",
    "\n",
    "Purpose is to update location grid search in the following ways:\n",
    "1. use templates whose constituent waveforms were normalized before stacking (fixes most false features)\n",
    "2. use elep to find picktimes instead of envelope cross correlation\n",
    "3. base velocity model of grid search on p and s picktimes when possible\n",
    "4. compare locations to ComCat potential matches from the <a href=\"https://assets.pnsn.org/red/\">redpy website</a> when possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ebc586",
   "metadata": {},
   "source": [
    "last updated November 19, 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5425951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import h5py\n",
    "import yaml\n",
    "import csv\n",
    "import math\n",
    "import eqcorrscan\n",
    "from eqcorrscan import Tribe\n",
    "from time import time\n",
    "import obspy\n",
    "from obspy import UTCDateTime, Trace\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from obspy.signal.cross_correlation import *\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy import distance\n",
    "from tqdm import trange\n",
    "\n",
    "import torch\n",
    "plt.rcParams.update({'font.size': 10})\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "import seisbench.models as sbm\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "from ELEP.elep.ensemble_statistics import ensemble_statistics\n",
    "from ELEP.elep.ensemble_coherence import ensemble_semblance \n",
    "from ELEP.elep.ensemble_learners import ensemble_regressor_cnn\n",
    "from mbf_elep_func import apply_mbf\n",
    "from ELEP.elep import mbf, mbf_utils\n",
    "from ELEP.elep import trigger_func\n",
    "from ELEP.elep.trigger_func import picks_summary_simple\n",
    "\n",
    "from ELEP.elep.mbf_utils import make_LogFq, make_LinFq, rec_filter_coeff, create_obspy_trace\n",
    "from ELEP.elep.mbf import MB_filter as MBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a076193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read config file for parameters\n",
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "smooth_length = config['smooth_length']\n",
    "fs = config['fs']\n",
    "tb = config['tb']\n",
    "ta = config['ta']\n",
    "fqmin = config['fqmin']\n",
    "fqmax = config['fqmax']\n",
    "chan = config['chan']\n",
    "homedir = config['homedir']\n",
    "readdir = config['readdir']\n",
    "minsta = config['minsta']\n",
    "grid_length = float(config['grid_length'])\n",
    "grid_height = float(config['grid_height'])\n",
    "step = config['step']\n",
    "t_step = config['t_step']\n",
    "vs_min = config['vs_min']\n",
    "vs_max = config['vs_max']\n",
    "vs_step = config['vs_step']\n",
    "volc_lat_lon = config['volc_lat_lon']\n",
    "volc_list_names = config['volc_list_names']\n",
    "nlta = config['nlta']\n",
    "\n",
    "vv = config['vv']\n",
    "\n",
    "tape = 0.05 #percent of waveform to taper on either side\n",
    "\n",
    "# istart = config['nlta']*fs\n",
    "# print(istart)\n",
    "\n",
    "\n",
    "print(volc_list_names[vv])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3fda39",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv')\n",
    "#associate network and station\n",
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#color map for plotting\n",
    "def get_cmap(n, name='viridis'): #hsv\n",
    "#     Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "#     RGB color; the keyword argument name must be a standard mpl colormap name.\n",
    "    return plt.cm.get_cmap(name, n)\n",
    "\n",
    "# define function to predict synthetic arrival times\n",
    "def travel_time(t0, x, y, vs, sta_x, sta_y):\n",
    "    dist = np.sqrt((sta_x - x)**2 + (sta_y - y)**2)\n",
    "    tt = t0 + dist/vs\n",
    "    return tt\n",
    "\n",
    "# define function to compute residual sum of squares\n",
    "def error(synth_arrivals,arrivals):\n",
    "    res = arrivals - synth_arrivals   #make sure arrivals are in the right order, maybe iterate through keys\n",
    "    res_sqr = res**2\n",
    "    rss = np.sum(res_sqr)\n",
    "    return rss\n",
    "\n",
    "# define function to iterate through grid and calculate travel time residuals\n",
    "def gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals):\n",
    "    rss_mat = np.zeros((len(t0),len(x_vect),len(y_vect)))\n",
    "    rss_mat[:,:,:] = np.nan\n",
    "    for i in range(len(t0)): \n",
    "        for j in range(len(x_vect)):\n",
    "            for k in range(len(y_vect)):\n",
    "                for m in range(len(vs)): #parameterize velocity\n",
    "                    synth_arrivals = []\n",
    "                    for h in range(len(sta_x)):\n",
    "                        tt = travel_time(t0[i],x_vect[j],y_vect[k],vs[m],sta_x[h],sta_y[h]) \n",
    "                    #add vs in nested loop, vector 1000-5000, per cluster to account for p and s waves\n",
    "                        synth_arrivals.append(tt)\n",
    "                    rss = error(np.array(synth_arrivals),np.array(arrivals))\n",
    "                    rss_mat[i,j,k] = rss\n",
    "    return rss_mat\n",
    "\n",
    "# define function to convert the location index into latitude and longitude\n",
    "def location(x_dist, y_dist, start_lat, start_lon):\n",
    "    bearing = 90-np.rad2deg(np.arctan(y_dist/x_dist))\n",
    "    dist = np.sqrt((x_dist)**2 + (y_dist)**2)\n",
    "    d = distance.geodesic(meters = dist)\n",
    "    loc_lat = d.destination(point=[start_lat,start_lon], bearing=bearing)[0]\n",
    "    loc_lon = d.destination(point=[start_lat,start_lon], bearing=bearing)[1]\n",
    "    return loc_lat, loc_lon, d\n",
    "\n",
    "# define function to find diameter in meters of the error on the location\n",
    "def error_diameter(new_array):\n",
    "    min_idx = np.min(new_array[:,1])\n",
    "    max_idx = np.max(new_array[:,1])\n",
    "    difference = max_idx-min_idx\n",
    "    diameter_m = difference*1000\n",
    "    return diameter_m \n",
    "\n",
    "#get clusterid from template name\n",
    "def getcl_id(t_name_str): #for normalized\n",
    "    t_cl = int(t_name_str.split('_')[-1])\n",
    "    return t_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81685e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML picker parameters\n",
    "paras_semblance = {'dt':0.025, 'semblance_order':4, 'window_flag':True, \n",
    "                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "p_thrd, s_thrd = 0.01, 0.05\n",
    "\n",
    "# download models\n",
    "pretrain_list = [\"pnw\",\"ethz\",\"instance\",\"scedc\",\"stead\",\"geofon\",\"neic\"]\n",
    "pn_pnw_model = sbm.EQTransformer.from_pretrained('pnw')\n",
    "pn_ethz_model = sbm.EQTransformer.from_pretrained(\"ethz\")\n",
    "pn_instance_model = sbm.EQTransformer.from_pretrained(\"instance\")\n",
    "pn_scedc_model = sbm.EQTransformer.from_pretrained(\"scedc\")\n",
    "pn_stead_model = sbm.EQTransformer.from_pretrained(\"stead\")\n",
    "pn_geofon_model = sbm.EQTransformer.from_pretrained(\"geofon\")\n",
    "pn_neic_model = sbm.EQTransformer.from_pretrained(\"neic\")\n",
    "\n",
    "#list of models to run through\n",
    "list_models = [pn_pnw_model,pn_ethz_model,pn_scedc_model,pn_neic_model,pn_geofon_model,pn_stead_model,pn_instance_model]\n",
    "\n",
    "pn_pnw_model.to(device); #imodel 0\n",
    "pn_ethz_model.to(device); #imodel 1\n",
    "pn_scedc_model.to(device); #imodel 2\n",
    "pn_neic_model.to(device); #imodel 3\n",
    "pn_geofon_model.to(device); #imodel 4\n",
    "pn_stead_model.to(device); #imodel 5\n",
    "pn_instance_model.to(device); #imodel 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774096b3",
   "metadata": {},
   "source": [
    "Find picktimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in h5 for volcano\n",
    "\n",
    "all_temps = []\n",
    "all_waves = []\n",
    "\n",
    "for filepath in glob(f'/home/smocz/expand_redpy_new_files/h5/normalized_{volc_list_names[vv].lower()}_templates_*.h5'):\n",
    "    net = filepath.split('_')[-2]\n",
    "    with h5py.File(filepath, \"r\") as f: #pull in fingerprints\n",
    "        template_name = f[\"template_name\"][()]\n",
    "        waveforms = f[\"waveforms\"][()]\n",
    "#         print(f.keys()) #print what data is in this file\n",
    "    [all_temps.append(i) for i in template_name]\n",
    "    [all_waves.append(i) for i in waveforms]\n",
    "    \n",
    "all_waves = np.array(all_waves)\n",
    "all_temps = [str(i)[2:-1] for i in all_temps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some info\n",
    "v = volc_md[volc_md['Volcano_Name'] == volc_list_names[vv]]['netsta'].values.tolist() #list of network and station per volc\n",
    "clid = np.unique([getcl_id(i) for i in all_temps]) #list of cluster ids\n",
    "cllen = len(str(max(clid))) #length of the largest cluster ID, used for zfill\n",
    "zz = chan[-2:].lower() #the last two letters of channel names (essentially the letters in chan)\n",
    "csv_name = f'{homedir}locations/{volc_list_names[vv]}_ELEP_normalized_picktimes.csv' #name of the csv for picktimes at this volcano\n",
    "h5_name = f'{homedir}h5/{volc_list_names[vv]}_ELEP_smb_pred.h5' #name of h5 file for smb_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24567ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create csv for volcano\n",
    "# with open(csv_name, 'w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(['Network','Station','Cluster_ID','Template_Name','SMB_peak']) #,'SMB_peak_MBF'\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37d1b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### PULL IN TEMPLATES ###\n",
    "# cl_trange = trange(max(clid), desc=\"Finding picktimes for each cluster\", leave=True)\n",
    "cl_trange=range(0,max(clid))\n",
    "for cl in cl_trange:\n",
    "#     print('------') #print a divider\n",
    "#     print(\"cluster:\",str(cl).zfill(cllen)) #print the cluster ID\n",
    "    \n",
    "    temps_w = [] #list of templates waveforms for this cluster\n",
    "    temps_n = [] #list of template names\n",
    "    temps_p = [] #list of template picks (list of arrays)\n",
    "    preds = [] #pred data for looking\n",
    "    \n",
    "    stopwatch0=time() #note the time\n",
    "    \n",
    "    s_trange = trange(len(v), desc=f\"Finding picktimes for each station at cluster {cl}\", leave=True) #has a progress bar\n",
    "#     s_trange = range(0,len(v)) #no progress bar\n",
    "\n",
    "    rows = [] #list of rows to append to csv\n",
    "    for s in s_trange: #loop through stations that have a template for this cluster\n",
    "        net, sta =  v[s].split('.') #add specific network per station\n",
    "        \n",
    "        for tt,t in enumerate(all_temps): #go through each template\n",
    "            if t.split('_')[0]==net and t.split('_')[1]==sta and t.split('_')[-1]==str(cl): #if the template is for this station and cluster\n",
    "                \n",
    "                ### PREPARE DATA ###\n",
    "#                 Trace(all_waves[tt]).plot(); #show before preparation\n",
    "                wave = all_waves[tt].copy() #create copy\n",
    "                \n",
    "                t_tapered = Trace(wave).taper(tape) #make waveform a trace to taper it\n",
    "                padded_wave = np.hstack((t_tapered.data[:],np.zeros(2480))) #take data from tapered trace and pad end \n",
    "                #with zeros so that len(t_trace)=6000 and will fit in the nueral network\n",
    "                t_trace = Trace(padded_wave,{'sampling_rate':fs}) #make back into a Trace, and set sampling rate\n",
    "#                 print(len(t_trace))\n",
    "                \n",
    "                temps_w.append(t_trace) #append trace\n",
    "                temps_n.append(t) #append name\n",
    "                \n",
    "#                 t_trace.plot(); #plot trace after preparation\n",
    "\n",
    "    \n",
    "                ### FIND PICKS! ###\n",
    "\n",
    "                #picking params\n",
    "                evt_data = Stream(traces=[t_trace])\n",
    "                sta_available = [sta]\n",
    "                list_models = list_models\n",
    "                twin = len(t_trace)-1\n",
    "\n",
    "\n",
    "                dt = 1/fs; fs = fs\n",
    "                nfqs = 5\n",
    "                nt = 6000; nc = 3\n",
    "                iend = len(wave)-1 #end of possible pick times\n",
    "                istart = math.ceil(tape*(len(wave)-1)) #beginning of possible pick times (currently excluding taper)\n",
    "                \n",
    "                fq_list = make_LogFq(fqmin, fqmax, dt, nfqs)\n",
    "                coeff_HP, coeff_LP = rec_filter_coeff(fq_list, dt)\n",
    "                MBF_paras = {'f_min':fqmin, 'f_max':fqmax, 'nfqs':nfqs, 'frequencies':fq_list, 'CN_HP':coeff_HP, 'CN_LP':coeff_LP,                     'dt':dt, 'fs':fs, 'nt':nt, 'nc':nc, 'npoles': 2}\n",
    "\n",
    "                paras_semblance = {'dt':dt, 'semblance_order':2, 'window_flag':True, \n",
    "                                   'semblance_win':0.5, 'weight_flag':'max'}\n",
    "                \n",
    "                #find picktimes!\n",
    "                peaks,smb_pred = apply_mbf(evt_data, sta_available,                 list_models, MBF_paras, paras_semblance, istart, iend) #smb_peak,smb_peak_mbf,\n",
    "                \n",
    "                \n",
    "                print(peaks[0]) # THESE PICKS ARE OFFSET BY ISTART\n",
    "                \n",
    "                picks = [i+istart for i in peaks[0]] # ADDED ISTART\n",
    "                temps_p.append(picks)\n",
    "                csv_picks=' '.join([str(i/fs)for i in picks]) #formatted for saving in csv\n",
    "                \n",
    "                preds.append([smb_pred])\n",
    "        \n",
    "                #append picks to csv\n",
    "                row = [net, sta, cl, t, csv_picks] ### APPEND PICKS NOT PEAKS\n",
    "                rows.append(row)\n",
    "\n",
    "    ### FILTERING PEAKS ###\n",
    "\n",
    "    one_peak = [] #list of n and y if the template only has one peak/picktime\n",
    "    for p in temps_p: #for saved picks\n",
    "        if len(p)==1:\n",
    "            one_peak.append('y')\n",
    "        else:\n",
    "            one_peak.append('n')\n",
    "            \n",
    "    if one_peak.count('y')/len(one_peak) ==1: #if there is only one peak per template\n",
    "        #write info to csv as is\n",
    "        for row in rows:\n",
    "            with open(csv_name, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(row)\n",
    "                file.close()\n",
    "            \n",
    "    if one_peak.count('y')/len(one_peak) >=0.5 and one_peak.count('y')/len(one_peak) <1: #if 75% of templates have one peak\n",
    "        \n",
    "        one_p_value = [] #list of peak values when there is only one\n",
    "        for peaks in temps_p:\n",
    "            if len(peaks)==1:\n",
    "                one_p_value.append(peaks[0])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        for en,peaks in enumerate(temps_p):\n",
    "            if len(peaks)>1:\n",
    "                sums = []#list of sum of differences between possible peaks and the \"confirmed\" peaks\n",
    "                for peak in peaks:\n",
    "                    sums.append(sum([abs(peak-v) for v in one_p_value]))\n",
    "                closest_peak = peaks[sums.index(min(sums))] #find which peak has the smallest summed distance\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            rows[en][-1] = str(closest_peak/fs) #update the corresponding row's pick for csv\n",
    "            \n",
    "        #write updated info to csv\n",
    "        for row in rows:\n",
    "            with open(csv_name, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(row)\n",
    "                file.close()\n",
    "                \n",
    "                \n",
    "    if one_peak.count('y')/len(one_peak) <0.5:\n",
    "        \n",
    "        for en,peaks in enumerate(temps_p):\n",
    "            if len(peaks)>1:\n",
    "                rows[en][-1] = 'UNCERTAIN'\n",
    "                \n",
    "        #write updated info to csv\n",
    "        for row in rows:\n",
    "            with open(csv_name, 'a', newline='') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow(row)\n",
    "                file.close()\n",
    "                \n",
    "    #save smb_pred to h5\n",
    "    try:\n",
    "        open(h5_name)\n",
    "        with h5py.File(h5_name, \"a\") as f: #append info\n",
    "            f.create_dataset(f\"smb_pred_cl_{cl}\", data=np.array(preds))\n",
    "            f.create_dataset(f\"template_names_cl_{cl}\", data=temps_n)\n",
    "    except:\n",
    "        with h5py.File(h5_name, \"w\") as f: #create file\n",
    "            f.create_dataset(f\"smb_pred_cl_{cl}\", data=np.array(preds))\n",
    "            f.create_dataset(f\"template_names_cl_{cl}\", data=temps_n)\n",
    "                    \n",
    "    ### PLOT ###\n",
    "    height = len(temps_w)\n",
    "    fig, ax0 = plt.subplots(figsize=(6,height))\n",
    "\n",
    "    yscale = 2 #how far to space waveforms from eachother\n",
    "    wavecolor = 'black'\n",
    "    for ww, wave in enumerate(temps_w):\n",
    "        ax0.plot(wave.data[:]/np.max(np.abs(wave.data))+yscale+(yscale*ww),color=wavecolor,linewidth=.5)\n",
    "        for i in temps_p[ww]: \n",
    "            plt.vlines(x=i,ymin=(yscale+(yscale*ww))-(0.5*yscale),ymax=(yscale+(yscale*ww))+(0.5*yscale),color='red')\n",
    "    plt.vlines(x=nlta*fs,ymin=0,ymax=(yscale+(yscale*ww))+(0.5*yscale),color='gray',linestyle='dashed')\n",
    "    #RED is possible pick times\n",
    "    #GRAY is the 10 second mark/end noise window for SNR\n",
    "\n",
    "    ax0.tick_params(axis='y', which='both',left=False,labelleft=False)\n",
    "\n",
    "\n",
    "    fig.suptitle(f'Cluster {cl} on {volc_list_names[vv]}')\n",
    "    fig.set_tight_layout(True)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8e1b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read h5 file\n",
    "cl = 13\n",
    "\n",
    "with h5py.File(h5_name, \"r\") as f:\n",
    "    h5_names_test = f[f\"template_names_cl_{cl}\"][()]\n",
    "    smb = f[f\"smb_pred_cl_{cl}\"][()]\n",
    "\n",
    "print(h5_names_test)\n",
    "\n",
    "for s in smb:\n",
    "    plt.plot(s[0][0]);\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95de224",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ss,smb_mbf in enumerate(preds):\n",
    "    smb_pred = smb_mbf[0]\n",
    "#     smb_pred_mbf = smb_mbf[1]\n",
    "    \n",
    "    fig, ax0 = plt.subplots(figsize=(6,6))\n",
    "    ax0.plot(smb_pred[0])\n",
    "    fig.suptitle(f'smb_pred {temps_p[ss]}')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b6a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peaks[0])\n",
    "peaks_test = [str(i+istart) for i in peaks[0]]\n",
    "print(peaks_test)\n",
    "# picks = list([i+istart for i in peaks[0]])\n",
    "print(' '.join(list(peaks_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b19930",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([610,780])\n",
    "peaks_test = [str(i+istart) for i in [610,780]]\n",
    "print(peaks_test)\n",
    "# picks = list([i+istart for i in peaks[0]])\n",
    "print(type(' '.join(list(peaks_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dff4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = len(temps_w)\n",
    "fig, ax0 = plt.subplots(figsize=(6,height))\n",
    "\n",
    "yscale = 2 #how far to space waveforms from eachother\n",
    "wavecolor = 'black'\n",
    "for ww, wave in enumerate(temps_w):\n",
    "    ax0.plot(wave.data[:]/np.max(np.abs(wave.data))+yscale+(yscale*ww),color=wavecolor,linewidth=.5)\n",
    "    for i in temps_p[ww][0][0]: \n",
    "        plt.vlines(x=i,ymin=(yscale+(yscale*ww))-(0.5*yscale),ymax=(yscale+(yscale*ww))+(0.5*yscale),color='red')\n",
    "plt.vlines(x=nlta*fs,ymin=0,ymax=(yscale+(yscale*ww))+(0.5*yscale),color='gray',linestyle='dashed')\n",
    "#RED is possible pick times\n",
    "#GRAY is the 10 second mark/end noise window for SNR\n",
    "\n",
    "ax0.tick_params(axis='y', which='both',left=False,labelleft=False)\n",
    "\n",
    "\n",
    "fig.suptitle(f'Cluster {cl} on {volc_list_names[vv]}')\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29626ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(temps_p)\n",
    "\n",
    "for ww in range(0,5):\n",
    "    for i in temps_p[ww]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "height = len(temps_w)\n",
    "fig, ax0 = plt.subplots(figsize=(6,height))\n",
    "\n",
    "yscale = 2 #how far to space waveforms from eachother\n",
    "wavecolor = 'black'\n",
    "for ww, wave in enumerate(temps_w):\n",
    "    ax0.plot(wave.data[:]/np.max(np.abs(wave.data))+yscale+(yscale*ww),color=wavecolor,linewidth=.5)\n",
    "    for i in temps_p[ww]: \n",
    "        print(i)\n",
    "        plt.vlines(x=int(i),ymin=(yscale+(yscale*ww))-(0.5*yscale),ymax=(yscale+(yscale*ww))+(0.5*yscale),color='red')\n",
    "plt.vlines(x=nlta*fs,ymin=0,ymax=(yscale+(yscale*ww))+(0.5*yscale),color='gray',linestyle='dashed')\n",
    "#RED is possible pick times\n",
    "#GRAY is the 10 second mark/end noise window for SNR\n",
    "\n",
    "ax0.tick_params(axis='y', which='both',left=False,labelleft=False)\n",
    "\n",
    "\n",
    "fig.suptitle(f'Cluster {cl} on {volc_list_names[vv]}')\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(temps_p[ww][2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c030ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_trace.data[:6000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6380b507",
   "metadata": {},
   "source": [
    "### Plotting times as vlines on templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc359a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull in csv with picktimes\n",
    "picktimes_df = pd.read_csv(csv_name)\n",
    "picktimes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587bd02c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loop to plot each template, and each picktime as a vline\n",
    "#one plot per redpy cluster\n",
    "\n",
    "### PULL IN TEMPLATES ###\n",
    "# cl_trange = trange(max(clid), desc=\"Finding picktimes for each cluster\", leave=True)\n",
    "for cl in range(0,max(clid)):\n",
    "#     print('------') #print a divider\n",
    "#     print(\"cluster:\",str(cl).zfill(cllen)) #print the cluster ID\n",
    "    \n",
    "    temps_w = [] #list of templates waveforms for this cluster\n",
    "    temps_n = [] #list of template names\n",
    "    temps_p = [] #list of template picks (list of lists)\n",
    "    \n",
    "    stopwatch0=time() #note the time\n",
    "    \n",
    "#     s_trange = trange(len(v), desc=f\"Finding picktimes for each station at cluster {cl}\", leave=True)\n",
    "    for s in range(0,len(v)): #loop through stations that have a template for this cluster\n",
    "        net, sta =  v[s].split('.') #add specific network per station\n",
    "        \n",
    "        for tt,t in enumerate(all_temps): #go through each template\n",
    "            if t.split('_')[0]==net and t.split('_')[1]==sta and t.split('_')[-1]==str(cl): #if the template is for this station and cluster\n",
    "                \n",
    "                ### PREPARE DATA ###\n",
    "                wave = Trace(all_waves[tt]).copy() #create copy\n",
    "                temps_w.append(wave)\n",
    "                temps_n.append(t)\n",
    "                \n",
    "                #pull picktimes from csv\n",
    "                smb = picktimes_df[picktimes_df['Template_Name']==t]['SMB_peak'].values.tolist()[0]\n",
    "                mbf = picktimes_df[picktimes_df['Template_Name']==t]['SMB_peak_MBF'].values.tolist()[0]\n",
    "                \n",
    "                temps_p.append([smb,mbf])\n",
    "    \n",
    "    #plot for cluster\n",
    "    \n",
    "    height = len(temps_w)\n",
    "    fig, ax0 = plt.subplots(figsize=(6,height))\n",
    "\n",
    "    yscale = 2 #how far to space waveforms from eachother\n",
    "    wavecolor = 'black'\n",
    "    for ww, wave in enumerate(temps_w):\n",
    "        ax0.plot(wave.data[:]/np.max(np.abs(wave.data))+yscale+(yscale*ww),color=wavecolor,linewidth=.5)\n",
    "        plt.vlines(x=temps_p[ww][0]*fs,ymin=(yscale+(yscale*ww))-(0.5*yscale),ymax=(yscale+(yscale*ww))+(0.5*yscale),color='red')\n",
    "        plt.vlines(x=temps_p[ww][1]*fs,ymin=(yscale+(yscale*ww))-(0.5*yscale),ymax=(yscale+(yscale*ww))+(0.5*yscale),color='blue')\n",
    "    plt.vlines(x=10*fs,ymin=0,ymax=(yscale+(yscale*ww))+(0.5*yscale),color='gray',linestyle='dashed')\n",
    "    #RED is smb\n",
    "    #BLUE is mbf\n",
    "    #GRAY is the 10 second mark/end noise window for SNR\n",
    "\n",
    "    ax0.tick_params(axis='y', which='both',left=False,labelleft=False)\n",
    "    \n",
    "\n",
    "    fig.suptitle(f'Cluster {cl} on {volc_list_names[vv]}')\n",
    "    fig.set_tight_layout(True)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8784fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run code normally (exclude mbf to speed it up)\n",
    "#for clusters that do not start before 10s, redo the picktime (istart=10)\n",
    "#when finding location, exclude outliers (outside expected phase shift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bfb2eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82da1415",
   "metadata": {},
   "source": [
    "### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2fb786",
   "metadata": {},
   "outputs": [],
   "source": [
    "array0 = np.array([0,0,0,3,4,5,6])\n",
    "array1 = np.zeros(6)\n",
    "print(np.hstack((array0,array1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f78b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(Trace(all_waves[tt],{'sampling_rate':40}))/40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069b5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = volc_sta[vv] \n",
    "clid = np.unique([getcl_id(i) for i in all_temps]) #list of cluster ids\n",
    "cllen = len(max(clid)) #length of the largest cluster ID, used for zfill\n",
    "zz = chan[-2:].lower() #the last two letters of channel names (essentially the letters in chan)\n",
    "#make csv?\n",
    "\n",
    "\n",
    "\n",
    "for cl in range(0,max(clid)):#options: cl in range(0,clid[-1]+1) for specific ranges; cl in clid for all Cluster IDs\n",
    "    temps_s = {} #empty dictionary that will be filled with the templates for this cluster\n",
    "    print('------') #print a divider\n",
    "    print(\"cluster:\",str(cl).zfill(cllen)) #print the cluster ID\n",
    "    stopwatch0=time() #note the time\n",
    "    for s in range(0,len(v)): #loop through stations\n",
    "        net, sta =  v[s].split('.') #add specific network per station\n",
    "\n",
    "########################################################################    \n",
    "#                       FINDING PICK TIMES                             #\n",
    "########################################################################\n",
    "\n",
    "        # try to read the h5 file\n",
    "        try:\n",
    "            T = Tribe().read(*glob(f'{homedir}templates/Volcano_{volc_list_names[vv]}_Network_{net}_Station_{sta}_Channel_*.tgz'))\n",
    "        except:\n",
    "            print(f'{net}.{sta} tgz does not exist')\n",
    "            continue\n",
    "        for t in T: #for each template in the tgz\n",
    "            if t.name.endswith(str(cl).zfill(cllen)): #if the template name endswith this cluster\n",
    "                temps_s[f'{net.lower()}.{t.name}']=t #save to dictionary and include network name\n",
    "                break\n",
    "\n",
    "    if len(temps_s) < minsta: #if the number of templates for this cluster is less than minsta (see config)\n",
    "        print('not enough stations with data for this cluster') #print a reminder\n",
    "        stopwatch2=time() #note the time\n",
    "        print(f'{stopwatch2-stopwatch0} s for this cluster') #say how many s to go through this cluster\n",
    "        continue #move onto the next cluster\n",
    "        \n",
    "    data_env_dict = {} #a dictionary of the envelopes for each template stream\n",
    "    for t in temps_s: #for each saved template (aka each template for this cluster and volc that exists)\n",
    "        data_envelope = obspy.signal.filter.envelope(temps_s[t].st[0].data) #make an envelope\n",
    "        data_envelope /= np.max(data_envelope) #average envelope (?)\n",
    "        data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length) #smooth the envelope\n",
    "        data_env_dict[t] = data_envelope #save the envelope to the dictionary\n",
    "\n",
    "    pick_times = {} #dictionary of picktimes for each template\n",
    "    for key in data_env_dict: #for each envelope\n",
    "        p, shift, relative_p = pick_time(ref_env=data_env_dict[list(data_env_dict.keys())[0]], \n",
    "                data_env_dict=data_env_dict[key],st=temps_s[key].st) #calculate picktimes\n",
    "        pick_times[key] = relative_p #save to dictionary\n",
    "    print(f'{cl} offsets are {pick_times}') #print pick times relative to first template stream (can be negative)\n",
    "    \n",
    "    #arranging picktimes from 0 (earliest station) to later (positive) times\n",
    "    #will NOT be used for plotting picktimes, but will be used for location\n",
    "    dif_dict = {} #dictionary of picktimes in reference to earliest picktime (in positive seconds after the earliest picktime)\n",
    "    max_value = max(pick_times, key=pick_times.get) #get key for max value of pick_times aka the earliest picktime\n",
    "    for key in pick_times: #for each picktime\n",
    "        dif = round(abs(pick_times[max_value] - pick_times[key]),4) #max value minus current value\n",
    "        dif_dict[key] = dif #save to dictionary with the same key as pick_times\n",
    "\n",
    "    \n",
    "########################################################################    \n",
    "#                       PLOTTING PICK TIMES                            #\n",
    "########################################################################\n",
    "\n",
    "#     cmap = get_cmap(len(temps_s)) #get cmap aka colors for the plot, see def(get_cmap) for color palette\n",
    "#     plt.figure(figsize=(10,10)) #set plot size\n",
    "#     plt.title('aligned templates, vlines are template starts') #plot title\n",
    "#     for tt,t in enumerate(temps_s): #for every template\n",
    "#         shift = round(pick_times[t]*fs) #find shift based on picktimes\n",
    "#         st0 = temps_s[t].st.copy() #make a copy for a reference\n",
    "#         maxdata = len(temps_s[t].st[0].data[:]) #find maximum length of template stream\n",
    "\n",
    "#         empty = Trace(np.zeros(shift)) #an empty trace/a trace filled with zeros\n",
    "#         if shift<0: #if shift is negative\n",
    "#             temps_s[t].st[0].data[:shift]=st0[0].data[-shift:] #shift to the left\n",
    "#         if shift>0: #if shift is positive\n",
    "#             temps_s[t].st[0].data[shift:]=st0[0].data[:-shift] #shift to the right\n",
    "#             temps_s[t].st[0].data[:shift]=empty.data[:shift] #get rid of leftover data from shift\n",
    "#         #note: if shift == 0, will be plotting with no shifting\n",
    "#         plt.plot(temps_s[t].st[0].data[:]/np.max(np.abs(temps_s[t].st[0].data[:]))+2*tt,color=cmap(tt), label=t) #plot stream\n",
    "#         plt.vlines(shift,ymin=-1,ymax=2*len(temps_s),color=cmap(tt)) #plot line in same color representing the start of the template\n",
    "#         plt.legend() #show the legend\n",
    "    \n",
    "#     first_sta = max(pick_times, key=pick_times.get) #gives you the template name for the first\n",
    "    #station to get a signal (largest/most positive pick time)\n",
    "    \n",
    "########################################################################    \n",
    "#                         FINDING LOCATION                             #\n",
    "########################################################################\n",
    "\n",
    "    # define input parameters\n",
    "    arrivals = [] #relative picktimes, dif_dict as a list\n",
    "    sta_lats = [] #station latitudes, from metadata\n",
    "    sta_lons = [] #station longitudes, from metadata\n",
    "    netsta_names = [] #list of station names with networks\n",
    "    for key in dif_dict: #for each station\n",
    "        arrivals.append(dif_dict[key]) #append pick time to arrivals\n",
    "        \n",
    "        #finding station name\n",
    "        if not key.endswith(str(cl).zfill(cllen)): #if the wrong cluster id\n",
    "            print('template name does not match cluster ID') #print an error\n",
    "            continue #and skip the rest\n",
    "        if key.endswith(f'{zz}rp{volc_list_names[vv][:2].lower()}{str(cl).zfill(cllen)}'): #if the key has a channel name in it (the 'hz')\n",
    "            md_netsta = key[:-(7+cllen)] #remember the stuff before channel name rpvo and cluster ID\n",
    "        if not key.endswith(f'{zz}rp{volc_list_names[vv][:2].lower()}{str(cl).zfill(cllen)}'): #if the key has NO channel name in it\n",
    "            md_netsta = key[:-(4+cllen)] #remember the stuff before rpvo and cluster ID\n",
    "#         print(md_netsta)\n",
    "        lat = volc_md[volc_md['netsta']==md_netsta.upper()]['Latitude'].values.tolist() #get latitude from metadata\n",
    "        sta_lats.append(lat[0]) #append\n",
    "        \n",
    "        lon = volc_md[volc_md['netsta']==md_netsta.upper()]['Longitude'].values.tolist() #get longitude form metadata\n",
    "        sta_lons.append(lon[0]) #append\n",
    "        \n",
    "        netsta_names.append(md_netsta.upper()) # make list of station names\n",
    "\n",
    "\n",
    "    # define grid origin in lat,lon\n",
    "    \n",
    "    #finding bottom left corner of grid map\n",
    "    lat_start = volc_lat_lon[volc_list_names[vv]][0] - (grid_length/222000) #volcano lat minus half of grid length in decimal lat long\n",
    "    lon_start = volc_lat_lon[volc_list_names[vv]][1] - (grid_height/222000) #volcano long minus half of grid height in decimal lat long\n",
    "        \n",
    "    #station lat lons to x y\n",
    "    sta_x = []\n",
    "    sta_y = []\n",
    "    for i in range(len(sta_lats)):\n",
    "        x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "        y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "        sta_x.append(x_dist)\n",
    "        sta_y.append(y_dist)\n",
    "\n",
    "    # set grid points\n",
    "    x_vect = np.arange(0, grid_length, step)\n",
    "    y_vect = np.arange(0, grid_height, step)\n",
    "    t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "    vs = np.arange(vs_min,vs_max,vs_step)\n",
    "\n",
    "    print('yo')\n",
    "    # carry out the gridsearch\n",
    "    rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals)\n",
    "    print('here')\n",
    "    # find lowest error lat, lon, and origin time\n",
    "    loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "    \n",
    "    # find the lat and lon of the location index\n",
    "    loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "    err_thr = np.min(np.log10(rss_mat))+.05\n",
    "    thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "    diameter = error_diameter(thr_array)\n",
    "    \n",
    "    print('location lat lon',loc_lat,loc_lon)\n",
    "\n",
    "    # plot a spatial map of error for lowest-error origin time\n",
    "#     fig,ax = plt.subplots()\n",
    "#     ax.scatter(x_vect[loc_idx[1]],y_vect[loc_idx[2]],s=100,marker='*',c='r')\n",
    "#     im = ax.imshow(np.log10(rss_mat[loc_idx[0],:,:].T),origin=\"lower\",extent=[0,grid_length,0,grid_height])\n",
    "#     fig.colorbar(im)\n",
    "#     plt.show()\n",
    "    \n",
    "    row = [volc_list_names[vv],' '.join(netsta_names),' '.join([str(i) for i in arrivals]),cl,loc_lat,loc_lon]\n",
    "    with open(homedir+f'/locations/{volc_list_names[vv]}_Template_Locations.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "        file.close()\n",
    "    \n",
    "    stopwatch1=time() #note the time\n",
    "    print(f'{stopwatch1-stopwatch0} s for this cluster') #print time it took to go through this cluster\n",
    "    \n",
    "    #write into csv relative picktimes in seconds after first_sta, probably list separated by \n",
    "    #spaces, like how stations are saved in events, make sure index is same for the template \n",
    "    #name and picktime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae80e1f",
   "metadata": {},
   "source": [
    "Compare to ComCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69b55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = 11 #51 is a cluster on hood with no matches\n",
    "url = f'https://assets.pnsn.org/red/hood/clusters/{cl}.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53695837",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(url)\n",
    "\n",
    "matches = str(r.content.splitlines()[20]).split('Potential local match:')[1:]\n",
    "\n",
    "c_times = []\n",
    "c_lats = []\n",
    "c_lons = []\n",
    "if len(matches)>0:\n",
    "    for m in matches:\n",
    "        m_feats = m.split(' ')\n",
    "        m_time = m_feats[1]\n",
    "        c_times.append(m_time)\n",
    "        m_lat = float(m_feats[2][1:-1])\n",
    "        c_lats.append(m_lat)\n",
    "        m_lon = float(m_feats[3][:-1])\n",
    "        c_lons.append(m_lon)\n",
    "        print(f'TIME {m_time} LAT {m_lat} LON {m_lon}')\n",
    "\n",
    "else:\n",
    "    print('no ComCat matches')\n",
    "    \n",
    "avg_c_lat = np.average(c_lats)\n",
    "avg_c_lon = np.average(c_lons)\n",
    "print(f'Average ComCat Location lat {avg_c_lat} lon {avg_c_lon}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ec052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ComCat and my locations to compare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
