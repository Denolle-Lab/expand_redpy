{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab72beec",
   "metadata": {},
   "source": [
    "Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55331a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import numpy as np\n",
    "import obspy\n",
    "from obspy import UTCDateTime\n",
    "from time import time\n",
    "import csv\n",
    "from glob import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a75a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "vv = config['vv']\n",
    "volc_list_names = config['volc_list_names']\n",
    "volc = volc_list_names[vv]\n",
    "year = config['year']\n",
    "years = config['years']\n",
    "\n",
    "homedir = config['homedir']\n",
    "readdir = config['readdir']\n",
    "\n",
    "minsta = config['minsta']\n",
    "\n",
    "#time in seconds before and after a detection to check for similar detections\n",
    "wi = config['wi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e345e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv') #read volcano metadata\n",
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str) \n",
    "#combine network and station to keep them associated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9739282f",
   "metadata": {},
   "source": [
    "Main loop - finding overlaps - updated december 1, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7b4d342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no detections for CC.BRSP in 2004\n",
      "no detections for CC.HIYU in 2004\n",
      "no detections for CC.LSON in 2004\n",
      "no detections for CC.PALM in 2004\n",
      "no detections for CC.SHRK in 2004\n",
      "no detections for CC.TIMB in 2004\n",
      "no detections for CC.YOCR in 2004\n",
      "no detections for TA.G05D in 2004\n",
      "no detections for UO.JESE in 2004\n",
      "no detections for UW.TIMB in 2004\n",
      "no detections for ZK.EAG1 in 2004\n",
      "---\n",
      "\n",
      "------------\n",
      "trying detection 0 cluster 006 2004-08-22T09:41:19.575300Z\n",
      "no data available for CC.BRSP in 2004\n",
      "no data available for CC.HIYU in 2004\n",
      "no data available for CC.LSON in 2004\n",
      "no data available for CC.PALM in 2004\n",
      "no data available for CC.SHRK in 2004\n",
      "no data available for CC.TIMB in 2004\n",
      "no data available for CC.YOCR in 2004\n",
      "no data available for TA.G05D in 2004\n",
      "no data available for UO.JESE in 2004\n",
      "Overlap with station UW.TDH detections\n",
      "no data available for UW.TIMB in 2004\n",
      "Overlap with station UW.VFP detections\n",
      "Overlap with station UW.VLL detections\n",
      "Overlap with station UW.VLM detections\n",
      "no data available for ZK.EAG1 in 2004\n",
      "match_list: [0, 2, 2, 2, 2]\n",
      "0.013595342636108398 seconds to test overlap for detection 0\n",
      "saving...\n",
      "check: Empty DataFrame\n",
      "Columns: [Earliest_Detection_Time, Cluster_ID, Stations_Found, Stations]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Detection_Time'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Detection_Time'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    111\u001b[0m check \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_name) \u001b[38;5;66;03m#read the csv we made\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheck:\u001b[39m\u001b[38;5;124m'\u001b[39m, check)\n\u001b[0;32m--> 113\u001b[0m checktimes \u001b[38;5;241m=\u001b[39m \u001b[43mcheck\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCluster_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDetection_Time\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mtolist() \n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m#find the datetimes that have already been saved\u001b[39;00m\n\u001b[1;32m    115\u001b[0m checking \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Detection_Time'"
     ]
    }
   ],
   "source": [
    "# for vv,v in enumerate(volc_list_names): #for each volcano\n",
    "#     volc = v\n",
    "    \n",
    "t0 = time() #record time\n",
    "\n",
    "# for year in years: #for each year\n",
    "year = 2004\n",
    "    #############################################################\n",
    "\n",
    "#get detections at this volcano in this year\n",
    "sta_list = volc_md[volc_md['Volcano_Name']==volc]['netsta'].values.tolist() #get a list of stations at this volcano\n",
    "readstadict = {} #will become a dictionary of dataframes of station detections\n",
    "for i in sta_list: #for each station at the volcano\n",
    "    try: \n",
    "        readstadict[i] = pd.read_csv(homedir+f'detections/{volc}_{i}_{year}_detections.csv') \n",
    "        #get a dictionary of detections at each station\n",
    "    except: #if the detection final does NOT exist\n",
    "        print(f'no detections for {i} in {year}') #say so\n",
    "        readstadict[i] = pd.DataFrame() #make an empty data frame maintain index\n",
    "\n",
    "print('---')\n",
    "\n",
    "rsta_list = [] #list version of readstadict\n",
    "for i in readstadict: #making rsta_list\n",
    "    rsta_list.append(readstadict[i])\n",
    "readsta = pd.concat(rsta_list) #make a single dataframe for the whole volcano from the list of station dataframs\n",
    "# print(type(rsta_list[0]))\n",
    "\n",
    "# print('----')\n",
    "\n",
    "# print(readsta)\n",
    "\n",
    "#for all detections we want to run (concatenated), make cl_list for the volcano\n",
    "temp_name_list = readsta['Template_Name'].values.tolist() #make a list of template names\n",
    "cl_list_long = [] # make a list of the numbers in each template name\n",
    "for i in temp_name_list: \n",
    "    if volc=='Baker' or volc=='Hood' or volc=='Newberry' or volc=='Rainier':\n",
    "        num = i[-3:] #account for zfill, to parameterize, use what make_templates uses to get zfill amount (cid[-1] from volcmd)\n",
    "    if volc=='St_Helens':\n",
    "        num = i[-4:] #account for zfill\n",
    "#     num = re.findall(r'\\d+', i)\n",
    "    cl_list_long.append(num) #*num for findall but some stations have numbers in their name, so using zfill instead of \n",
    "    #just numbers\n",
    "cl_list = np.unique(cl_list_long) #get rid of duplicates\n",
    "\n",
    "csv_name = homedir+f'events/{volc}_{year}_events.csv'\n",
    "with open(csv_name, 'w', newline='') as file: #make a csv to save to\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Earliest_Detection_Time\",\"Cluster_ID\",\"Stations_Found\",\"Stations\"]) #,\"Stations_Diff\"\n",
    "    #detection time, cluster id, and number of stations with a detection for this event, what stations that is, ?\n",
    "    file.close()\n",
    "    #############################################################\n",
    "\n",
    "#     for cl in cl_list: #for each cluster\n",
    "cl = '006'\n",
    "        #####################################################################\n",
    "t1 = time() #record time\n",
    "times = [] #get list of datetimes for all templates for this cluster\n",
    "for i in np.unique(temp_name_list): #for each template\n",
    "    if i.endswith(cl): #if the template ends with the cluster ID\n",
    "        all_times = readsta[readsta['Template_Name']==i]['Detection_Time'].values.tolist() #find the times for that template\n",
    "        for at in all_times:\n",
    "            times.append(at) #append each template time to the list of datetimes\n",
    "for ii,i in enumerate(times): #for all detections\n",
    "    t3 = time()\n",
    "#run through each detection time on all stations\n",
    "    print( )\n",
    "    print('------------')\n",
    "    print('trying detection',ii,'cluster',cl,i)\n",
    "\n",
    "    times_diff = []\n",
    "    match_list = []\n",
    "    for ss,s in enumerate(sta_list): #for each station\n",
    "        statimes = [] #get the times for this station\n",
    "        try:\n",
    "            readstadict[s]['Template_Name']\n",
    "        except:\n",
    "            print(f'no data available for {s} in {year}')\n",
    "            continue\n",
    "        for a in np.unique(readstadict[s]['Template_Name'].values.tolist()):\n",
    "            # for every template in this station's df\n",
    "            if a.endswith(cl): #if it matches this cluster\n",
    "                all_times = readstadict[s][readstadict[s]['Template_Name']==a]['Detection_Time'].values.tolist() \n",
    "                #get a list of detection times at this station for this cluster\n",
    "                for at in all_times:\n",
    "                    statimes.append(at) #append to statimes\n",
    "        match=0 #set variable to arbitrary number\n",
    "        for tt,t in enumerate(statimes): #for every datetime in the station's detections for this cluster\n",
    "            ts = UTCDateTime(t)-wi #find the time wi s before\n",
    "            te = UTCDateTime(t)+wi #find the time wi s after\n",
    "            if UTCDateTime(i)>ts and UTCDateTime(i)<te:\n",
    "                diff = UTCDateTime(i)-UTCDateTime(t)\n",
    "                match=2 #if there is an overlap, reset the variable and break out of the loop\n",
    "                print('Overlap with station '+s+' detections')\n",
    "                break\n",
    "#                     times_diff.append(diff)\n",
    "        match_list.append(match)\n",
    "    print('match_list:',match_list)\n",
    "\n",
    "    t2 = time()\n",
    "    print(t2-t3,'seconds to test overlap for detection',ii)\n",
    "\n",
    "    save_list = [] #for saving a list of stations\n",
    "    for mm,m in enumerate(match_list):\n",
    "        if m==2:\n",
    "            save_list.append(sta_list[mm])\n",
    "\n",
    "    if match_list.count(2) >= minsta: #if at least 4 matches equal 2:\n",
    "#         if match==2 and match0==2 and match1==2 and match2==2:\n",
    "        print('saving...')\n",
    "        check = pd.read_csv(csv_name) #read the csv we made\n",
    "        print('check:', check)\n",
    "        checktimes = check[check['Cluster_ID']==int(cl)]['Earliest_Detection_Time'].values.tolist() \n",
    "        #find the datetimes that have already been saved\n",
    "        checking = 0\n",
    "        for tt,t in enumerate(checktimes):\n",
    "            ts = UTCDateTime(t)-wi\n",
    "            te = UTCDateTime(t)+wi\n",
    "            if UTCDateTime(i)>ts and UTCDateTime(i)<te: \n",
    "                print('already recorded event')\n",
    "                checking=1 #checking still =1 because we don't want the normal save process\n",
    "                #if i is earlier (less than) than the recorded event, overwrite the event (we want the earliest detected time)\n",
    "                if i<t: #if this time is earlier than the saved time\n",
    "                    print('replacing...')\n",
    "                    print('old time:',t,'new time:',i) \n",
    "                    check.replace(t,i,inplace=True) #replace t with i in the df\n",
    "                    check.to_csv(csv_name,index=False)\n",
    "                    #read panda dataframe - change dataframe, overwrite the csv with this dataframe\n",
    "                    print('changed csv')\n",
    "                break\n",
    "        if checking==1: continue # because it has already been saved\n",
    "        row = [i,int(cl),match_list.count(2),' '.join(save_list),] #put together the row, #, ' '.join(times_diff)\n",
    "        print(row)\n",
    "\n",
    "        with open(csv_name, 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(row) #save the row to csv\n",
    "            file.close()\n",
    "\n",
    "        #####################################################################\n",
    "\n",
    "#         t4 = time()\n",
    "#         print(t4-t1,'seconds for cluster',cl)\n",
    "#     t5 = time()\n",
    "#     print(t5-t0,'seconds to find all new events on',volc,'in',year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e3ad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo-py38-shared"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
