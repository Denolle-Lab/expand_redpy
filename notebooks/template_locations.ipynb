{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a4ebc8",
   "metadata": {},
   "source": [
    "This is based on Francesca's picktime and grid search code - see her github for alt version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08884cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched-filter CPU is not compiled! Should be here: /home/jupyter_share/miniconda3/envs/seismo/lib/python3.8/site-packages/fast_matched_filter/lib/matched_filter_CPU.so\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import csv\n",
    "import eqcorrscan\n",
    "from eqcorrscan import Tribe\n",
    "import obspy\n",
    "from obspy import UTCDateTime, Trace\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from obspy.signal.cross_correlation import *\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aad013",
   "metadata": {},
   "source": [
    "### Find Picktimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84331d83",
   "metadata": {},
   "source": [
    "read config file for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70386a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/smocz/expand_redpy/scripts/config.yaml') as file:\n",
    "    config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "smooth_length = config['smooth_length']\n",
    "fs = config['fs']\n",
    "tb = config['tb']\n",
    "ta = config['ta']\n",
    "fqmin = config['fqmin']\n",
    "fqmax = config['fqmax']\n",
    "chan = config['chan']\n",
    "homedir = config['homedir']\n",
    "readdir = config['readdir']\n",
    "minsta = config['minsta']\n",
    "grid_length = float(config['grid_length'])\n",
    "grid_height = float(config['grid_height'])\n",
    "step = config['step']\n",
    "t_step = config['t_step']\n",
    "vs = config['vs']\n",
    "volc_lat_lon = config['volc_lat_lon']\n",
    "\n",
    "vv = config['vv']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7b490",
   "metadata": {},
   "source": [
    "Read REDpy Catalogs and Volcano Metadata File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5a6b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baker = pd.read_csv(readdir+'Baker_catalog.csv')\n",
    "Hood = pd.read_csv(readdir+'Hood_catalog.csv')\n",
    "\n",
    "\n",
    "St_Helens = pd.read_csv(readdir+'MountStHelens_catalog.csv')\n",
    "\n",
    "# Combining borehole and local catalogs with St_Helens\n",
    "\n",
    "Helens_Borehole = pd.read_csv(readdir+'MSHborehole_catalog.csv')\n",
    "Helens_Borehole['Clustered'] += 2000 \n",
    "# Cluster 0 in Helens_Borehole is now Cluster 2000 in St_Helens\n",
    "Helens_Local = pd.read_csv(readdir+'MSHlocal_catalog.csv')\n",
    "Helens_Local['Clustered'] += 3000\n",
    "# Cluster 0 in Helens_Local is now Cluster 3000 in St_Helens\n",
    "\n",
    "# Use St_Helens to access all three St Helens catalogs\n",
    "St_Helens = pd.concat([St_Helens,Helens_Borehole,Helens_Local])\n",
    "\n",
    "Newberry = pd.read_csv(readdir+'Newberry_catalog.csv')\n",
    "Rainier = pd.read_csv(readdir+'Rainier_catalog.csv')\n",
    "\n",
    "volc_md = pd.read_csv(readdir+'Volcano_Metadata.csv')\n",
    "# read metadata file to create dataframe of labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f5cf35",
   "metadata": {},
   "source": [
    "Associate networks and stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7b1940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_md['netsta'] = volc_md['Network'].astype(str)+'.'+volc_md['Station'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9cc4c4",
   "metadata": {},
   "source": [
    "Create Lists of Stations for Each Volcano Using volc_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "442494f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Baker_sta = volc_md[volc_md['Volcano_Name'] == 'Baker']['netsta'].values.tolist()\n",
    "Hood_sta = volc_md[volc_md['Volcano_Name'] == 'Hood']['netsta'].values.tolist() \n",
    "St_Helens_sta = volc_md[volc_md['Volcano_Name'] == 'St_Helens']['netsta'].values.tolist()\n",
    "Newberry_sta = volc_md[volc_md['Volcano_Name'] == 'Newberry']['netsta'].values.tolist() \n",
    "Rainier_sta = volc_md[volc_md['Volcano_Name'] == 'Rainier']['netsta'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d934fb1",
   "metadata": {},
   "source": [
    "Create Lists of Volcano Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bbda7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "volc_list = [Baker,Hood,Newberry,Rainier,St_Helens] # list of dataframes for each volcano\n",
    "volc_list_names = ['Baker','Hood','Newberry','Rainier','St_Helens'] # list of names of each volcano\n",
    "volc_sta = [Baker_sta,Hood_sta,Newberry_sta,Rainier_sta,St_Helens_sta] # lists of stations connected to respective volcanoes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b61d78",
   "metadata": {},
   "source": [
    "Define pick_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc7ef31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_time(ref_env, data_env_dict, st): \n",
    "    est_picktimes=str(st[0].stats.starttime)\n",
    "    xcor = correlate(data_env_dict,ref_env,int(50*fs))\n",
    "    index = np.argmax(xcor)\n",
    "    cc = round(xcor[index],9) #correlation coefficient\n",
    "    shift = 50*fs-index #how much it is shifted from the reference envelope\n",
    "    #print(shift, cc, key)\n",
    "    relative_p = shift/fs\n",
    "    p = UTCDateTime(est_picktimes) + shift/fs  # p is the new phase pick for each station\n",
    "    return p, shift, relative_p\n",
    "def get_cmap(n, name='viridis'): #hsv\n",
    "#     Returns a function that maps each index in 0, 1, ..., n-1 to a distinct \n",
    "#     RGB color; the keyword argument name must be a standard mpl colormap name.\n",
    "    return plt.cm.get_cmap(name, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cbc770",
   "metadata": {},
   "source": [
    "Define location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9afd823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to predict synthetic arrival times\n",
    "def travel_time(t0, x, y, vs, sta_x, sta_y):\n",
    "    dist = np.sqrt((sta_x - x)**2 + (sta_y - y)**2)\n",
    "    tt = t0 + dist/vs\n",
    "    return tt\n",
    "\n",
    "# define function to compute residual sum of squares\n",
    "def error(synth_arrivals,arrivals):\n",
    "    res = arrivals - synth_arrivals   #make sure arrivals are in the right order, maybe iterate through keys\n",
    "    res_sqr = res**2\n",
    "    rss = np.sum(res_sqr)\n",
    "    return rss\n",
    "\n",
    "# define function to iterate through grid and calculate travel time residuals\n",
    "def gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals):\n",
    "    rss_mat = np.zeros((len(t0),len(x_vect),len(y_vect)))\n",
    "    rss_mat[:,:,:] = np.nan\n",
    "    for i in range(len(t0)):\n",
    "        for j in range(len(x_vect)):\n",
    "            for k in range(len(y_vect)):\n",
    "                synth_arrivals = []\n",
    "                for h in range(len(sta_x)):\n",
    "                    tt = travel_time(t0[i],x_vect[j],y_vect[k],vs,sta_x[h],sta_y[h])\n",
    "                    synth_arrivals.append(tt)\n",
    "                rss = error(np.array(synth_arrivals),np.array(arrivals))\n",
    "                rss_mat[i,j,k] = rss\n",
    "    return rss_mat\n",
    "\n",
    "# define function to convert the location index into latitude and longitude\n",
    "def location(x_dist, y_dist, start_lat, start_lon):\n",
    "    bearing = 90-np.rad2deg(np.arctan(y_dist/x_dist))\n",
    "    dist = np.sqrt((x_dist)**2 + (y_dist)**2)\n",
    "    d = distance.geodesic(meters = dist)\n",
    "    loc_lat = d.destination(point=[start_lat,start_lon], bearing=bearing)[0]\n",
    "    loc_lon = d.destination(point=[start_lat,start_lon], bearing=bearing)[1]\n",
    "    return loc_lat, loc_lon, d\n",
    "\n",
    "# define function to find diameter in meters of the error on the location\n",
    "def error_diameter(new_array):\n",
    "    min_idx = np.min(new_array[:,1])\n",
    "    max_idx = np.max(new_array[:,1])\n",
    "    difference = max_idx-min_idx\n",
    "    diameter_m = difference*1000\n",
    "    return diameter_m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d0f270",
   "metadata": {},
   "source": [
    "Find picktimes and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d964c0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.7773426 -121.8132008\n",
      "48.46202728468468 -122.12851611531532\n"
     ]
    }
   ],
   "source": [
    "#finding bottom left corner of grid map\n",
    "lat_start = volc_lat_lon[volc_list_names[vv]][0] - (grid_length/222000) #volcano lat minus half of grid length in decimal lat long\n",
    "lon_start = volc_lat_lon[volc_list_names[vv]][1] - (grid_height/222000) #volcano long minus half of grid height in decimal lat long\n",
    "print(volc_lat_lon[volc_list_names[vv]][0], volc_lat_lon[volc_list_names[vv]][1])\n",
    "print(lat_start,lon_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7537b9d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "cluster: 13\n",
      "13 offsets are {'cn.vdbehzrpba013': 0.0, 'uw.mbwehzrpba013': 10.65, 'uw.passhhzrpba013': 2.975, 'uw.shukbhzrpba013': 7.2}\n",
      "start lat and lon 48.46202728468468 -122.12851611531532\n",
      "location lat lon 48.88447074616077 -121.91035841805004\n"
     ]
    }
   ],
   "source": [
    "# for vv,v in enumerate(volc_sta): #vv is the number in the list, v is the station list for current volcano\n",
    "v = volc_sta[vv]\n",
    "clid = volc_list[vv]['Clustered'].values.tolist() #find the largest cluster ID for a volcano to set range\n",
    "cllen = len(str(clid[-1])) #length of the largest cluster ID, used for zfill\n",
    "zz = chan[-2:].lower() #the last two letters of channel names (essentially the letters in chan)\n",
    "#make csv?\n",
    "\n",
    "with open(homedir+f'/locations/{volc_list_names[vv]}_Template_Locations.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Volcano_Name','netsta','Cluster_ID','Latitude','Longitude',])\n",
    "    file.close()\n",
    "\n",
    "for cl in range(0, clid[-1]+1):#normally range(0,clid[-1]+1), range(13,14) for testing #for each cluster\n",
    "    temps_s = {} #empty dictionary that will be filled with the templates for this cluster\n",
    "    #indexes are the same\n",
    "    print('------')\n",
    "    print(\"cluster:\",cl)\n",
    "    for s in range(0,len(v)): #loop through stations\n",
    "        net, sta =  v[s].split('.') #add specific network per station\n",
    "#         print(f'Volcano_{volc_list_names[vv]}_Network_{net}_Station_{sta}')\n",
    "#         try: \n",
    "\n",
    "########################################################################    \n",
    "#                            PICK TIMES                                #\n",
    "########################################################################\n",
    "\n",
    "        # try to read the .tgz file and get the template for this cluster\n",
    "        T = Tribe().read(*glob(f'{homedir}templates/Volcano_{volc_list_names[vv]}_Network_{net}_Station_{sta}_Channel_*.tgz'))\n",
    "        for t in T: #for each template in the Tribe\n",
    "            if t.name.endswith(str(cl).zfill(len(str(cl)))): #if the template name endswith this cluster\n",
    "                temps_s[f'{net.lower()}.{t.name}']=t #save to dictionary and include network name for overlapping station names\n",
    "                break\n",
    "#         except:\n",
    "#             print(f'Either .tgz does not exist or cl {str(cl).zfill(len(str(zf)))} does not have a template')\n",
    "#             pass\n",
    "#     print(temps_s.keys())\n",
    "    if len(temps_s) < minsta:\n",
    "        continue\n",
    "    data_env_dict = {}\n",
    "    for t in temps_s: #for each saved template (aka each template for this cluster and volc that exists)\n",
    "        data_envelope = obspy.signal.filter.envelope(temps_s[t].st[0].data) #make an envelope\n",
    "        data_envelope /= np.max(data_envelope) #average envelope (?)\n",
    "        data_envelope = obspy.signal.util.smooth(data_envelope, smooth_length) #smooth the envelope\n",
    "        data_env_dict[t] = data_envelope #save the envelope\n",
    "#     print(data_env_dict.keys())\n",
    "    pick_times = {} #dictionary of picktimes for each template\n",
    "    for key in data_env_dict: #for each envelope\n",
    "        p, shift, relative_p = pick_time(ref_env=data_env_dict[list(data_env_dict.keys())[0]], \n",
    "                data_env_dict=data_env_dict[key],st=temps_s[key].st) #calculate picktimes\n",
    "        pick_times[key] = relative_p #save to dictionary\n",
    "    print(f'{cl} offsets are {pick_times}')\n",
    "    \n",
    "    #arranging picktimes (largest number is earliest, so want diff between largest and everything else)\n",
    "    #will NOT be used for plotting, but will be used for location\n",
    "    dif_dict = {} #dictionary of picktimes in reference to earliest picktime (in positive seconds after the earliest picktime)\n",
    "    max_value = max(pick_times, key=pick_times.get) #get key for max value of pick_times aka the earliest picktime\n",
    "    for key in pick_times: #for each picktime\n",
    "        dif = round(abs(pick_times[max_value] - pick_times[key]),4) #max value minus current value\n",
    "        dif_dict[key] = dif #save to dictionary with the same key as pick_times\n",
    "\n",
    "    \n",
    "    #plotting picktimes and offsets\n",
    "#     cmap = get_cmap(len(temps_s)) #get cmap aka colors for the plot, see def(get_cmap) for color palette\n",
    "#     plt.figure(figsize=(10,10)) #set plot size\n",
    "#     plt.title('aligned templates, vlines are template starts') #plot title\n",
    "#     for tt,t in enumerate(temps_s): #for every template\n",
    "#         shift = round(pick_times[t]*fs) #find shift based on picktimes\n",
    "#         st0 = temps_s[t].st.copy() #make a copy for a reference\n",
    "#         maxdata = len(temps_s[t].st[0].data[:]) #find maximum length of template stream\n",
    "\n",
    "#         empty = Trace(np.zeros(shift)) #an empty trace/a trace filled with zeros\n",
    "#         if shift<0: #if shift is negative\n",
    "#             temps_s[t].st[0].data[:shift]=st0[0].data[-shift:] #shift to the left\n",
    "#         if shift>0: #if shift is positive\n",
    "#             temps_s[t].st[0].data[shift:]=st0[0].data[:-shift] #shift to the right\n",
    "#             temps_s[t].st[0].data[:shift]=empty.data[:shift] #get rid of leftover data from shift\n",
    "#         #note: if shift == 0, will be plotting with no shifting\n",
    "#         plt.plot(temps_s[t].st[0].data[:]/np.max(np.abs(temps_s[t].st[0].data[:]))+2*tt,color=cmap(tt), label=t) #plot stream\n",
    "#         plt.vlines(shift,ymin=-1,ymax=2*len(temps_s),color=cmap(tt)) #plot line in same color representing the start of the template\n",
    "#         plt.legend() #show the legend\n",
    "    \n",
    "#     first_sta = max(pick_times, key=pick_times.get) #gives you the template name for the first\n",
    "    #station to get a signal (largest/most positive pick time)\n",
    "    \n",
    "########################################################################    \n",
    "#                             LOCATION                                 #\n",
    "########################################################################\n",
    "\n",
    "    # define input parameters\n",
    "    arrivals = [] #relative picktimes, dif_dict as a list\n",
    "    sta_lats = [] #station latitudes, from metadata\n",
    "    sta_lons = [] #station longitudes, from metadata\n",
    "    netsta_names = [] #list of station names with networks\n",
    "    for key in dif_dict:\n",
    "        arrivals.append(dif_dict[key]) #append pick time to arrivals\n",
    "        #finding station name\n",
    "        \n",
    "        if not key.endswith(str(cl).zfill(cllen)): #if the wrong cluster id\n",
    "            print('template name does not match cluster ID') #print an error\n",
    "            continue #and skip the rest\n",
    "        if key.endswith(f'{zz}rp{volc_list_names[vv][:2].lower()}{str(cl).zfill(cllen)}'): #if the key has a channel name in it (the 'hz')\n",
    "            md_netsta = key[:-(7+cllen)] #remember the stuff before channel name rpvo and cluster ID\n",
    "        if not key.endswith(f'{zz}rp{volc_list_names[vv][:2].lower()}{str(cl).zfill(cllen)}'): #if the key has NO channel name in it\n",
    "            md_netsta = key[:-(4+cllen)] #remember the stuff before rpvo and cluster ID\n",
    "#         print(md_netsta)\n",
    "        lat = volc_md[volc_md['netsta']==md_netsta.upper()]['Latitude'].values.tolist() #get latitude from metadata\n",
    "        sta_lats.append(lat[0]) #append\n",
    "        \n",
    "        lon = volc_md[volc_md['netsta']==md_netsta.upper()]['Longitude'].values.tolist() #get longitude form metadata\n",
    "        sta_lons.append(lon[0]) #append\n",
    "        \n",
    "        netsta_names.append(md_netsta.upper()) # make list of netstas \n",
    "#     print(arrivals)\n",
    "#     print(sta_lats)\n",
    "#     print(sta_lons)\n",
    "            \n",
    "\n",
    "    # define grid origin in lat,lon\n",
    "    \n",
    "    #finding bottom left corner of grid map\n",
    "    lat_start = volc_lat_lon[volc_list_names[vv]][0] - (grid_length/222000) #volcano lat minus half of grid length in decimal lat long\n",
    "    lon_start = volc_lat_lon[volc_list_names[vv]][1] - (grid_height/222000) #volcano long minus half of grid height in decimal lat long\n",
    "#     print(volc_lat_lon[volc_list_names[vv]][0], volc_lat_lon[volc_list_names[vv]][1])\n",
    "    print('start lat and lon',lat_start,lon_start)\n",
    "        \n",
    "    #station lat lons to x y\n",
    "    sta_x = []\n",
    "    sta_y = []\n",
    "    for i in range(len(sta_lats)):\n",
    "        x_dist = distance.distance([lat_start,lon_start],[lat_start,sta_lons[i]]).m\n",
    "        y_dist = distance.distance([lat_start,lon_start],[sta_lats[i],lon_start]).m\n",
    "        sta_x.append(x_dist)\n",
    "        sta_y.append(y_dist)\n",
    "\n",
    "    # set grid points\n",
    "    x_vect = np.arange(0, grid_length, step)\n",
    "    y_vect = np.arange(0, grid_height, step)\n",
    "    t0 = np.arange(0,np.max(arrivals),t_step)\n",
    "\n",
    "    # carry out the gridsearch\n",
    "    rss_mat = gridsearch(t0,x_vect,y_vect,sta_x,sta_y,vs,arrivals)\n",
    "\n",
    "    # find lowest error lat, lon, and origin time\n",
    "    loc_idx = np.unravel_index([np.argmin(rss_mat)], rss_mat.shape)\n",
    "    \n",
    "    # find the lat and lon of the location index\n",
    "    loc_lat, loc_lon, d = location(x_vect[loc_idx[1]], y_vect[loc_idx[2]], lat_start, lon_start)\n",
    "    err_thr = np.min(np.log10(rss_mat))+.05\n",
    "    thr_array = np.argwhere(np.log10(rss_mat)<err_thr)\n",
    "    diameter = error_diameter(thr_array)\n",
    "    \n",
    "    print('location lat lon',loc_lat,loc_lon)\n",
    "\n",
    "    # plot a spatial map of error for lowest-error origin time\n",
    "#     fig,ax = plt.subplots()\n",
    "#     ax.scatter(x_vect[loc_idx[1]],y_vect[loc_idx[2]],s=100,marker='*',c='r')\n",
    "#     im = ax.imshow(np.log10(rss_mat[loc_idx[0],:,:].T),origin=\"lower\",extent=[0,grid_length,0,grid_height])\n",
    "#     fig.colorbar(im)\n",
    "#     plt.show()\n",
    "    \n",
    "    row = [volc_list_names[vv],' '.join(netsta_names),cl,loc_lat,loc_lon]\n",
    "    with open(homedir+f'/locations/{volc_list_names[vv]}_Template_Locations.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(row)\n",
    "        file.close()\n",
    "    \n",
    "    #write into csv relative picktimes in seconds after first_sta, probably list separated by \n",
    "    #spaces, like how stations are saved in events, make sure index is same for the template \n",
    "    #name and picktime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc8705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seismo (SHARED)",
   "language": "python",
   "name": "seismo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
